{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ca4a4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce1e3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c350687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "text = \"\"\"  [\"Women's Father sAdamse когда There's Is Fliest Night in the Manlife Room gasps Imperial RO more of a story of peculiarity andgasp whippedidos up by the You won't get this wrong any other guy I just want to tell you how I feel You got to make you understand Never gonna give you up Never gonna let you down Never gonna run around it He's hurt you Never gonna make you cry Never gonna say goodbye Never gonna tell the lie And hurt you We've known each other for so long Your heart's been aching but You're too shy to say it It's how we've all know what's been going on We know the game I'm with Gonna play it And if you ask me why I feel it You don't tell me you're too bad to see Never gonna give you up Never gonna let you down Never gonna run around it He's hurt you Never gonna make you cry Never gonna say goodbye Never gonna tell the lie And hurt you Never gonna give you up Never gonna let you down Never gonna run around it He's hurt you Never gonna make you cry Never gonna say goodbye Never gonna tell the lie And hurt you Never gonna give you up Never gonna give you up Never gonna give you up We've known each other for so long Your heart's been aching but You're too shy to say It's how we've all know what's been going on We know the game I'm with Gonna play it I just wanna tell you how I feel You got to make you understand Never gonna give you up Never gonna let you down Never gonna run around it He's hurt you Never gonna make you cry Never gonna say goodbye Never gonna tell the lie And hurt you Never gonna give you up Never gonna let you down Never gonna run around it He's hurt you Never gonna make you cry Never gonna say goodbye Never gonna tell the lie And hurt you Never gonna give you up Never gonna let you down Never gonna run around it He's hurt you Never gonna make you cry Never gonna say goodbye Never gonna tell the lie And hurt you\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f113e82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:[Node(id='Women', type='Person', properties={})]\n",
      "Relationships:[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "documents = [Document(page_content=text)]\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "print(f\"Nodes:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "956bc1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jGraph\n",
    "os.environ[\"NEO4J_URI\"] = \"neo4j+s://ffa05957.databases.neo4j.io\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"KYDXnv3miiiEFXu9p1119eo77ugFg_gcBqFxLlVD1h4\"\n",
    "\n",
    "graph = Neo4jGraph(refresh_schema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea232a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_graph_documents(graph_documents, include_source=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a554b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing query tool...\n",
      "{'n': {'id': 'e1b070507716879d5c75a6117d566034', 'text': '  [\"Women\\'s Father sAdamse когда There\\'s Is Fliest Night in the Manlife Room gasps Imperial RO more of a story of peculiarity andgasp whippedidos up by the You won\\'t get this wrong any other guy I just want to tell you how I feel You got to make you understand Never gonna give you up Never gonna let you down Never gonna run around it He\\'s hurt you Never gonna make you cry Never gonna say goodbye Never gonna tell the lie And hurt you We\\'ve known each other for so long Your heart\\'s been aching but You\\'re too shy to say it It\\'s how we\\'ve all know what\\'s been going on We know the game I\\'m with Gonna play it And if you ask me why I feel it You don\\'t tell me you\\'re too bad to see Never gonna give you up Never gonna let you down Never gonna run around it He\\'s hurt you Never gonna make you cry Never gonna say goodbye Never gonna tell the lie And hurt you Never gonna give you up Never gonna let you down Never gonna run around it He\\'s hurt you Never gonna make you cry Never gonna say goodbye Never gonna tell the lie And hurt you Never gonna give you up Never gonna give you up Never gonna give you up We\\'ve known each other for so long Your heart\\'s been aching but You\\'re too shy to say It\\'s how we\\'ve all know what\\'s been going on We know the game I\\'m with Gonna play it I just wanna tell you how I feel You got to make you understand Never gonna give you up Never gonna let you down Never gonna run around it He\\'s hurt you Never gonna make you cry Never gonna say goodbye Never gonna tell the lie And hurt you Never gonna give you up Never gonna let you down Never gonna run around it He\\'s hurt you Never gonna make you cry Never gonna say goodbye Never gonna tell the lie And hurt you Never gonna give you up Never gonna let you down Never gonna run around it He\\'s hurt you Never gonna make you cry Never gonna say goodbye Never gonna tell the lie And hurt you\\n'}, 'r': ({'id': 'e1b070507716879d5c75a6117d566034', 'text': '  [\"Women\\'s Father sAdamse когда There\\'s Is Fliest Night in the Manlife Room gasps Imperial RO more of a story of peculiarity andgasp whippedidos up by the You won\\'t get this wrong any other guy I just want to tell you how I feel You got to make you understand Never gonna give you up Never gonna let you down Never gonna run around it He\\'s hurt you Never gonna make you cry Never gonna say goodbye Never gonna tell the lie And hurt you We\\'ve known each other for so long Your heart\\'s been aching but You\\'re too shy to say it It\\'s how we\\'ve all know what\\'s been going on We know the game I\\'m with Gonna play it And if you ask me why I feel it You don\\'t tell me you\\'re too bad to see Never gonna give you up Never gonna let you down Never gonna run around it He\\'s hurt you Never gonna make you cry Never gonna say goodbye Never gonna tell the lie And hurt you Never gonna give you up Never gonna let you down Never gonna run around it He\\'s hurt you Never gonna make you cry Never gonna say goodbye Never gonna tell the lie And hurt you Never gonna give you up Never gonna give you up Never gonna give you up We\\'ve known each other for so long Your heart\\'s been aching but You\\'re too shy to say It\\'s how we\\'ve all know what\\'s been going on We know the game I\\'m with Gonna play it I just wanna tell you how I feel You got to make you understand Never gonna give you up Never gonna let you down Never gonna run around it He\\'s hurt you Never gonna make you cry Never gonna say goodbye Never gonna tell the lie And hurt you Never gonna give you up Never gonna let you down Never gonna run around it He\\'s hurt you Never gonna make you cry Never gonna say goodbye Never gonna tell the lie And hurt you Never gonna give you up Never gonna let you down Never gonna run around it He\\'s hurt you Never gonna make you cry Never gonna say goodbye Never gonna tell the lie And hurt you\\n'}, 'MENTIONS', {'id': 'Women'}), 'm': {'id': 'Women'}}\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Neo4j credentials\n",
    "NEO4J_URI = \"neo4j+s://ffa05957.databases.neo4j.io\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"KYDXnv3miiiEFXu9p1119eo77ugFg_gcBqFxLlVD1h4\"\n",
    "# Create a class to interact with Neo4j\n",
    "class Neo4jQueryTool:\n",
    "    def __init__(self, uri, username, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def run_query(self, query):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(query)\n",
    "            return [record.data() for record in result]\n",
    "\n",
    "# Create an instance of the query tool\n",
    "tool = Neo4jQueryTool(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "\n",
    "# Test the tool with a sample query\n",
    "try:\n",
    "    print(\"Testing query tool...\")\n",
    "    \n",
    "    # Replace with your test query\n",
    "    sample_query = \"\"\"\n",
    "        MATCH (n)-[r]->(m)\n",
    "        RETURN n, r, m\n",
    "        LIMIT 10\n",
    "    \"\"\"\n",
    "    \n",
    "    results = tool.run_query(sample_query)\n",
    "    for record in results:\n",
    "        print(record)\n",
    "finally:\n",
    "    tool.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2522ed43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] C:\\Users\\SURYA\\AppData\\Local\\Temp\\ipykernel_26252\\988719966.py:16: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain.run(\"Deep Neural Networks\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (n)\n",
      "WHERE n.name = \"Deep Neural Networks\"\n",
      "RETURN n\n",
      "\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I am sorry, I don't know the answer.\n"
     ]
    }
   ],
   "source": [
    "from langchain_neo4j import GraphCypherQAChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Initialize the Gemini model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "\n",
    "# Assuming 'graph' is your Neo4jGraph instance\n",
    "qa_chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True  # Explicitly allow dangerous requests\n",
    ")\n",
    "\n",
    "# Now you can run your query\n",
    "response = qa_chain.run(\"Deep Neural Networks\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f079e608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding model...\n",
      "Embedding model initialized.\n",
      "Preparing documents and creating FAISS vector store...\n",
      "FAISS vector store created successfully.\n",
      "Creating RAG chain...\n",
      "RAG chain created.\n"
     ]
    }
   ],
   "source": [
    "# rag_faiss_system.py\n",
    "\n",
    "# --- Dependencies ---\n",
    "# Make sure you have installed the necessary packages:\n",
    "# pip install langchain faiss-cpu sentence-transformers langchain-google-genai python-dotenv\n",
    "# (or faiss-gpu if you have CUDA installed and want GPU acceleration)\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS # Use langchain_community\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings # Use langchain_community\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # Import Gemini\n",
    "from langchain.docstore.document import Document # For structured documents\n",
    "\n",
    "# --- Configuration ---\n",
    "# Load environment variables (especially GOOGLE_API_KEY)\n",
    "load_dotenv() \n",
    "\n",
    "# Check if the API key is available\n",
    "if os.getenv(\"GOOGLE_API_KEY\") is None:\n",
    "    print(\"Error: GOOGLE_API_KEY environment variable not set.\")\n",
    "    print(\"Please get an API key from Google AI Studio (https://aistudio.google.com/app/apikey)\")\n",
    "    print(\"and set it as an environment variable (e.g., in a .env file).\")\n",
    "    exit() # Exit if the key is missing\n",
    "\n",
    "# --- Main Function ---\n",
    "# Step 1: Initialize the embedding model\n",
    "print(\"Initializing embedding model...\")\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'} # Specify device (cpu or cuda)\n",
    "    # Set encode_kwargs={'normalize_embeddings': True} if using cosine similarity,\n",
    "    # but FAISS IndexFlatL2 uses L2 distance (Euclidean), so normalization is optional \n",
    "    # but often helpful. Let's keep it simple for L2.\n",
    ")\n",
    "print(\"Embedding model initialized.\")\n",
    "\n",
    "# Step 2 & 3: Prepare documents and create FAISS index\n",
    "print(\"Preparing documents and creating FAISS vector store...\")\n",
    "documents_data = [\n",
    "    {\"content\": \"\"\"\n",
    "        Neuro Prune: An Adaptive Approach for Efficient\n",
    "Deep Neural Network Optimization on Edge\n",
    "Devices\n",
    "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
    "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
    "grk.cse@psgtech.ac.in\n",
    "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
    "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
    "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
    "Abstract—The optimization of deep neural networks for de-\n",
    "ployment on edge devices is a significant research area due to\n",
    "the demand for applications such as augmented reality, smart\n",
    "cameras, and autonomous navigation. However, deploying large\n",
    "deep learning models on edge devices poses challenges related to\n",
    "computational power, energy consumption, and latency. Pruning\n",
    "is a method to reduce the model size, accelerate inference, and\n",
    "save power. The objective of the paper is to propose the Neuro\n",
    "Prune algorithm and to apply it for the optimization of deep\n",
    "neural networks on edge devices. Efforts have been made to\n",
    "compare pruned and unpruned models. As a result, the pruned\n",
    "model has an accuracy increase of 0.22%.\n",
    "I. INTRODUCTION\n",
    "The rise of intelligent systems, ranging from autonomous\n",
    "drones to augmented reality (AR) devices and smart surveil-\n",
    "lance cameras, has intensified the need for efficient deep neural\n",
    "network deployment on edge devices with constrained com-\n",
    "putational resources. While advanced deep learning models\n",
    "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
    "their deployment on such devices is hindered by significant\n",
    "computational demands, large memory footprints, and high\n",
    "energy consumption. These challenges pose critical barriers to\n",
    "real-time applications and sustainable deployment.\n",
    "This paper builds on existing pruning techniques, includ-\n",
    "ing unstructured pruning, structured pruning, and approaches\n",
    "inspired by quantization, highlighting their strengths and lim-\n",
    "itations. In response to the challenges identified, we introduce\n",
    "Neuro Prune, a novel pruning approach that integrates princi-\n",
    "ples from reinforcement learning and the Lottery Ticket Hy-\n",
    "pothesis to achieve an optimal balance between accuracy, com-\n",
    "putational efficiency, and energy consumption. By integrating\n",
    "these principles, Neuro Prune provides a robust framework\n",
    "for optimizing deep neural networks for deployment on edge\n",
    "devices without sacrificing performance.\n",
    "Neuro Prune reduces model size and computational re-\n",
    "quirements while maintaining or even improving the model’s\n",
    "accuracy. These findings underscore the potential of Neuro\n",
    "Prune to enable real-time, energy-efficient deep learning on\n",
    "resource-constrained devices.\n",
    "Neuro Prune employs a systematic process to optimize\n",
    "neural networks for edge deployment:\n",
    "• Mask Initialization: Each layer’s weights are paired with\n",
    "a binary mask (original_mask), initialized to ones.\n",
    "• Activation Tracking: During forward passes over the\n",
    "dataset, the magnitudes of weight activations are mon-\n",
    "itored and recorded.\n",
    "• Normalize and Prune: The recorded activations are\n",
    "normalized, and a threshold based on the pruning fraction\n",
    "is computed. Using this threshold, the masks are dynam-\n",
    "ically updated to prune unimportant weights.\n",
    "• Evaluate and Log Metrics: After pruning, metrics such\n",
    "as model sparsity and accuracy are computed. A reward\n",
    "metric is derived to balance accuracy and sparsity using\n",
    "a tunable parameter, λweight.\n",
    "• Retrain the Model: The pruned model undergoes retrain-\n",
    "ing to improve the loss in accuracy while maintaining low\n",
    "computational requirements\n",
    "Neuro Prune’s unique combination of reinforcement learn-\n",
    "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
    "pothesis for identifying optimal subnetworks enables it to\n",
    "achieve remarkable efficiency. This approach ensures substan-\n",
    "tial reduction in model size and energy consumption while\n",
    "preserving accuracy and inference speed, making it an ideal\n",
    "solution for edge-based deep learning applications.\n",
    "Through extensive empirical evaluations and theoretical\n",
    "insights, this paper demonstrates how Neuro Pruning outper-\n",
    "forms traditional pruning methods, setting a new benchmark\n",
    "for sustainable, high-performance AI deployment on edge\n",
    "devices. By situating our work within the broader research\n",
    "context, we aim to offer a critical contribution to the ongoing\n",
    "evolution of neural network optimization techniques.\n",
    "II. BACKGROUND\n",
    "A. Need for Pruning Techniques\n",
    "Pruning is a crucial optimization technique for deploying\n",
    "deep neural networks in resource-constrained environments\n",
    "like healthcare. For example, wearable devices like smart-\n",
    "watches that monitor heart rates or portable EEG systems\n",
    "for brain activity analysis require lightweight models due to\n",
    "their limited computational power and memory. Pruning helps\n",
    "address these constraints by reducing the memory footprint\n",
    "and computational load, enabling efficient model deployment.\n",
    "Additionally, it minimizes energy consumption, which is vital\n",
    "for sustainable and real-time healthcare applications. Faster\n",
    "inference enabled by pruning is particularly beneficial for\n",
    "time-sensitive tasks like seizure detection or emergency di-\n",
    "agnostics. Moreover, pruning enhances model interpretability\n",
    "by retaining only the most critical components, which is\n",
    "essential for gaining trust and ensuring regulatory compliance\n",
    "in healthcare. By reducing the hardware and operational costs,\n",
    "pruning also makes deploying advanced AI solutions on edge\n",
    "devices more cost-effective and accessible.\n",
    "B. Types of Pruning Techniques\n",
    "Pruning methods are broadly classified based on granu-\n",
    "larity, timing, and approach. Granularity-based pruning in-\n",
    "cludes structured pruning, which removes entire components\n",
    "such as neurons or filters to streamline computations, and\n",
    "unstructured pruning, which eliminates individual weights\n",
    "for finer optimization, though it often requires specialized\n",
    "hardware. Timing-based pruning involves pre-training pruning,\n",
    "which optimizes models before training; post-training prun-\n",
    "ing, where redundant components are removed from trained\n",
    "models with possible retraining; and dynamic pruning during\n",
    "training, which adjusts models in real-time for improved\n",
    "adaptability. Metric-based pruning uses parameters like weight\n",
    "magnitude or gradient contribution to identify and remove\n",
    "less critical components, while application-specific pruning\n",
    "tailors strategies to specific tasks, such as optimizing for EEG\n",
    "signal analysis, or hardware constraints like GPUs. Together,\n",
    "these methods enable the development of efficient, scalable,\n",
    "and task-specific models, particularly suited for edge device\n",
    "deployment in healthcare and other domains.\n",
    "III. RELATED WORK\n",
    "Jielei Wang et al.[12] introduced an absorption pruning\n",
    "method for object detection in remote sensing imagery. This\n",
    "study achieves efficient compression with minimal accuracy\n",
    "loss but requires careful layer-wise pruning ratio tuning.\n",
    "Jan Muller et al., in their research[12], propose a neural\n",
    "network pruning method for multi-object tracking (MOT) that\n",
    "reduces model size by up to 70\n",
    "Liang Li et al., in their work[9], introduce a novel pruning\n",
    "method for DNNs that utilizes a self-adaptive mechanism\n",
    "based on weight sparsity ratios and a protective reconstruction\n",
    "mechanism. Their approach improves both model compres-\n",
    "sion and accuracy, outperforming state-of-the-art methods on\n",
    "CIFAR-10 and ImageNet datasets.\n",
    "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
    "learning with edge devices like Google Coral AI and Nvidia\n",
    "Jetson Nano. Their work revolutionizes computer vision and\n",
    "real-time tracking but raises energy concerns due to the\n",
    "computational demands of advanced algorithms.\n",
    "Satoru Koda et al.[7] explore how weight pruning in\n",
    "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
    "larly with Mahalanobis-based approaches, by improving global\n",
    "feature extraction and leveraging weights not critical for\n",
    "classification.\n",
    "Shvetha S Kumar et al. analyze and compare three pruning\n",
    "techniques—L1-norm filter pruning, channel pruning, and\n",
    "weight pruning—on CNNs for accuracy and inference time,\n",
    "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
    "pared to GTX1080 Ti in their study[8].\n",
    "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
    "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
    "on stochastic networks, extending it to related problems and\n",
    "demonstrating its efficiency on real-world networks in their\n",
    "study[13].\n",
    "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
    "Training outperforms Post-Training Quantization, and sparsity\n",
    "training with Optimal Reduction pruning improves YOLOv4’s\n",
    "efficiency and edge-device performance, despite potential\n",
    "high-pruning rate drawbacks.\n",
    "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
    "method for DNNs. This approach optimizes inference speed\n",
    "and resource efficiency on edge devices with minimal accuracy\n",
    "loss but requires careful tuning of layer-specific pruning ratios.\n",
    "Yongqi An et al.[1] introduce a retraining-free structured\n",
    "pruning framework for Large Language Models (LLMs). Their\n",
    "method incorporates a fluctuation-based pruning metric, adap-\n",
    "tive structure search, and a bias compensation mechanism to\n",
    "achieve efficient pruning. FLAP significantly enhances infer-\n",
    "ence speed and reduces model size without requiring retrain-\n",
    "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
    "and Wanda-sp on LLaMA models across various benchmarks.\n",
    "From referring to these research papers, we derived an\n",
    "idea for pruning on edge devices. Additionally, our proposed\n",
    "solution is inspired by the research of Jaron Maene et al.[10],\n",
    "which suggests that sparse subnetworks in dense networks can\n",
    "achieve similar accuracy when retrained. This study demon-\n",
    "strates stable training with linear mode connectivity, support-\n",
    "ing the idea that lottery tickets retrain to similar regions, but\n",
    "questions their independence from dense training and iterative\n",
    "pruning.\n",
    "Eran Malach et al.[11] further strengthen the lottery ticket\n",
    "hypothesis by proving that over-parameterized neural networks\n",
    "with random weights always contain a subnetwork matching\n",
    "the accuracy of a target network, without additional training.\n",
    "IV. NEUROLOGICAL PRUNING APPROACH\n",
    "The Neurological Pruning Algorithm is designed to opti-\n",
    "mize neural networks by iteratively pruning less important\n",
    "weights while maintaining high performance. Unlike tradi-\n",
    "tional methods, this approach incorporates adaptive, layer-\n",
    "specific pruning thresholds that dynamically adjust based\n",
    "on weight activations, ensuring efficient model optimization\n",
    "across various stages of training. Drawing inspiration from\n",
    "the Lottery Ticket Hypothesis, which suggests that sparse\n",
    "subnetworks can perform similarly to their dense counterparts\n",
    "when retrained, the algorithm selectively deactivates weights\n",
    "using binary masks to enhance computational efficiency with-\n",
    "out sacrificing accuracy.\n",
    "The algorithm operates in several epochs, during which\n",
    "weight activations are evaluated. Based on a quantile-based\n",
    "strategy, pruning thresholds are established for each layer, en-\n",
    "suring that only weights contributing minimally to the model’s\n",
    "output are removed. After pruning, the model undergoes fine-\n",
    "tuning to restore any lost accuracy and further optimize the\n",
    "network. This step enhances the adaptability and scalability\n",
    "of the pruning method, making it suitable for deployment in\n",
    "resource-constrained environments\n",
    "A. Key Characteristics and Advantages\n",
    "The Neurological Pruning Algorithm boasts several key\n",
    "characteristics that distinguish it from conventional pruning\n",
    "techniques. One notable feature is its layer-specific pruning,\n",
    "which evaluates the importance of weights individually for\n",
    "each layer, leading to a more nuanced and efficient sparsifi-\n",
    "cation process. This approach prevents the model from losing\n",
    "essential features that may otherwise be pruned by a global\n",
    "approach.\n",
    "The algorithm’s use of quantile-based thresholding en-\n",
    "sures that only weights with the least contribution to the\n",
    "model’s output are pruned. This fine-tuned method avoids\n",
    "performance degradation by carefully targeting weights that\n",
    "are less significant to the network’s overall functionality.\n",
    "Furthermore, the pruning process is iterative, allowing for\n",
    "continuous model evaluation and real-time adjustments based\n",
    "on performance feedback, ensuring that accuracy is retained\n",
    "even as sparsity increases.\n",
    "Additionally, the algorithm maintains a careful balance\n",
    "between sparsity and accuracy, enabling a reduction in the\n",
    "model’s computational complexity while preserving its predic-\n",
    "tive capabilities. This characteristic is particularly important\n",
    "for applications requiring rapid inference times, such as in\n",
    "real-time healthcare systems and edge computing scenarios,\n",
    "where both energy efficiency and computational power are\n",
    "crucial.\n",
    "B. Impact on Model Performance\n",
    "The Neurological Pruning Algorithm significantly improves\n",
    "the efficiency of neural networks, making them more suitable\n",
    "for deployment on edge devices. Key performance metrics\n",
    "such as compression ratio (CR) and accuracy retention\n",
    "(AR) are critical indicators of the algorithm’s effectiveness.\n",
    "For instance, in evaluating a ResNet-18 model, the algorithm\n",
    "achieved a CR of 1, indicating that while the number of\n",
    "parameters remained the same, the model’s efficiency im-\n",
    "proved through selective pruning, reducing computational load\n",
    "without sacrificing performance.\n",
    "Moreover, the algorithm demonstrates the ability to achieve\n",
    "high accuracy retention, with models maintaining or even\n",
    "slightly improving their predictive performance post-pruning.\n",
    "This outcome highlights the algorithm’s capability to effi-\n",
    "ciently remove redundant weights while preserving the core\n",
    "functionality of the network. The result is a model that is\n",
    "not only more computationally efficient but also adaptable\n",
    "across different architectures and datasets, making it suit-\n",
    "able for a wide range of applications, from image classification\n",
    "to natural language processing .\n",
    "In practical terms, the Neurological Pruning Algorithm en-\n",
    "ables faster inference times and lower energy consumption,\n",
    "two crucial aspects for deploying AI models in resource-\n",
    "constrained environments. As such, it holds significant poten-\n",
    "tial for improving the viability of real-time, energy-efficient\n",
    "AI applications, particularly in fields like healthcare, where\n",
    "both speed and sustainability are essential.\n",
    "V. NEUROLOGICAL PRUNING ALGORITHM\n",
    "The Neurological Pruning Algorithm is a structured and\n",
    "adaptive methodology designed to optimize neural networks\n",
    "for resource-constrained environments. This algorithm goes\n",
    "beyond traditional pruning approaches by incorporating layer-\n",
    "specific pruning thresholds that dynamically adjust to the\n",
    "importance of weight activations within each layer. Drawing\n",
    "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
    "inforcement learning, the algorithm strikes an optimal balance\n",
    "between model sparsity and accuracy .\n",
    "A. Algorithm Description\n",
    "The Neurological Pruning Algorithm iteratively refines the\n",
    "neural network through a series of training epochs. During\n",
    "each epoch, weight activations are tracked, normalized, and\n",
    "analyzed to identify low-importance weights. These weights\n",
    "are selectively deactivated using binary masks, ensuring com-\n",
    "putational efficiency without compromising the network’s pre-\n",
    "dictive performance. The algorithm further integrates a fine-\n",
    "tuning phase, enabling the pruned network to recover and\n",
    "enhance its accuracy. The detailed process is presented in\n",
    "Algorithm 1.\n",
    "By dynamically adjusting to layer-specific characteristics\n",
    "and maintaining performance metrics, the Neurological Prun-\n",
    "ing Algorithm ensures a significant reduction in computational\n",
    "and memory requirements, paving the way for efficient deploy-\n",
    "ment of deep learning models on edge devices .\n",
    "B. Input Parameters and Output\n",
    "Inputs:\n",
    "• Pretrained model (M)\n",
    "• Dataset (D)\n",
    "• Prune fraction (p)\n",
    "• Total epochs (E)\n",
    "Output:\n",
    "• Pruned and fine-tuned model (Mpruned)\n",
    "C. Key Features and Advantages\n",
    "The Neurological Pruning Algorithm is designed with sev-\n",
    "eral distinctive features that contribute to its effectiveness\n",
    "in optimizing neural network performance. These features\n",
    "include:\n",
    "Algorithm 1 Simplified Neurological Pruning Algorithm\n",
    "Require: Pretrained model M, Dataset D, Prune fraction p,\n",
    "Total epochs E\n",
    "Ensure: Pruned and fine-tuned model Mpruned\n",
    "1: Initialize binary masks M with all ones\n",
    "2: for each epoch e = 1 to E do\n",
    "3:\n",
    "Compute weight activations A during forward passes\n",
    "4:\n",
    "for each layer i in M do\n",
    "5:\n",
    "Compute pruning threshold Ti as p-quantile of Ai\n",
    "6:\n",
    "Update mask Mi: retain weights with Ai > Ti\n",
    "7:\n",
    "Apply mask to layer: θi ←θi · Mi\n",
    "8:\n",
    "end for\n",
    "9:\n",
    "Evaluate model performance and update metrics\n",
    "10: end for\n",
    "11: Fine-tune Mpruned on D\n",
    "12: return Mpruned\n",
    "• Layer-specific Pruning: The algorithm applies pruning\n",
    "on a per-layer basis, evaluating the weight activations\n",
    "within each layer individually. This approach ensures that\n",
    "the sparsity introduced by pruning is well-distributed and\n",
    "context-dependent, facilitating more nuanced control over\n",
    "model optimization .\n",
    "• Quantile-based Thresholding: A quantile-based strat-\n",
    "egy is employed to determine the pruning threshold for\n",
    "each layer. This method ensures that only weights with\n",
    "minimal contribution to the network’s output are pruned,\n",
    "thereby reducing the risk of adversely affecting model\n",
    "accuracy while enhancing sparsity .\n",
    "• Iterative Pruning and Evaluation: The pruning pro-\n",
    "cess follows an iterative approach, enabling continuous\n",
    "assessment of the model’s performance after each pruning\n",
    "step. This ensures dynamic adjustments to maintain a bal-\n",
    "ance between model sparsity and predictive performance\n",
    "throughout training.\n",
    "• Preservation of Model Accuracy: By adjusting pruning\n",
    "thresholds in response to ongoing performance metrics,\n",
    "the algorithm ensures that model accuracy is not com-\n",
    "promised. The careful tuning of these thresholds allows\n",
    "for significant sparsity while maintaining the robustness\n",
    "of the network’s predictive capabilities .\n",
    "D. Impact on Neural Network Performance\n",
    "The Neurological Pruning Algorithm has been extensively\n",
    "evaluated across various benchmark datasets and model archi-\n",
    "tectures, demonstrating its capacity to achieve the following\n",
    "outcomes:\n",
    "• High Sparsity with Minimal Accuracy Loss: The algo-\n",
    "rithm effectively prunes unnecessary weights, achieving\n",
    "high sparsity levels without causing significant degrada-\n",
    "tion in model accuracy. This outcome is critical for the\n",
    "efficient deployment of models in resource-constrained\n",
    "environments.\n",
    "• Reduced Computational Complexity: By sparsifying\n",
    "the weight matrices, the algorithm reduces the compu-\n",
    "tational load during inference, which translates to faster\n",
    "model execution and lower resource consumption. This\n",
    "characteristic is especially beneficial in edge computing\n",
    "and real-time applications .\n",
    "• Broad Applicability: The algorithm is highly flexible\n",
    "and can be adapted to various neural network architec-\n",
    "tures and datasets, making it suitable for a wide range\n",
    "of tasks, from image recognition to natural language\n",
    "processing .\n",
    "VI. EVALUATION METRICS\n",
    "A. Performance Comparison\n",
    "Fig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\n",
    "TABLE I\n",
    "COMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\n",
    "Metric\n",
    "Unpruned\n",
    "Model\n",
    "Pruned\n",
    "Model\n",
    "Accuracy\n",
    "0.9562\n",
    "0.9583\n",
    "Train Loss\n",
    "0.1414\n",
    "0.1625\n",
    "Inference\n",
    "Time\n",
    "(s)\n",
    "0.0318\n",
    "0.0314\n",
    "No of Parame-\n",
    "ters(million)\n",
    "11.7\n",
    "11.7\n",
    "B. Evaluation Metrics Calculation\n",
    "The key metrics used to evaluate the proposed pruning\n",
    "technique are as follows:\n",
    "Compression Ratio (CR): The Compression Ratio (CR) is\n",
    "calculated as:\n",
    "CR = Total Parameters After Pruning\n",
    "Total Parameters Before Pruning\n",
    "(1)\n",
    "CR = 11.7M\n",
    "11.7M = 1\n",
    "Accuracy Retention (AR): The Accuracy Retention (AR) is\n",
    "calculated as:\n",
    "AR =\n",
    "\u0012Accuracy After Pruning −Baseline Accuracy\n",
    "Baseline Accuracy\n",
    "\u0013\n",
    "×100\n",
    "(2)\n",
    "AR =\n",
    "\u00120.958284 −0.956206\n",
    "0.956206\n",
    "\u0013\n",
    "× 100 = 0.22%\n",
    "This result indicates that the pruned model achieves an ac-\n",
    "curacy improvement of 0.22% when compared to unpruned\n",
    "model.\n",
    "C. Illustration of Results\n",
    "Fig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\n",
    "18.\n",
    "D. Discussion\n",
    "The evaluation metrics highlight the effectiveness of the\n",
    "proposed Neurological Pruning method on the ResNet-18\n",
    "model. The Compression Ratio (CR) remains 1, indicating\n",
    "no reduction in the total number of parameters, which suggests\n",
    "that this approach optimizes the neural network by selectively\n",
    "pruning connections rather than reducing the overall parameter\n",
    "count. The Accuracy Retention (AR), which is approximately\n",
    "0.22%, demonstrates that the Neurological Pruning technique\n",
    "successfully improved the accuracy compared to the baseline\n",
    "model.\n",
    "These results validate the potential of Neurological Pruning\n",
    "as an innovative approach to optimizing deep learning models.\n",
    "By maintaining accuracy while likely enhancing sparsity and\n",
    "computational efficiency, this method presents a promising di-\n",
    "rection for achieving high-performance models under resource\n",
    "constraints.\n",
    "VII. CONCLUSION\n",
    "In summary, the Neurological Pruning Algorithm provides\n",
    "a systematic and principled approach to optimizing neural\n",
    "networks. By balancing the trade-off between sparsity and\n",
    "accuracy, it enables significant reductions in computational\n",
    "complexity without sacrificing the network’s predictive per-\n",
    "formance. Future work will focus on extending the algorithm\n",
    "to dynamic network architectures and incorporating advanced\n",
    "regularization techniques to further enhance its robustness and\n",
    "adaptability in diverse applications.\n",
    "REFERENCES\n",
    "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\n",
    "Fluctuation-based Adaptive Structured Pruning for Large Language\n",
    "Models.\n",
    "[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\n",
    "Simplification of Deep Neural Network-Based Object Detector for Real-\n",
    "Time Edge Computing.\n",
    "[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\n",
    "Devices Object Detection by Filter Pruning.\n",
    "[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\n",
    "and Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\n",
    "Tracking Algorithms in Edge Devices.\n",
    "[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\n",
    "Chen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\n",
    "Compression Method for Object Detection Network for Edge Devices.\n",
    "[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\n",
    "Cao. Absorption Pruning of Deep Neural Network for Object Detection\n",
    "in Remote Sensing Imagery.\n",
    "[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\n",
    "and Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\n",
    "Distribution Detection: An Empirical Survey.\n",
    "[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\n",
    "Sahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\n",
    "[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\n",
    "Compress DNNs.\n",
    "[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\n",
    "derstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\n",
    "[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\n",
    "Proving the Lottery Ticket Hypothesis: Pruning is All You Need.\n",
    "[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\n",
    "Devices via Reconstruction-Based Channel Pruning.\n",
    "[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\n",
    "Determine Reliable Paths on Networks with Random and Correlated\n",
    "Link Travel Times.\n",
    "[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\n",
    "and D. Suganthi. Real-Time Object Detection on Edge Devices Using\n",
    "Mobile Neural Networks.\n",
    "[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\n",
    "Tang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\n",
    "Models.\n",
    "[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\n",
    "Li, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\n",
    "Deng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\n",
    "Detection of Tea Leaves with One Bud and Two Leaves Based on\n",
    "ShuffleNetv2-YOLOv5-Lite-E.\n",
    "Neuro Prune: An Adaptive Approach for Efficient\n",
    "Deep Neural Network Optimization on Edge\n",
    "Devices\n",
    "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
    "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
    "grk.cse@psgtech.ac.in\n",
    "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
    "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
    "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
    "Abstract—The optimization of deep neural networks for de-\n",
    "ployment on edge devices is a significant research area due to\n",
    "the demand for applications such as augmented reality, smart\n",
    "cameras, and autonomous navigation. However, deploying large\n",
    "deep learning models on edge devices poses challenges related to\n",
    "computational power, energy consumption, and latency. Pruning\n",
    "is a method to reduce the model size, accelerate inference, and\n",
    "save power. The objective of the paper is to propose the Neuro\n",
    "Prune algorithm and to apply it for the optimization of deep\n",
    "neural networks on edge devices. Efforts have been made to\n",
    "compare pruned and unpruned models. As a result, the pruned\n",
    "model has an accuracy increase of 0.22%.\n",
    "I. INTRODUCTION\n",
    "The rise of intelligent systems, ranging from autonomous\n",
    "drones to augmented reality (AR) devices and smart surveil-\n",
    "lance cameras, has intensified the need for efficient deep neural\n",
    "network deployment on edge devices with constrained com-\n",
    "putational resources. While advanced deep learning models\n",
    "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
    "their deployment on such devices is hindered by significant\n",
    "computational demands, large memory footprints, and high\n",
    "energy consumption. These challenges pose critical barriers to\n",
    "real-time applications and sustainable deployment.\n",
    "This paper builds on existing pruning techniques, includ-\n",
    "ing unstructured pruning, structured pruning, and approaches\n",
    "inspired by quantization, highlighting their strengths and lim-\n",
    "itations. In response to the challenges identified, we introduce\n",
    "Neuro Prune, a novel pruning approach that integrates princi-\n",
    "ples from reinforcement learning and the Lottery Ticket Hy-\n",
    "pothesis to achieve an optimal balance between accuracy, com-\n",
    "putational efficiency, and energy consumption. By integrating\n",
    "these principles, Neuro Prune provides a robust framework\n",
    "for optimizing deep neural networks for deployment on edge\n",
    "devices without sacrificing performance.\n",
    "Neuro Prune reduces model size and computational re-\n",
    "quirements while maintaining or even improving the model’s\n",
    "accuracy. These findings underscore the potential of Neuro\n",
    "Prune to enable real-time, energy-efficient deep learning on\n",
    "resource-constrained devices.\n",
    "Neuro Prune employs a systematic process to optimize\n",
    "neural networks for edge deployment:\n",
    "• Mask Initialization: Each layer’s weights are paired with\n",
    "a binary mask (original_mask), initialized to ones.\n",
    "• Activation Tracking: During forward passes over the\n",
    "dataset, the magnitudes of weight activations are mon-\n",
    "itored and recorded.\n",
    "• Normalize and Prune: The recorded activations are\n",
    "normalized, and a threshold based on the pruning fraction\n",
    "is computed. Using this threshold, the masks are dynam-\n",
    "ically updated to prune unimportant weights.\n",
    "• Evaluate and Log Metrics: After pruning, metrics such\n",
    "as model sparsity and accuracy are computed. A reward\n",
    "metric is derived to balance accuracy and sparsity using\n",
    "a tunable parameter, λweight.\n",
    "• Retrain the Model: The pruned model undergoes retrain-\n",
    "ing to improve the loss in accuracy while maintaining low\n",
    "computational requirements\n",
    "Neuro Prune’s unique combination of reinforcement learn-\n",
    "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
    "pothesis for identifying optimal subnetworks enables it to\n",
    "achieve remarkable efficiency. This approach ensures substan-\n",
    "tial reduction in model size and energy consumption while\n",
    "preserving accuracy and inference speed, making it an ideal\n",
    "solution for edge-based deep learning applications.\n",
    "Through extensive empirical evaluations and theoretical\n",
    "insights, this paper demonstrates how Neuro Pruning outper-\n",
    "forms traditional pruning methods, setting a new benchmark\n",
    "for sustainable, high-performance AI deployment on edge\n",
    "devices. By situating our work within the broader research\n",
    "context, we aim to offer a critical contribution to the ongoing\n",
    "evolution of neural network optimization techniques.\n",
    "II. BACKGROUND\n",
    "A. Need for Pruning Techniques\n",
    "Pruning is a crucial optimization technique for deploying\n",
    "deep neural networks in resource-constrained environments\n",
    "like healthcare. For example, wearable devices like smart-\n",
    "watches that monitor heart rates or portable EEG systems\n",
    "for brain activity analysis require lightweight models due to\n",
    "their limited computational power and memory. Pruning helps\n",
    "address these constraints by reducing the memory footprint\n",
    "and computational load, enabling efficient model deployment.\n",
    "Additionally, it minimizes energy consumption, which is vital\n",
    "for sustainable and real-time healthcare applications. Faster\n",
    "inference enabled by pruning is particularly beneficial for\n",
    "time-sensitive tasks like seizure detection or emergency di-\n",
    "agnostics. Moreover, pruning enhances model interpretability\n",
    "by retaining only the most critical components, which is\n",
    "essential for gaining trust and ensuring regulatory compliance\n",
    "in healthcare. By reducing the hardware and operational costs,\n",
    "pruning also makes deploying advanced AI solutions on edge\n",
    "devices more cost-effective and accessible.\n",
    "B. Types of Pruning Techniques\n",
    "Pruning methods are broadly classified based on granu-\n",
    "larity, timing, and approach. Granularity-based pruning in-\n",
    "cludes structured pruning, which removes entire components\n",
    "such as neurons or filters to streamline computations, and\n",
    "unstructured pruning, which eliminates individual weights\n",
    "for finer optimization, though it often requires specialized\n",
    "hardware. Timing-based pruning involves pre-training pruning,\n",
    "which optimizes models before training; post-training prun-\n",
    "ing, where redundant components are removed from trained\n",
    "models with possible retraining; and dynamic pruning during\n",
    "training, which adjusts models in real-time for improved\n",
    "adaptability. Metric-based pruning uses parameters like weight\n",
    "magnitude or gradient contribution to identify and remove\n",
    "less critical components, while application-specific pruning\n",
    "tailors strategies to specific tasks, such as optimizing for EEG\n",
    "signal analysis, or hardware constraints like GPUs. Together,\n",
    "these methods enable the development of efficient, scalable,\n",
    "and task-specific models, particularly suited for edge device\n",
    "deployment in healthcare and other domains.\n",
    "III. RELATED WORK\n",
    "Jielei Wang et al.[12] introduced an absorption pruning\n",
    "method for object detection in remote sensing imagery. This\n",
    "study achieves efficient compression with minimal accuracy\n",
    "loss but requires careful layer-wise pruning ratio tuning.\n",
    "Jan Muller et al., in their research[12], propose a neural\n",
    "network pruning method for multi-object tracking (MOT) that\n",
    "reduces model size by up to 70\n",
    "Liang Li et al., in their work[9], introduce a novel pruning\n",
    "method for DNNs that utilizes a self-adaptive mechanism\n",
    "based on weight sparsity ratios and a protective reconstruction\n",
    "mechanism. Their approach improves both model compres-\n",
    "sion and accuracy, outperforming state-of-the-art methods on\n",
    "CIFAR-10 and ImageNet datasets.\n",
    "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
    "learning with edge devices like Google Coral AI and Nvidia\n",
    "Jetson Nano. Their work revolutionizes computer vision and\n",
    "real-time tracking but raises energy concerns due to the\n",
    "computational demands of advanced algorithms.\n",
    "Satoru Koda et al.[7] explore how weight pruning in\n",
    "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
    "larly with Mahalanobis-based approaches, by improving global\n",
    "feature extraction and leveraging weights not critical for\n",
    "classification.\n",
    "Shvetha S Kumar et al. analyze and compare three pruning\n",
    "techniques—L1-norm filter pruning, channel pruning, and\n",
    "weight pruning—on CNNs for accuracy and inference time,\n",
    "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
    "pared to GTX1080 Ti in their study[8].\n",
    "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
    "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
    "on stochastic networks, extending it to related problems and\n",
    "demonstrating its efficiency on real-world networks in their\n",
    "study[13].\n",
    "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
    "Training outperforms Post-Training Quantization, and sparsity\n",
    "training with Optimal Reduction pruning improves YOLOv4’s\n",
    "efficiency and edge-device performance, despite potential\n",
    "high-pruning rate drawbacks.\n",
    "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
    "method for DNNs. This approach optimizes inference speed\n",
    "and resource efficiency on edge devices with minimal accuracy\n",
    "loss but requires careful tuning of layer-specific pruning ratios.\n",
    "Yongqi An et al.[1] introduce a retraining-free structured\n",
    "pruning framework for Large Language Models (LLMs). Their\n",
    "method incorporates a fluctuation-based pruning metric, adap-\n",
    "tive structure search, and a bias compensation mechanism to\n",
    "achieve efficient pruning. FLAP significantly enhances infer-\n",
    "ence speed and reduces model size without requiring retrain-\n",
    "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
    "and Wanda-sp on LLaMA models across various benchmarks.\n",
    "From referring to these research papers, we derived an\n",
    "idea for pruning on edge devices. Additionally, our proposed\n",
    "solution is inspired by the research of Jaron Maene et al.[10],\n",
    "which suggests that sparse subnetworks in dense networks can\n",
    "achieve similar accuracy when retrained. This study demon-\n",
    "strates stable training with linear mode connectivity, support-\n",
    "ing the idea that lottery tickets retrain to similar regions, but\n",
    "questions their independence from dense training and iterative\n",
    "pruning.\n",
    "Eran Malach et al.[11] further strengthen the lottery ticket\n",
    "hypothesis by proving that over-parameterized neural networks\n",
    "with random weights always contain a subnetwork matching\n",
    "the accuracy of a target network, without additional training.\n",
    "IV. NEUROLOGICAL PRUNING APPROACH\n",
    "The Neurological Pruning Algorithm is designed to opti-\n",
    "mize neural networks by iteratively pruning less important\n",
    "weights while maintaining high performance. Unlike tradi-\n",
    "tional methods, this approach incorporates adaptive, layer-\n",
    "specific pruning thresholds that dynamically adjust based\n",
    "on weight activations, ensuring efficient model optimization\n",
    "across various stages of training. Drawing inspiration from\n",
    "the Lottery Ticket Hypothesis, which suggests that sparse\n",
    "subnetworks can perform similarly to their dense counterparts\n",
    "when retrained, the algorithm selectively deactivates weights\n",
    "using binary masks to enhance computational efficiency with-\n",
    "out sacrificing accuracy.\n",
    "The algorithm operates in several epochs, during which\n",
    "weight activations are evaluated. Based on a quantile-based\n",
    "strategy, pruning thresholds are established for each layer, en-\n",
    "suring that only weights contributing minimally to the model’s\n",
    "output are removed. After pruning, the model undergoes fine-\n",
    "tuning to restore any lost accuracy and further optimize the\n",
    "network. This step enhances the adaptability and scalability\n",
    "of the pruning method, making it suitable for deployment in\n",
    "resource-constrained environments\n",
    "A. Key Characteristics and Advantages\n",
    "The Neurological Pruning Algorithm boasts several key\n",
    "characteristics that distinguish it from conventional pruning\n",
    "techniques. One notable feature is its layer-specific pruning,\n",
    "which evaluates the importance of weights individually for\n",
    "each layer, leading to a more nuanced and efficient sparsifi-\n",
    "cation process. This approach prevents the model from losing\n",
    "essential features that may otherwise be pruned by a global\n",
    "approach.\n",
    "The algorithm’s use of quantile-based thresholding en-\n",
    "sures that only weights with the least contribution to the\n",
    "model’s output are pruned. This fine-tuned method avoids\n",
    "performance degradation by carefully targeting weights that\n",
    "are less significant to the network’s overall functionality.\n",
    "Furthermore, the pruning process is iterative, allowing for\n",
    "continuous model evaluation and real-time adjustments based\n",
    "on performance feedback, ensuring that accuracy is retained\n",
    "even as sparsity increases.\n",
    "Additionally, the algorithm maintains a careful balance\n",
    "between sparsity and accuracy, enabling a reduction in the\n",
    "model’s computational complexity while preserving its predic-\n",
    "tive capabilities. This characteristic is particularly important\n",
    "for applications requiring rapid inference times, such as in\n",
    "real-time healthcare systems and edge computing scenarios,\n",
    "where both energy efficiency and computational power are\n",
    "crucial.\n",
    "B. Impact on Model Performance\n",
    "The Neurological Pruning Algorithm significantly improves\n",
    "the efficiency of neural networks, making them more suitable\n",
    "for deployment on edge devices. Key performance metrics\n",
    "such as compression ratio (CR) and accuracy retention\n",
    "(AR) are critical indicators of the algorithm’s effectiveness.\n",
    "For instance, in evaluating a ResNet-18 model, the algorithm\n",
    "achieved a CR of 1, indicating that while the number of\n",
    "parameters remained the same, the model’s efficiency im-\n",
    "proved through selective pruning, reducing computational load\n",
    "without sacrificing performance.\n",
    "Moreover, the algorithm demonstrates the ability to achieve\n",
    "high accuracy retention, with models maintaining or even\n",
    "slightly improving their predictive performance post-pruning.\n",
    "This outcome highlights the algorithm’s capability to effi-\n",
    "ciently remove redundant weights while preserving the core\n",
    "functionality of the network. The result is a model that is\n",
    "not only more computationally efficient but also adaptable\n",
    "across different architectures and datasets, making it suit-\n",
    "able for a wide range of applications, from image classification\n",
    "to natural language processing .\n",
    "In practical terms, the Neurological Pruning Algorithm en-\n",
    "ables faster inference times and lower energy consumption,\n",
    "two crucial aspects for deploying AI models in resource-\n",
    "constrained environments. As such, it holds significant poten-\n",
    "tial for improving the viability of real-time, energy-efficient\n",
    "AI applications, particularly in fields like healthcare, where\n",
    "both speed and sustainability are essential.\n",
    "V. NEUROLOGICAL PRUNING ALGORITHM\n",
    "The Neurological Pruning Algorithm is a structured and\n",
    "adaptive methodology designed to optimize neural networks\n",
    "for resource-constrained environments. This algorithm goes\n",
    "beyond traditional pruning approaches by incorporating layer-\n",
    "specific pruning thresholds that dynamically adjust to the\n",
    "importance of weight activations within each layer. Drawing\n",
    "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
    "inforcement learning, the algorithm strikes an optimal balance\n",
    "between model sparsity and accuracy .\n",
    "A. Algorithm Description\n",
    "The Neurological Pruning Algorithm iteratively refines the\n",
    "neural network through a series of training epochs. During\n",
    "each epoch, weight activations are tracked, normalized, and\n",
    "analyzed to identify low-importance weights. These weights\n",
    "are selectively deactivated using binary masks, ensuring com-\n",
    "putational efficiency without compromising the network’s pre-\n",
    "dictive performance. The algorithm further integrates a fine-\n",
    "tuning phase, enabling the pruned network to recover and\n",
    "enhance its accuracy. The detailed process is presented in\n",
    "Algorithm 1.\n",
    "By dynamically adjusting to layer-specific characteristics\n",
    "and maintaining performance metrics, the Neurological Prun-\n",
    "ing Algorithm ensures a significant reduction in computational\n",
    "and memory requirements, paving the way for efficient deploy-\n",
    "ment of deep learning models on edge devices .\n",
    "B. Input Parameters and Output\n",
    "Inputs:\n",
    "• Pretrained model (M)\n",
    "• Dataset (D)\n",
    "• Prune fraction (p)\n",
    "• Total epochs (E)\n",
    "Output:\n",
    "• Pruned and fine-tuned model (Mpruned)\n",
    "C. Key Features and Advantages\n",
    "The Neurological Pruning Algorithm is designed with sev-\n",
    "eral distinctive features that contribute to its effectiveness\n",
    "in optimizing neural network performance. These features\n",
    "include:\n",
    "Algorithm 1 Simplified Neurological Pruning Algorithm\n",
    "Require: Pretrained model M, Dataset D, Prune fraction p,\n",
    "Total epochs E\n",
    "Ensure: Pruned and fine-tuned model Mpruned\n",
    "1: Initialize binary masks M with all ones\n",
    "2: for each epoch e = 1 to E do\n",
    "3:\n",
    "Compute weight activations A during forward passes\n",
    "4:\n",
    "for each layer i in M do\n",
    "5:\n",
    "Compute pruning threshold Ti as p-quantile of Ai\n",
    "6:\n",
    "Update mask Mi: retain weights with Ai > Ti\n",
    "7:\n",
    "Apply mask to layer: θi ←θi · Mi\n",
    "8:\n",
    "end for\n",
    "9:\n",
    "Evaluate model performance and update metrics\n",
    "10: end for\n",
    "11: Fine-tune Mpruned on D\n",
    "12: return Mpruned\n",
    "• Layer-specific Pruning: The algorithm applies pruning\n",
    "on a per-layer basis, evaluating the weight activations\n",
    "within each layer individually. This approach ensures that\n",
    "the sparsity introduced by pruning is well-distributed and\n",
    "context-dependent, facilitating more nuanced control over\n",
    "model optimization .\n",
    "• Quantile-based Thresholding: A quantile-based strat-\n",
    "egy is employed to determine the pruning threshold for\n",
    "each layer. This method ensures that only weights with\n",
    "minimal contribution to the network’s output are pruned,\n",
    "thereby reducing the risk of adversely affecting model\n",
    "accuracy while enhancing sparsity .\n",
    "• Iterative Pruning and Evaluation: The pruning pro-\n",
    "cess follows an iterative approach, enabling continuous\n",
    "assessment of the model’s performance after each pruning\n",
    "step. This ensures dynamic adjustments to maintain a bal-\n",
    "ance between model sparsity and predictive performance\n",
    "throughout training.\n",
    "• Preservation of Model Accuracy: By adjusting pruning\n",
    "thresholds in response to ongoing performance metrics,\n",
    "the algorithm ensures that model accuracy is not com-\n",
    "promised. The careful tuning of these thresholds allows\n",
    "for significant sparsity while maintaining the robustness\n",
    "of the network’s predictive capabilities .\n",
    "D. Impact on Neural Network Performance\n",
    "The Neurological Pruning Algorithm has been extensively\n",
    "evaluated across various benchmark datasets and model archi-\n",
    "tectures, demonstrating its capacity to achieve the following\n",
    "outcomes:\n",
    "• High Sparsity with Minimal Accuracy Loss: The algo-\n",
    "rithm effectively prunes unnecessary weights, achieving\n",
    "high sparsity levels without causing significant degrada-\n",
    "tion in model accuracy. This outcome is critical for the\n",
    "efficient deployment of models in resource-constrained\n",
    "environments.\n",
    "• Reduced Computational Complexity: By sparsifying\n",
    "the weight matrices, the algorithm reduces the compu-\n",
    "tational load during inference, which translates to faster\n",
    "model execution and lower resource consumption. This\n",
    "characteristic is especially beneficial in edge computing\n",
    "and real-time applications .\n",
    "• Broad Applicability: The algorithm is highly flexible\n",
    "and can be adapted to various neural network architec-\n",
    "tures and datasets, making it suitable for a wide range\n",
    "of tasks, from image recognition to natural language\n",
    "processing .\n",
    "VI. EVALUATION METRICS\n",
    "A. Performance Comparison\n",
    "Fig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\n",
    "TABLE I\n",
    "COMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\n",
    "Metric\n",
    "Unpruned\n",
    "Model\n",
    "Pruned\n",
    "Model\n",
    "Accuracy\n",
    "0.9562\n",
    "0.9583\n",
    "Train Loss\n",
    "0.1414\n",
    "0.1625\n",
    "Inference\n",
    "Time\n",
    "(s)\n",
    "0.0318\n",
    "0.0314\n",
    "No of Parame-\n",
    "ters(million)\n",
    "11.7\n",
    "11.7\n",
    "B. Evaluation Metrics Calculation\n",
    "The key metrics used to evaluate the proposed pruning\n",
    "technique are as follows:\n",
    "Compression Ratio (CR): The Compression Ratio (CR) is\n",
    "calculated as:\n",
    "CR = Total Parameters After Pruning\n",
    "Total Parameters Before Pruning\n",
    "(1)\n",
    "CR = 11.7M\n",
    "11.7M = 1\n",
    "Accuracy Retention (AR): The Accuracy Retention (AR) is\n",
    "calculated as:\n",
    "AR =\n",
    "\u0012Accuracy After Pruning −Baseline Accuracy\n",
    "Baseline Accuracy\n",
    "\u0013\n",
    "×100\n",
    "(2)\n",
    "AR =\n",
    "\u00120.958284 −0.956206\n",
    "0.956206\n",
    "\u0013\n",
    "× 100 = 0.22%\n",
    "This result indicates that the pruned model achieves an ac-\n",
    "curacy improvement of 0.22% when compared to unpruned\n",
    "model.\n",
    "C. Illustration of Results\n",
    "Fig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\n",
    "18.\n",
    "D. Discussion\n",
    "The evaluation metrics highlight the effectiveness of the\n",
    "proposed Neurological Pruning method on the ResNet-18\n",
    "model. The Compression Ratio (CR) remains 1, indicating\n",
    "no reduction in the total number of parameters, which suggests\n",
    "that this approach optimizes the neural network by selectively\n",
    "pruning connections rather than reducing the overall parameter\n",
    "count. The Accuracy Retention (AR), which is approximately\n",
    "0.22%, demonstrates that the Neurological Pruning technique\n",
    "successfully improved the accuracy compared to the baseline\n",
    "model.\n",
    "These results validate the potential of Neurological Pruning\n",
    "as an innovative approach to optimizing deep learning models.\n",
    "By maintaining accuracy while likely enhancing sparsity and\n",
    "computational efficiency, this method presents a promising di-\n",
    "rection for achieving high-performance models under resource\n",
    "constraints.\n",
    "VII. CONCLUSION\n",
    "In summary, the Neurological Pruning Algorithm provides\n",
    "a systematic and principled approach to optimizing neural\n",
    "networks. By balancing the trade-off between sparsity and\n",
    "accuracy, it enables significant reductions in computational\n",
    "complexity without sacrificing the network’s predictive per-\n",
    "formance. Future work will focus on extending the algorithm\n",
    "to dynamic network architectures and incorporating advanced\n",
    "regularization techniques to further enhance its robustness and\n",
    "adaptability in diverse applications.\n",
    "REFERENCES\n",
    "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\n",
    "Fluctuation-based Adaptive Structured Pruning for Large Language\n",
    "Models.\n",
    "[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\n",
    "Simplification of Deep Neural Network-Based Object Detector for Real-\n",
    "Time Edge Computing.\n",
    "[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\n",
    "Devices Object Detection by Filter Pruning.\n",
    "[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\n",
    "and Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\n",
    "Tracking Algorithms in Edge Devices.\n",
    "[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\n",
    "Chen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\n",
    "Compression Method for Object Detection Network for Edge Devices.\n",
    "[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\n",
    "Cao. Absorption Pruning of Deep Neural Network for Object Detection\n",
    "in Remote Sensing Imagery.\n",
    "[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\n",
    "and Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\n",
    "Distribution Detection: An Empirical Survey.\n",
    "[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\n",
    "Sahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\n",
    "[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\n",
    "Compress DNNs.\n",
    "[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\n",
    "derstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\n",
    "[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\n",
    "Proving the Lottery Ticket Hypothesis: Pruning is All You Need.\n",
    "[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\n",
    "Devices via Reconstruction-Based Channel Pruning.\n",
    "[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\n",
    "Determine Reliable Paths on Networks with Random and Correlated\n",
    "Link Travel Times.\n",
    "[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\n",
    "and D. Suganthi. Real-Time Object Detection on Edge Devices Using\n",
    "Mobile Neural Networks.\n",
    "[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\n",
    "Tang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\n",
    "Models.\n",
    "[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\n",
    "Li, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\n",
    "Deng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\n",
    "Detection of Tea Leaves with One Bud and Two Leaves Based on\n",
    "ShuffleNetv2-YOLOv5-Lite-E.\n",
    "\"\"\", \"metadata\": {\"source\": \"geo_facts.txt\", \"page\": 1}},\n",
    "    {\"content\": \"The Great Wall of China is located in China and is a famous landmark.\", \"metadata\": {\"source\": \"world_landmarks.txt\", \"page\": 5}},\n",
    "    {\"content\": \"Paris is known for the Eiffel Tower.\", \"metadata\": {\"source\": \"geo_facts.txt\", \"page\": 2}},\n",
    "]\n",
    "\n",
    "# Convert raw data to LangChain Document objects\n",
    "documents = [Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"]) for doc in documents_data]\n",
    "\n",
    "# Create FAISS vector store directly from documents\n",
    "# This handles embedding the texts and building the index internally\n",
    "try:\n",
    "    vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "    print(\"FAISS vector store created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating FAISS vector store: {e}\")\n",
    "# Exit if vector store creation fails\n",
    "\n",
    "# Step 4: Load Gemini model for text generation\n",
    "\n",
    "# Step 5: Create the Retrieval-Augmented Generation pipeline\n",
    "print(\"Creating RAG chain...\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\", # Other options: \"mmr\", \"similarity_score_threshold\"\n",
    "    search_kwargs={'k': 2} # Retrieve top 2 relevant documents\n",
    ")\n",
    "\n",
    "# Use the recommended from_chain_type method\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # Options: \"stuff\", \"map_reduce\", \"refine\", \"map_rerank\"\n",
    "                        # \"stuff\" puts all retrieved docs into the context\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True # Include source documents in the output\n",
    "    # chain_type_kwargs={\"prompt\": YOUR_CUSTOM_PROMPT} # Optional: customize prompt\n",
    ")\n",
    "print(\"RAG chain created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c4dcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying the RAG system with: 'who is surya narayanaa'\n",
      "\n",
      "--- RAG System Output ---\n",
      "Answer: Surya Narayanaa N T is listed as a co-author (5th author) of the research paper \"Neuro Prune: An Adaptive Approach for Efficient Deep Neural Network Optimization on Edge Devices.\" He is a student (BE-CSE (AI&ML)) at PSG College of Technology. His email address is suryanarayanaant@gmail.com.\n",
      "\n",
      "Sources:\n",
      "  Source 1:\n",
      "    Content: \n",
      "        Neuro Prune: An Adaptive Approach for Efficient\n",
      "Deep Neural Network Optimization on Edge\n",
      "Devices\n",
      "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
      "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
      "grk.cse@psgtech.ac.in\n",
      "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
      "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
      "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
      "Abstract—The optimization of deep neural networks for de-\n",
      "ployment on edge devices is a significant research area due to\n",
      "the demand for applications such as augmented reality, smart\n",
      "cameras, and autonomous navigation. However, deploying large\n",
      "deep learning models on edge devices poses challenges related to\n",
      "computational power, energy consumption, and latency. Pruning\n",
      "is a method to reduce the model size, accelerate inference, and\n",
      "save power. The objective of the paper is to propose the Neuro\n",
      "Prune algorithm and to apply it for the optimization of deep\n",
      "neural networks on edge devices. Efforts have been made to\n",
      "compare pruned and unpruned models. As a result, the pruned\n",
      "model has an accuracy increase of 0.22%.\n",
      "I. INTRODUCTION\n",
      "The rise of intelligent systems, ranging from autonomous\n",
      "drones to augmented reality (AR) devices and smart surveil-\n",
      "lance cameras, has intensified the need for efficient deep neural\n",
      "network deployment on edge devices with constrained com-\n",
      "putational resources. While advanced deep learning models\n",
      "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
      "their deployment on such devices is hindered by significant\n",
      "computational demands, large memory footprints, and high\n",
      "energy consumption. These challenges pose critical barriers to\n",
      "real-time applications and sustainable deployment.\n",
      "This paper builds on existing pruning techniques, includ-\n",
      "ing unstructured pruning, structured pruning, and approaches\n",
      "inspired by quantization, highlighting their strengths and lim-\n",
      "itations. In response to the challenges identified, we introduce\n",
      "Neuro Prune, a novel pruning approach that integrates princi-\n",
      "ples from reinforcement learning and the Lottery Ticket Hy-\n",
      "pothesis to achieve an optimal balance between accuracy, com-\n",
      "putational efficiency, and energy consumption. By integrating\n",
      "these principles, Neuro Prune provides a robust framework\n",
      "for optimizing deep neural networks for deployment on edge\n",
      "devices without sacrificing performance.\n",
      "Neuro Prune reduces model size and computational re-\n",
      "quirements while maintaining or even improving the model’s\n",
      "accuracy. These findings underscore the potential of Neuro\n",
      "Prune to enable real-time, energy-efficient deep learning on\n",
      "resource-constrained devices.\n",
      "Neuro Prune employs a systematic process to optimize\n",
      "neural networks for edge deployment:\n",
      "• Mask Initialization: Each layer’s weights are paired with\n",
      "a binary mask (original_mask), initialized to ones.\n",
      "• Activation Tracking: During forward passes over the\n",
      "dataset, the magnitudes of weight activations are mon-\n",
      "itored and recorded.\n",
      "• Normalize and Prune: The recorded activations are\n",
      "normalized, and a threshold based on the pruning fraction\n",
      "is computed. Using this threshold, the masks are dynam-\n",
      "ically updated to prune unimportant weights.\n",
      "• Evaluate and Log Metrics: After pruning, metrics such\n",
      "as model sparsity and accuracy are computed. A reward\n",
      "metric is derived to balance accuracy and sparsity using\n",
      "a tunable parameter, λweight.\n",
      "• Retrain the Model: The pruned model undergoes retrain-\n",
      "ing to improve the loss in accuracy while maintaining low\n",
      "computational requirements\n",
      "Neuro Prune’s unique combination of reinforcement learn-\n",
      "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
      "pothesis for identifying optimal subnetworks enables it to\n",
      "achieve remarkable efficiency. This approach ensures substan-\n",
      "tial reduction in model size and energy consumption while\n",
      "preserving accuracy and inference speed, making it an ideal\n",
      "solution for edge-based deep learning applications.\n",
      "Through extensive empirical evaluations and theoretical\n",
      "insights, this paper demonstrates how Neuro Pruning outper-\n",
      "forms traditional pruning methods, setting a new benchmark\n",
      "for sustainable, high-performance AI deployment on edge\n",
      "devices. By situating our work within the broader research\n",
      "context, we aim to offer a critical contribution to the ongoing\n",
      "evolution of neural network optimization techniques.\n",
      "II. BACKGROUND\n",
      "A. Need for Pruning Techniques\n",
      "Pruning is a crucial optimization technique for deploying\n",
      "deep neural networks in resource-constrained environments\n",
      "like healthcare. For example, wearable devices like smart-\n",
      "watches that monitor heart rates or portable EEG systems\n",
      "for brain activity analysis require lightweight models due to\n",
      "their limited computational power and memory. Pruning helps\n",
      "address these constraints by reducing the memory footprint\n",
      "and computational load, enabling efficient model deployment.\n",
      "Additionally, it minimizes energy consumption, which is vital\n",
      "for sustainable and real-time healthcare applications. Faster\n",
      "inference enabled by pruning is particularly beneficial for\n",
      "time-sensitive tasks like seizure detection or emergency di-\n",
      "agnostics. Moreover, pruning enhances model interpretability\n",
      "by retaining only the most critical components, which is\n",
      "essential for gaining trust and ensuring regulatory compliance\n",
      "in healthcare. By reducing the hardware and operational costs,\n",
      "pruning also makes deploying advanced AI solutions on edge\n",
      "devices more cost-effective and accessible.\n",
      "B. Types of Pruning Techniques\n",
      "Pruning methods are broadly classified based on granu-\n",
      "larity, timing, and approach. Granularity-based pruning in-\n",
      "cludes structured pruning, which removes entire components\n",
      "such as neurons or filters to streamline computations, and\n",
      "unstructured pruning, which eliminates individual weights\n",
      "for finer optimization, though it often requires specialized\n",
      "hardware. Timing-based pruning involves pre-training pruning,\n",
      "which optimizes models before training; post-training prun-\n",
      "ing, where redundant components are removed from trained\n",
      "models with possible retraining; and dynamic pruning during\n",
      "training, which adjusts models in real-time for improved\n",
      "adaptability. Metric-based pruning uses parameters like weight\n",
      "magnitude or gradient contribution to identify and remove\n",
      "less critical components, while application-specific pruning\n",
      "tailors strategies to specific tasks, such as optimizing for EEG\n",
      "signal analysis, or hardware constraints like GPUs. Together,\n",
      "these methods enable the development of efficient, scalable,\n",
      "and task-specific models, particularly suited for edge device\n",
      "deployment in healthcare and other domains.\n",
      "III. RELATED WORK\n",
      "Jielei Wang et al.[12] introduced an absorption pruning\n",
      "method for object detection in remote sensing imagery. This\n",
      "study achieves efficient compression with minimal accuracy\n",
      "loss but requires careful layer-wise pruning ratio tuning.\n",
      "Jan Muller et al., in their research[12], propose a neural\n",
      "network pruning method for multi-object tracking (MOT) that\n",
      "reduces model size by up to 70\n",
      "Liang Li et al., in their work[9], introduce a novel pruning\n",
      "method for DNNs that utilizes a self-adaptive mechanism\n",
      "based on weight sparsity ratios and a protective reconstruction\n",
      "mechanism. Their approach improves both model compres-\n",
      "sion and accuracy, outperforming state-of-the-art methods on\n",
      "CIFAR-10 and ImageNet datasets.\n",
      "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
      "learning with edge devices like Google Coral AI and Nvidia\n",
      "Jetson Nano. Their work revolutionizes computer vision and\n",
      "real-time tracking but raises energy concerns due to the\n",
      "computational demands of advanced algorithms.\n",
      "Satoru Koda et al.[7] explore how weight pruning in\n",
      "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
      "larly with Mahalanobis-based approaches, by improving global\n",
      "feature extraction and leveraging weights not critical for\n",
      "classification.\n",
      "Shvetha S Kumar et al. analyze and compare three pruning\n",
      "techniques—L1-norm filter pruning, channel pruning, and\n",
      "weight pruning—on CNNs for accuracy and inference time,\n",
      "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
      "pared to GTX1080 Ti in their study[8].\n",
      "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
      "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
      "on stochastic networks, extending it to related problems and\n",
      "demonstrating its efficiency on real-world networks in their\n",
      "study[13].\n",
      "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
      "Training outperforms Post-Training Quantization, and sparsity\n",
      "training with Optimal Reduction pruning improves YOLOv4’s\n",
      "efficiency and edge-device performance, despite potential\n",
      "high-pruning rate drawbacks.\n",
      "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
      "method for DNNs. This approach optimizes inference speed\n",
      "and resource efficiency on edge devices with minimal accuracy\n",
      "loss but requires careful tuning of layer-specific pruning ratios.\n",
      "Yongqi An et al.[1] introduce a retraining-free structured\n",
      "pruning framework for Large Language Models (LLMs). Their\n",
      "method incorporates a fluctuation-based pruning metric, adap-\n",
      "tive structure search, and a bias compensation mechanism to\n",
      "achieve efficient pruning. FLAP significantly enhances infer-\n",
      "ence speed and reduces model size without requiring retrain-\n",
      "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
      "and Wanda-sp on LLaMA models across various benchmarks.\n",
      "From referring to these research papers, we derived an\n",
      "idea for pruning on edge devices. Additionally, our proposed\n",
      "solution is inspired by the research of Jaron Maene et al.[10],\n",
      "which suggests that sparse subnetworks in dense networks can\n",
      "achieve similar accuracy when retrained. This study demon-\n",
      "strates stable training with linear mode connectivity, support-\n",
      "ing the idea that lottery tickets retrain to similar regions, but\n",
      "questions their independence from dense training and iterative\n",
      "pruning.\n",
      "Eran Malach et al.[11] further strengthen the lottery ticket\n",
      "hypothesis by proving that over-parameterized neural networks\n",
      "with random weights always contain a subnetwork matching\n",
      "the accuracy of a target network, without additional training.\n",
      "IV. NEUROLOGICAL PRUNING APPROACH\n",
      "The Neurological Pruning Algorithm is designed to opti-\n",
      "mize neural networks by iteratively pruning less important\n",
      "weights while maintaining high performance. Unlike tradi-\n",
      "tional methods, this approach incorporates adaptive, layer-\n",
      "specific pruning thresholds that dynamically adjust based\n",
      "on weight activations, ensuring efficient model optimization\n",
      "across various stages of training. Drawing inspiration from\n",
      "the Lottery Ticket Hypothesis, which suggests that sparse\n",
      "subnetworks can perform similarly to their dense counterparts\n",
      "when retrained, the algorithm selectively deactivates weights\n",
      "using binary masks to enhance computational efficiency with-\n",
      "out sacrificing accuracy.\n",
      "The algorithm operates in several epochs, during which\n",
      "weight activations are evaluated. Based on a quantile-based\n",
      "strategy, pruning thresholds are established for each layer, en-\n",
      "suring that only weights contributing minimally to the model’s\n",
      "output are removed. After pruning, the model undergoes fine-\n",
      "tuning to restore any lost accuracy and further optimize the\n",
      "network. This step enhances the adaptability and scalability\n",
      "of the pruning method, making it suitable for deployment in\n",
      "resource-constrained environments\n",
      "A. Key Characteristics and Advantages\n",
      "The Neurological Pruning Algorithm boasts several key\n",
      "characteristics that distinguish it from conventional pruning\n",
      "techniques. One notable feature is its layer-specific pruning,\n",
      "which evaluates the importance of weights individually for\n",
      "each layer, leading to a more nuanced and efficient sparsifi-\n",
      "cation process. This approach prevents the model from losing\n",
      "essential features that may otherwise be pruned by a global\n",
      "approach.\n",
      "The algorithm’s use of quantile-based thresholding en-\n",
      "sures that only weights with the least contribution to the\n",
      "model’s output are pruned. This fine-tuned method avoids\n",
      "performance degradation by carefully targeting weights that\n",
      "are less significant to the network’s overall functionality.\n",
      "Furthermore, the pruning process is iterative, allowing for\n",
      "continuous model evaluation and real-time adjustments based\n",
      "on performance feedback, ensuring that accuracy is retained\n",
      "even as sparsity increases.\n",
      "Additionally, the algorithm maintains a careful balance\n",
      "between sparsity and accuracy, enabling a reduction in the\n",
      "model’s computational complexity while preserving its predic-\n",
      "tive capabilities. This characteristic is particularly important\n",
      "for applications requiring rapid inference times, such as in\n",
      "real-time healthcare systems and edge computing scenarios,\n",
      "where both energy efficiency and computational power are\n",
      "crucial.\n",
      "B. Impact on Model Performance\n",
      "The Neurological Pruning Algorithm significantly improves\n",
      "the efficiency of neural networks, making them more suitable\n",
      "for deployment on edge devices. Key performance metrics\n",
      "such as compression ratio (CR) and accuracy retention\n",
      "(AR) are critical indicators of the algorithm’s effectiveness.\n",
      "For instance, in evaluating a ResNet-18 model, the algorithm\n",
      "achieved a CR of 1, indicating that while the number of\n",
      "parameters remained the same, the model’s efficiency im-\n",
      "proved through selective pruning, reducing computational load\n",
      "without sacrificing performance.\n",
      "Moreover, the algorithm demonstrates the ability to achieve\n",
      "high accuracy retention, with models maintaining or even\n",
      "slightly improving their predictive performance post-pruning.\n",
      "This outcome highlights the algorithm’s capability to effi-\n",
      "ciently remove redundant weights while preserving the core\n",
      "functionality of the network. The result is a model that is\n",
      "not only more computationally efficient but also adaptable\n",
      "across different architectures and datasets, making it suit-\n",
      "able for a wide range of applications, from image classification\n",
      "to natural language processing .\n",
      "In practical terms, the Neurological Pruning Algorithm en-\n",
      "ables faster inference times and lower energy consumption,\n",
      "two crucial aspects for deploying AI models in resource-\n",
      "constrained environments. As such, it holds significant poten-\n",
      "tial for improving the viability of real-time, energy-efficient\n",
      "AI applications, particularly in fields like healthcare, where\n",
      "both speed and sustainability are essential.\n",
      "V. NEUROLOGICAL PRUNING ALGORITHM\n",
      "The Neurological Pruning Algorithm is a structured and\n",
      "adaptive methodology designed to optimize neural networks\n",
      "for resource-constrained environments. This algorithm goes\n",
      "beyond traditional pruning approaches by incorporating layer-\n",
      "specific pruning thresholds that dynamically adjust to the\n",
      "importance of weight activations within each layer. Drawing\n",
      "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
      "inforcement learning, the algorithm strikes an optimal balance\n",
      "between model sparsity and accuracy .\n",
      "A. Algorithm Description\n",
      "The Neurological Pruning Algorithm iteratively refines the\n",
      "neural network through a series of training epochs. During\n",
      "each epoch, weight activations are tracked, normalized, and\n",
      "analyzed to identify low-importance weights. These weights\n",
      "are selectively deactivated using binary masks, ensuring com-\n",
      "putational efficiency without compromising the network’s pre-\n",
      "dictive performance. The algorithm further integrates a fine-\n",
      "tuning phase, enabling the pruned network to recover and\n",
      "enhance its accuracy. The detailed process is presented in\n",
      "Algorithm 1.\n",
      "By dynamically adjusting to layer-specific characteristics\n",
      "and maintaining performance metrics, the Neurological Prun-\n",
      "ing Algorithm ensures a significant reduction in computational\n",
      "and memory requirements, paving the way for efficient deploy-\n",
      "ment of deep learning models on edge devices .\n",
      "B. Input Parameters and Output\n",
      "Inputs:\n",
      "• Pretrained model (M)\n",
      "• Dataset (D)\n",
      "• Prune fraction (p)\n",
      "• Total epochs (E)\n",
      "Output:\n",
      "• Pruned and fine-tuned model (Mpruned)\n",
      "C. Key Features and Advantages\n",
      "The Neurological Pruning Algorithm is designed with sev-\n",
      "eral distinctive features that contribute to its effectiveness\n",
      "in optimizing neural network performance. These features\n",
      "include:\n",
      "Algorithm 1 Simplified Neurological Pruning Algorithm\n",
      "Require: Pretrained model M, Dataset D, Prune fraction p,\n",
      "Total epochs E\n",
      "Ensure: Pruned and fine-tuned model Mpruned\n",
      "1: Initialize binary masks M with all ones\n",
      "2: for each epoch e = 1 to E do\n",
      "3:\n",
      "Compute weight activations A during forward passes\n",
      "4:\n",
      "for each layer i in M do\n",
      "5:\n",
      "Compute pruning threshold Ti as p-quantile of Ai\n",
      "6:\n",
      "Update mask Mi: retain weights with Ai > Ti\n",
      "7:\n",
      "Apply mask to layer: θi ←θi · Mi\n",
      "8:\n",
      "end for\n",
      "9:\n",
      "Evaluate model performance and update metrics\n",
      "10: end for\n",
      "11: Fine-tune Mpruned on D\n",
      "12: return Mpruned\n",
      "• Layer-specific Pruning: The algorithm applies pruning\n",
      "on a per-layer basis, evaluating the weight activations\n",
      "within each layer individually. This approach ensures that\n",
      "the sparsity introduced by pruning is well-distributed and\n",
      "context-dependent, facilitating more nuanced control over\n",
      "model optimization .\n",
      "• Quantile-based Thresholding: A quantile-based strat-\n",
      "egy is employed to determine the pruning threshold for\n",
      "each layer. This method ensures that only weights with\n",
      "minimal contribution to the network’s output are pruned,\n",
      "thereby reducing the risk of adversely affecting model\n",
      "accuracy while enhancing sparsity .\n",
      "• Iterative Pruning and Evaluation: The pruning pro-\n",
      "cess follows an iterative approach, enabling continuous\n",
      "assessment of the model’s performance after each pruning\n",
      "step. This ensures dynamic adjustments to maintain a bal-\n",
      "ance between model sparsity and predictive performance\n",
      "throughout training.\n",
      "• Preservation of Model Accuracy: By adjusting pruning\n",
      "thresholds in response to ongoing performance metrics,\n",
      "the algorithm ensures that model accuracy is not com-\n",
      "promised. The careful tuning of these thresholds allows\n",
      "for significant sparsity while maintaining the robustness\n",
      "of the network’s predictive capabilities .\n",
      "D. Impact on Neural Network Performance\n",
      "The Neurological Pruning Algorithm has been extensively\n",
      "evaluated across various benchmark datasets and model archi-\n",
      "tectures, demonstrating its capacity to achieve the following\n",
      "outcomes:\n",
      "• High Sparsity with Minimal Accuracy Loss: The algo-\n",
      "rithm effectively prunes unnecessary weights, achieving\n",
      "high sparsity levels without causing significant degrada-\n",
      "tion in model accuracy. This outcome is critical for the\n",
      "efficient deployment of models in resource-constrained\n",
      "environments.\n",
      "• Reduced Computational Complexity: By sparsifying\n",
      "the weight matrices, the algorithm reduces the compu-\n",
      "tational load during inference, which translates to faster\n",
      "model execution and lower resource consumption. This\n",
      "characteristic is especially beneficial in edge computing\n",
      "and real-time applications .\n",
      "• Broad Applicability: The algorithm is highly flexible\n",
      "and can be adapted to various neural network architec-\n",
      "tures and datasets, making it suitable for a wide range\n",
      "of tasks, from image recognition to natural language\n",
      "processing .\n",
      "VI. EVALUATION METRICS\n",
      "A. Performance Comparison\n",
      "Fig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\n",
      "TABLE I\n",
      "COMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\n",
      "Metric\n",
      "Unpruned\n",
      "Model\n",
      "Pruned\n",
      "Model\n",
      "Accuracy\n",
      "0.9562\n",
      "0.9583\n",
      "Train Loss\n",
      "0.1414\n",
      "0.1625\n",
      "Inference\n",
      "Time\n",
      "(s)\n",
      "0.0318\n",
      "0.0314\n",
      "No of Parame-\n",
      "ters(million)\n",
      "11.7\n",
      "11.7\n",
      "B. Evaluation Metrics Calculation\n",
      "The key metrics used to evaluate the proposed pruning\n",
      "technique are as follows:\n",
      "Compression Ratio (CR): The Compression Ratio (CR) is\n",
      "calculated as:\n",
      "CR = Total Parameters After Pruning\n",
      "Total Parameters Before Pruning\n",
      "(1)\n",
      "CR = 11.7M\n",
      "11.7M = 1\n",
      "Accuracy Retention (AR): The Accuracy Retention (AR) is\n",
      "calculated as:\n",
      "AR =\n",
      "\u0012Accuracy After Pruning −Baseline Accuracy\n",
      "Baseline Accuracy\n",
      "\u0013\n",
      "×100\n",
      "(2)\n",
      "AR =\n",
      "\u00120.958284 −0.956206\n",
      "0.956206\n",
      "\u0013\n",
      "× 100 = 0.22%\n",
      "This result indicates that the pruned model achieves an ac-\n",
      "curacy improvement of 0.22% when compared to unpruned\n",
      "model.\n",
      "C. Illustration of Results\n",
      "Fig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\n",
      "18.\n",
      "D. Discussion\n",
      "The evaluation metrics highlight the effectiveness of the\n",
      "proposed Neurological Pruning method on the ResNet-18\n",
      "model. The Compression Ratio (CR) remains 1, indicating\n",
      "no reduction in the total number of parameters, which suggests\n",
      "that this approach optimizes the neural network by selectively\n",
      "pruning connections rather than reducing the overall parameter\n",
      "count. The Accuracy Retention (AR), which is approximately\n",
      "0.22%, demonstrates that the Neurological Pruning technique\n",
      "successfully improved the accuracy compared to the baseline\n",
      "model.\n",
      "These results validate the potential of Neurological Pruning\n",
      "as an innovative approach to optimizing deep learning models.\n",
      "By maintaining accuracy while likely enhancing sparsity and\n",
      "computational efficiency, this method presents a promising di-\n",
      "rection for achieving high-performance models under resource\n",
      "constraints.\n",
      "VII. CONCLUSION\n",
      "In summary, the Neurological Pruning Algorithm provides\n",
      "a systematic and principled approach to optimizing neural\n",
      "networks. By balancing the trade-off between sparsity and\n",
      "accuracy, it enables significant reductions in computational\n",
      "complexity without sacrificing the network’s predictive per-\n",
      "formance. Future work will focus on extending the algorithm\n",
      "to dynamic network architectures and incorporating advanced\n",
      "regularization techniques to further enhance its robustness and\n",
      "adaptability in diverse applications.\n",
      "REFERENCES\n",
      "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\n",
      "Fluctuation-based Adaptive Structured Pruning for Large Language\n",
      "Models.\n",
      "[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\n",
      "Simplification of Deep Neural Network-Based Object Detector for Real-\n",
      "Time Edge Computing.\n",
      "[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\n",
      "Devices Object Detection by Filter Pruning.\n",
      "[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\n",
      "and Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\n",
      "Tracking Algorithms in Edge Devices.\n",
      "[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\n",
      "Chen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\n",
      "Compression Method for Object Detection Network for Edge Devices.\n",
      "[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\n",
      "Cao. Absorption Pruning of Deep Neural Network for Object Detection\n",
      "in Remote Sensing Imagery.\n",
      "[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\n",
      "and Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\n",
      "Distribution Detection: An Empirical Survey.\n",
      "[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\n",
      "Sahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\n",
      "[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\n",
      "Compress DNNs.\n",
      "[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\n",
      "derstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\n",
      "[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\n",
      "Proving the Lottery Ticket Hypothesis: Pruning is All You Need.\n",
      "[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\n",
      "Devices via Reconstruction-Based Channel Pruning.\n",
      "[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\n",
      "Determine Reliable Paths on Networks with Random and Correlated\n",
      "Link Travel Times.\n",
      "[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\n",
      "and D. Suganthi. Real-Time Object Detection on Edge Devices Using\n",
      "Mobile Neural Networks.\n",
      "[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\n",
      "Tang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\n",
      "Models.\n",
      "[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\n",
      "Li, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\n",
      "Deng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\n",
      "Detection of Tea Leaves with One Bud and Two Leaves Based on\n",
      "ShuffleNetv2-YOLOv5-Lite-E.\n",
      "Neuro Prune: An Adaptive Approach for Efficient\n",
      "Deep Neural Network Optimization on Edge\n",
      "Devices\n",
      "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
      "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
      "grk.cse@psgtech.ac.in\n",
      "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
      "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
      "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
      "Abstract—The optimization of deep neural networks for de-\n",
      "ployment on edge devices is a significant research area due to\n",
      "the demand for applications such as augmented reality, smart\n",
      "cameras, and autonomous navigation. However, deploying large\n",
      "deep learning models on edge devices poses challenges related to\n",
      "computational power, energy consumption, and latency. Pruning\n",
      "is a method to reduce the model size, accelerate inference, and\n",
      "save power. The objective of the paper is to propose the Neuro\n",
      "Prune algorithm and to apply it for the optimization of deep\n",
      "neural networks on edge devices. Efforts have been made to\n",
      "compare pruned and unpruned models. As a result, the pruned\n",
      "model has an accuracy increase of 0.22%.\n",
      "I. INTRODUCTION\n",
      "The rise of intelligent systems, ranging from autonomous\n",
      "drones to augmented reality (AR) devices and smart surveil-\n",
      "lance cameras, has intensified the need for efficient deep neural\n",
      "network deployment on edge devices with constrained com-\n",
      "putational resources. While advanced deep learning models\n",
      "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
      "their deployment on such devices is hindered by significant\n",
      "computational demands, large memory footprints, and high\n",
      "energy consumption. These challenges pose critical barriers to\n",
      "real-time applications and sustainable deployment.\n",
      "This paper builds on existing pruning techniques, includ-\n",
      "ing unstructured pruning, structured pruning, and approaches\n",
      "inspired by quantization, highlighting their strengths and lim-\n",
      "itations. In response to the challenges identified, we introduce\n",
      "Neuro Prune, a novel pruning approach that integrates princi-\n",
      "ples from reinforcement learning and the Lottery Ticket Hy-\n",
      "pothesis to achieve an optimal balance between accuracy, com-\n",
      "putational efficiency, and energy consumption. By integrating\n",
      "these principles, Neuro Prune provides a robust framework\n",
      "for optimizing deep neural networks for deployment on edge\n",
      "devices without sacrificing performance.\n",
      "Neuro Prune reduces model size and computational re-\n",
      "quirements while maintaining or even improving the model’s\n",
      "accuracy. These findings underscore the potential of Neuro\n",
      "Prune to enable real-time, energy-efficient deep learning on\n",
      "resource-constrained devices.\n",
      "Neuro Prune employs a systematic process to optimize\n",
      "neural networks for edge deployment:\n",
      "• Mask Initialization: Each layer’s weights are paired with\n",
      "a binary mask (original_mask), initialized to ones.\n",
      "• Activation Tracking: During forward passes over the\n",
      "dataset, the magnitudes of weight activations are mon-\n",
      "itored and recorded.\n",
      "• Normalize and Prune: The recorded activations are\n",
      "normalized, and a threshold based on the pruning fraction\n",
      "is computed. Using this threshold, the masks are dynam-\n",
      "ically updated to prune unimportant weights.\n",
      "• Evaluate and Log Metrics: After pruning, metrics such\n",
      "as model sparsity and accuracy are computed. A reward\n",
      "metric is derived to balance accuracy and sparsity using\n",
      "a tunable parameter, λweight.\n",
      "• Retrain the Model: The pruned model undergoes retrain-\n",
      "ing to improve the loss in accuracy while maintaining low\n",
      "computational requirements\n",
      "Neuro Prune’s unique combination of reinforcement learn-\n",
      "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
      "pothesis for identifying optimal subnetworks enables it to\n",
      "achieve remarkable efficiency. This approach ensures substan-\n",
      "tial reduction in model size and energy consumption while\n",
      "preserving accuracy and inference speed, making it an ideal\n",
      "solution for edge-based deep learning applications.\n",
      "Through extensive empirical evaluations and theoretical\n",
      "insights, this paper demonstrates how Neuro Pruning outper-\n",
      "forms traditional pruning methods, setting a new benchmark\n",
      "for sustainable, high-performance AI deployment on edge\n",
      "devices. By situating our work within the broader research\n",
      "context, we aim to offer a critical contribution to the ongoing\n",
      "evolution of neural network optimization techniques.\n",
      "II. BACKGROUND\n",
      "A. Need for Pruning Techniques\n",
      "Pruning is a crucial optimization technique for deploying\n",
      "deep neural networks in resource-constrained environments\n",
      "like healthcare. For example, wearable devices like smart-\n",
      "watches that monitor heart rates or portable EEG systems\n",
      "for brain activity analysis require lightweight models due to\n",
      "their limited computational power and memory. Pruning helps\n",
      "address these constraints by reducing the memory footprint\n",
      "and computational load, enabling efficient model deployment.\n",
      "Additionally, it minimizes energy consumption, which is vital\n",
      "for sustainable and real-time healthcare applications. Faster\n",
      "inference enabled by pruning is particularly beneficial for\n",
      "time-sensitive tasks like seizure detection or emergency di-\n",
      "agnostics. Moreover, pruning enhances model interpretability\n",
      "by retaining only the most critical components, which is\n",
      "essential for gaining trust and ensuring regulatory compliance\n",
      "in healthcare. By reducing the hardware and operational costs,\n",
      "pruning also makes deploying advanced AI solutions on edge\n",
      "devices more cost-effective and accessible.\n",
      "B. Types of Pruning Techniques\n",
      "Pruning methods are broadly classified based on granu-\n",
      "larity, timing, and approach. Granularity-based pruning in-\n",
      "cludes structured pruning, which removes entire components\n",
      "such as neurons or filters to streamline computations, and\n",
      "unstructured pruning, which eliminates individual weights\n",
      "for finer optimization, though it often requires specialized\n",
      "hardware. Timing-based pruning involves pre-training pruning,\n",
      "which optimizes models before training; post-training prun-\n",
      "ing, where redundant components are removed from trained\n",
      "models with possible retraining; and dynamic pruning during\n",
      "training, which adjusts models in real-time for improved\n",
      "adaptability. Metric-based pruning uses parameters like weight\n",
      "magnitude or gradient contribution to identify and remove\n",
      "less critical components, while application-specific pruning\n",
      "tailors strategies to specific tasks, such as optimizing for EEG\n",
      "signal analysis, or hardware constraints like GPUs. Together,\n",
      "these methods enable the development of efficient, scalable,\n",
      "and task-specific models, particularly suited for edge device\n",
      "deployment in healthcare and other domains.\n",
      "III. RELATED WORK\n",
      "Jielei Wang et al.[12] introduced an absorption pruning\n",
      "method for object detection in remote sensing imagery. This\n",
      "study achieves efficient compression with minimal accuracy\n",
      "loss but requires careful layer-wise pruning ratio tuning.\n",
      "Jan Muller et al., in their research[12], propose a neural\n",
      "network pruning method for multi-object tracking (MOT) that\n",
      "reduces model size by up to 70\n",
      "Liang Li et al., in their work[9], introduce a novel pruning\n",
      "method for DNNs that utilizes a self-adaptive mechanism\n",
      "based on weight sparsity ratios and a protective reconstruction\n",
      "mechanism. Their approach improves both model compres-\n",
      "sion and accuracy, outperforming state-of-the-art methods on\n",
      "CIFAR-10 and ImageNet datasets.\n",
      "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
      "learning with edge devices like Google Coral AI and Nvidia\n",
      "Jetson Nano. Their work revolutionizes computer vision and\n",
      "real-time tracking but raises energy concerns due to the\n",
      "computational demands of advanced algorithms.\n",
      "Satoru Koda et al.[7] explore how weight pruning in\n",
      "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
      "larly with Mahalanobis-based approaches, by improving global\n",
      "feature extraction and leveraging weights not critical for\n",
      "classification.\n",
      "Shvetha S Kumar et al. analyze and compare three pruning\n",
      "techniques—L1-norm filter pruning, channel pruning, and\n",
      "weight pruning—on CNNs for accuracy and inference time,\n",
      "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
      "pared to GTX1080 Ti in their study[8].\n",
      "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
      "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
      "on stochastic networks, extending it to related problems and\n",
      "demonstrating its efficiency on real-world networks in their\n",
      "study[13].\n",
      "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
      "Training outperforms Post-Training Quantization, and sparsity\n",
      "training with Optimal Reduction pruning improves YOLOv4’s\n",
      "efficiency and edge-device performance, despite potential\n",
      "high-pruning rate drawbacks.\n",
      "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
      "method for DNNs. This approach optimizes inference speed\n",
      "and resource efficiency on edge devices with minimal accuracy\n",
      "loss but requires careful tuning of layer-specific pruning ratios.\n",
      "Yongqi An et al.[1] introduce a retraining-free structured\n",
      "pruning framework for Large Language Models (LLMs). Their\n",
      "method incorporates a fluctuation-based pruning metric, adap-\n",
      "tive structure search, and a bias compensation mechanism to\n",
      "achieve efficient pruning. FLAP significantly enhances infer-\n",
      "ence speed and reduces model size without requiring retrain-\n",
      "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
      "and Wanda-sp on LLaMA models across various benchmarks.\n",
      "From referring to these research papers, we derived an\n",
      "idea for pruning on edge devices. Additionally, our proposed\n",
      "solution is inspired by the research of Jaron Maene et al.[10],\n",
      "which suggests that sparse subnetworks in dense networks can\n",
      "achieve similar accuracy when retrained. This study demon-\n",
      "strates stable training with linear mode connectivity, support-\n",
      "ing the idea that lottery tickets retrain to similar regions, but\n",
      "questions their independence from dense training and iterative\n",
      "pruning.\n",
      "Eran Malach et al.[11] further strengthen the lottery ticket\n",
      "hypothesis by proving that over-parameterized neural networks\n",
      "with random weights always contain a subnetwork matching\n",
      "the accuracy of a target network, without additional training.\n",
      "IV. NEUROLOGICAL PRUNING APPROACH\n",
      "The Neurological Pruning Algorithm is designed to opti-\n",
      "mize neural networks by iteratively pruning less important\n",
      "weights while maintaining high performance. Unlike tradi-\n",
      "tional methods, this approach incorporates adaptive, layer-\n",
      "specific pruning thresholds that dynamically adjust based\n",
      "on weight activations, ensuring efficient model optimization\n",
      "across various stages of training. Drawing inspiration from\n",
      "the Lottery Ticket Hypothesis, which suggests that sparse\n",
      "subnetworks can perform similarly to their dense counterparts\n",
      "when retrained, the algorithm selectively deactivates weights\n",
      "using binary masks to enhance computational efficiency with-\n",
      "out sacrificing accuracy.\n",
      "The algorithm operates in several epochs, during which\n",
      "weight activations are evaluated. Based on a quantile-based\n",
      "strategy, pruning thresholds are established for each layer, en-\n",
      "suring that only weights contributing minimally to the model’s\n",
      "output are removed. After pruning, the model undergoes fine-\n",
      "tuning to restore any lost accuracy and further optimize the\n",
      "network. This step enhances the adaptability and scalability\n",
      "of the pruning method, making it suitable for deployment in\n",
      "resource-constrained environments\n",
      "A. Key Characteristics and Advantages\n",
      "The Neurological Pruning Algorithm boasts several key\n",
      "characteristics that distinguish it from conventional pruning\n",
      "techniques. One notable feature is its layer-specific pruning,\n",
      "which evaluates the importance of weights individually for\n",
      "each layer, leading to a more nuanced and efficient sparsifi-\n",
      "cation process. This approach prevents the model from losing\n",
      "essential features that may otherwise be pruned by a global\n",
      "approach.\n",
      "The algorithm’s use of quantile-based thresholding en-\n",
      "sures that only weights with the least contribution to the\n",
      "model’s output are pruned. This fine-tuned method avoids\n",
      "performance degradation by carefully targeting weights that\n",
      "are less significant to the network’s overall functionality.\n",
      "Furthermore, the pruning process is iterative, allowing for\n",
      "continuous model evaluation and real-time adjustments based\n",
      "on performance feedback, ensuring that accuracy is retained\n",
      "even as sparsity increases.\n",
      "Additionally, the algorithm maintains a careful balance\n",
      "between sparsity and accuracy, enabling a reduction in the\n",
      "model’s computational complexity while preserving its predic-\n",
      "tive capabilities. This characteristic is particularly important\n",
      "for applications requiring rapid inference times, such as in\n",
      "real-time healthcare systems and edge computing scenarios,\n",
      "where both energy efficiency and computational power are\n",
      "crucial.\n",
      "B. Impact on Model Performance\n",
      "The Neurological Pruning Algorithm significantly improves\n",
      "the efficiency of neural networks, making them more suitable\n",
      "for deployment on edge devices. Key performance metrics\n",
      "such as compression ratio (CR) and accuracy retention\n",
      "(AR) are critical indicators of the algorithm’s effectiveness.\n",
      "For instance, in evaluating a ResNet-18 model, the algorithm\n",
      "achieved a CR of 1, indicating that while the number of\n",
      "parameters remained the same, the model’s efficiency im-\n",
      "proved through selective pruning, reducing computational load\n",
      "without sacrificing performance.\n",
      "Moreover, the algorithm demonstrates the ability to achieve\n",
      "high accuracy retention, with models maintaining or even\n",
      "slightly improving their predictive performance post-pruning.\n",
      "This outcome highlights the algorithm’s capability to effi-\n",
      "ciently remove redundant weights while preserving the core\n",
      "functionality of the network. The result is a model that is\n",
      "not only more computationally efficient but also adaptable\n",
      "across different architectures and datasets, making it suit-\n",
      "able for a wide range of applications, from image classification\n",
      "to natural language processing .\n",
      "In practical terms, the Neurological Pruning Algorithm en-\n",
      "ables faster inference times and lower energy consumption,\n",
      "two crucial aspects for deploying AI models in resource-\n",
      "constrained environments. As such, it holds significant poten-\n",
      "tial for improving the viability of real-time, energy-efficient\n",
      "AI applications, particularly in fields like healthcare, where\n",
      "both speed and sustainability are essential.\n",
      "V. NEUROLOGICAL PRUNING ALGORITHM\n",
      "The Neurological Pruning Algorithm is a structured and\n",
      "adaptive methodology designed to optimize neural networks\n",
      "for resource-constrained environments. This algorithm goes\n",
      "beyond traditional pruning approaches by incorporating layer-\n",
      "specific pruning thresholds that dynamically adjust to the\n",
      "importance of weight activations within each layer. Drawing\n",
      "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
      "inforcement learning, the algorithm strikes an optimal balance\n",
      "between model sparsity and accuracy .\n",
      "A. Algorithm Description\n",
      "The Neurological Pruning Algorithm iteratively refines the\n",
      "neural network through a series of training epochs. During\n",
      "each epoch, weight activations are tracked, normalized, and\n",
      "analyzed to identify low-importance weights. These weights\n",
      "are selectively deactivated using binary masks, ensuring com-\n",
      "putational efficiency without compromising the network’s pre-\n",
      "dictive performance. The algorithm further integrates a fine-\n",
      "tuning phase, enabling the pruned network to recover and\n",
      "enhance its accuracy. The detailed process is presented in\n",
      "Algorithm 1.\n",
      "By dynamically adjusting to layer-specific characteristics\n",
      "and maintaining performance metrics, the Neurological Prun-\n",
      "ing Algorithm ensures a significant reduction in computational\n",
      "and memory requirements, paving the way for efficient deploy-\n",
      "ment of deep learning models on edge devices .\n",
      "B. Input Parameters and Output\n",
      "Inputs:\n",
      "• Pretrained model (M)\n",
      "• Dataset (D)\n",
      "• Prune fraction (p)\n",
      "• Total epochs (E)\n",
      "Output:\n",
      "• Pruned and fine-tuned model (Mpruned)\n",
      "C. Key Features and Advantages\n",
      "The Neurological Pruning Algorithm is designed with sev-\n",
      "eral distinctive features that contribute to its effectiveness\n",
      "in optimizing neural network performance. These features\n",
      "include:\n",
      "Algorithm 1 Simplified Neurological Pruning Algorithm\n",
      "Require: Pretrained model M, Dataset D, Prune fraction p,\n",
      "Total epochs E\n",
      "Ensure: Pruned and fine-tuned model Mpruned\n",
      "1: Initialize binary masks M with all ones\n",
      "2: for each epoch e = 1 to E do\n",
      "3:\n",
      "Compute weight activations A during forward passes\n",
      "4:\n",
      "for each layer i in M do\n",
      "5:\n",
      "Compute pruning threshold Ti as p-quantile of Ai\n",
      "6:\n",
      "Update mask Mi: retain weights with Ai > Ti\n",
      "7:\n",
      "Apply mask to layer: θi ←θi · Mi\n",
      "8:\n",
      "end for\n",
      "9:\n",
      "Evaluate model performance and update metrics\n",
      "10: end for\n",
      "11: Fine-tune Mpruned on D\n",
      "12: return Mpruned\n",
      "• Layer-specific Pruning: The algorithm applies pruning\n",
      "on a per-layer basis, evaluating the weight activations\n",
      "within each layer individually. This approach ensures that\n",
      "the sparsity introduced by pruning is well-distributed and\n",
      "context-dependent, facilitating more nuanced control over\n",
      "model optimization .\n",
      "• Quantile-based Thresholding: A quantile-based strat-\n",
      "egy is employed to determine the pruning threshold for\n",
      "each layer. This method ensures that only weights with\n",
      "minimal contribution to the network’s output are pruned,\n",
      "thereby reducing the risk of adversely affecting model\n",
      "accuracy while enhancing sparsity .\n",
      "• Iterative Pruning and Evaluation: The pruning pro-\n",
      "cess follows an iterative approach, enabling continuous\n",
      "assessment of the model’s performance after each pruning\n",
      "step. This ensures dynamic adjustments to maintain a bal-\n",
      "ance between model sparsity and predictive performance\n",
      "throughout training.\n",
      "• Preservation of Model Accuracy: By adjusting pruning\n",
      "thresholds in response to ongoing performance metrics,\n",
      "the algorithm ensures that model accuracy is not com-\n",
      "promised. The careful tuning of these thresholds allows\n",
      "for significant sparsity while maintaining the robustness\n",
      "of the network’s predictive capabilities .\n",
      "D. Impact on Neural Network Performance\n",
      "The Neurological Pruning Algorithm has been extensively\n",
      "evaluated across various benchmark datasets and model archi-\n",
      "tectures, demonstrating its capacity to achieve the following\n",
      "outcomes:\n",
      "• High Sparsity with Minimal Accuracy Loss: The algo-\n",
      "rithm effectively prunes unnecessary weights, achieving\n",
      "high sparsity levels without causing significant degrada-\n",
      "tion in model accuracy. This outcome is critical for the\n",
      "efficient deployment of models in resource-constrained\n",
      "environments.\n",
      "• Reduced Computational Complexity: By sparsifying\n",
      "the weight matrices, the algorithm reduces the compu-\n",
      "tational load during inference, which translates to faster\n",
      "model execution and lower resource consumption. This\n",
      "characteristic is especially beneficial in edge computing\n",
      "and real-time applications .\n",
      "• Broad Applicability: The algorithm is highly flexible\n",
      "and can be adapted to various neural network architec-\n",
      "tures and datasets, making it suitable for a wide range\n",
      "of tasks, from image recognition to natural language\n",
      "processing .\n",
      "VI. EVALUATION METRICS\n",
      "A. Performance Comparison\n",
      "Fig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\n",
      "TABLE I\n",
      "COMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\n",
      "Metric\n",
      "Unpruned\n",
      "Model\n",
      "Pruned\n",
      "Model\n",
      "Accuracy\n",
      "0.9562\n",
      "0.9583\n",
      "Train Loss\n",
      "0.1414\n",
      "0.1625\n",
      "Inference\n",
      "Time\n",
      "(s)\n",
      "0.0318\n",
      "0.0314\n",
      "No of Parame-\n",
      "ters(million)\n",
      "11.7\n",
      "11.7\n",
      "B. Evaluation Metrics Calculation\n",
      "The key metrics used to evaluate the proposed pruning\n",
      "technique are as follows:\n",
      "Compression Ratio (CR): The Compression Ratio (CR) is\n",
      "calculated as:\n",
      "CR = Total Parameters After Pruning\n",
      "Total Parameters Before Pruning\n",
      "(1)\n",
      "CR = 11.7M\n",
      "11.7M = 1\n",
      "Accuracy Retention (AR): The Accuracy Retention (AR) is\n",
      "calculated as:\n",
      "AR =\n",
      "\u0012Accuracy After Pruning −Baseline Accuracy\n",
      "Baseline Accuracy\n",
      "\u0013\n",
      "×100\n",
      "(2)\n",
      "AR =\n",
      "\u00120.958284 −0.956206\n",
      "0.956206\n",
      "\u0013\n",
      "× 100 = 0.22%\n",
      "This result indicates that the pruned model achieves an ac-\n",
      "curacy improvement of 0.22% when compared to unpruned\n",
      "model.\n",
      "C. Illustration of Results\n",
      "Fig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\n",
      "18.\n",
      "D. Discussion\n",
      "The evaluation metrics highlight the effectiveness of the\n",
      "proposed Neurological Pruning method on the ResNet-18\n",
      "model. The Compression Ratio (CR) remains 1, indicating\n",
      "no reduction in the total number of parameters, which suggests\n",
      "that this approach optimizes the neural network by selectively\n",
      "pruning connections rather than reducing the overall parameter\n",
      "count. The Accuracy Retention (AR), which is approximately\n",
      "0.22%, demonstrates that the Neurological Pruning technique\n",
      "successfully improved the accuracy compared to the baseline\n",
      "model.\n",
      "These results validate the potential of Neurological Pruning\n",
      "as an innovative approach to optimizing deep learning models.\n",
      "By maintaining accuracy while likely enhancing sparsity and\n",
      "computational efficiency, this method presents a promising di-\n",
      "rection for achieving high-performance models under resource\n",
      "constraints.\n",
      "VII. CONCLUSION\n",
      "In summary, the Neurological Pruning Algorithm provides\n",
      "a systematic and principled approach to optimizing neural\n",
      "networks. By balancing the trade-off between sparsity and\n",
      "accuracy, it enables significant reductions in computational\n",
      "complexity without sacrificing the network’s predictive per-\n",
      "formance. Future work will focus on extending the algorithm\n",
      "to dynamic network architectures and incorporating advanced\n",
      "regularization techniques to further enhance its robustness and\n",
      "adaptability in diverse applications.\n",
      "REFERENCES\n",
      "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\n",
      "Fluctuation-based Adaptive Structured Pruning for Large Language\n",
      "Models.\n",
      "[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\n",
      "Simplification of Deep Neural Network-Based Object Detector for Real-\n",
      "Time Edge Computing.\n",
      "[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\n",
      "Devices Object Detection by Filter Pruning.\n",
      "[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\n",
      "and Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\n",
      "Tracking Algorithms in Edge Devices.\n",
      "[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\n",
      "Chen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\n",
      "Compression Method for Object Detection Network for Edge Devices.\n",
      "[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\n",
      "Cao. Absorption Pruning of Deep Neural Network for Object Detection\n",
      "in Remote Sensing Imagery.\n",
      "[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\n",
      "and Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\n",
      "Distribution Detection: An Empirical Survey.\n",
      "[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\n",
      "Sahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\n",
      "[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\n",
      "Compress DNNs.\n",
      "[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\n",
      "derstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\n",
      "[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\n",
      "Proving the Lottery Ticket Hypothesis: Pruning is All You Need.\n",
      "[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\n",
      "Devices via Reconstruction-Based Channel Pruning.\n",
      "[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\n",
      "Determine Reliable Paths on Networks with Random and Correlated\n",
      "Link Travel Times.\n",
      "[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\n",
      "and D. Suganthi. Real-Time Object Detection on Edge Devices Using\n",
      "Mobile Neural Networks.\n",
      "[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\n",
      "Tang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\n",
      "Models.\n",
      "[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\n",
      "Li, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\n",
      "Deng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\n",
      "Detection of Tea Leaves with One Bud and Two Leaves Based on\n",
      "ShuffleNetv2-YOLOv5-Lite-E.\n",
      "\n",
      "    Metadata: {'source': 'geo_facts.txt', 'page': 1}\n",
      "  Source 2:\n",
      "    Content: The Great Wall of China is located in China and is a famous landmark.\n",
      "    Metadata: {'source': 'world_landmarks.txt', 'page': 5}\n",
      "--- End of Output ---\n"
     ]
    }
   ],
   "source": [
    "# rag_faiss_system.py\n",
    "\n",
    "# --- Dependencies ---\n",
    "# Make sure you have installed the necessary packages:\n",
    "# pip install langchain faiss-cpu sentence-transformers langchain-google-genai python-dotenv\n",
    "# (or faiss-gpu if you have CUDA installed and want GPU acceleration)\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS # Use langchain_community\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings # Use langchain_community\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # Import Gemini\n",
    "from langchain.docstore.document import Document # For structured documents\n",
    "\n",
    "# --- Configuration ---\n",
    "# Load environment variables (especially GOOGLE_API_KEY)\n",
    "load_dotenv() \n",
    "\n",
    "# Check if the API key is available\n",
    "if os.getenv(\"GOOGLE_API_KEY\") is None:\n",
    "    print(\"Error: GOOGLE_API_KEY environment variable not set.\")\n",
    "    print(\"Please get an API key from Google AI Studio (https://aistudio.google.com/app/apikey)\")\n",
    "    print(\"and set it as an environment variable (e.g., in a .env file).\")\n",
    "    exit() # Exit if the key is missing\n",
    "\n",
    "# --- Main Function ---\n",
    "# Step 1: Initialize the embedding model\n",
    "print(\"Initializing embedding model...\")\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'} # Specify device (cpu or cuda)\n",
    "    # Set encode_kwargs={'normalize_embeddings': True} if using cosine similarity,\n",
    "    # but FAISS IndexFlatL2 uses L2 distance (Euclidean), so normalization is optional \n",
    "    # but often helpful. Let's keep it simple for L2.\n",
    ")\n",
    "print(\"Embedding model initialized.\")\n",
    "\n",
    "# Step 2 & 3: Prepare documents and create FAISS index\n",
    "print(\"Preparing documents and creating FAISS vector store...\")\n",
    "documents_data = [\n",
    "    {\"content\": \"\"\"\n",
    "        Neuro Prune: An Adaptive Approach for Efficient\n",
    "Deep Neural Network Optimization on Edge\n",
    "Devices\n",
    "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
    "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
    "grk.cse@psgtech.ac.in\n",
    "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
    "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
    "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
    "Abstract—The optimization of deep neural networks for de-\n",
    "ployment on edge devices is a significant research area due to\n",
    "the demand for applications such as augmented reality, smart\n",
    "cameras, and autonomous navigation. However, deploying large\n",
    "deep learning models on edge devices poses challenges related to\n",
    "computational power, energy consumption, and latency. Pruning\n",
    "is a method to reduce the model size, accelerate inference, and\n",
    "save power. The objective of the paper is to propose the Neuro\n",
    "Prune algorithm and to apply it for the optimization of deep\n",
    "neural networks on edge devices. Efforts have been made to\n",
    "compare pruned and unpruned models. As a result, the pruned\n",
    "model has an accuracy increase of 0.22%.\n",
    "I. INTRODUCTION\n",
    "The rise of intelligent systems, ranging from autonomous\n",
    "drones to augmented reality (AR) devices and smart surveil-\n",
    "lance cameras, has intensified the need for efficient deep neural\n",
    "network deployment on edge devices with constrained com-\n",
    "putational resources. While advanced deep learning models\n",
    "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
    "their deployment on such devices is hindered by significant\n",
    "computational demands, large memory footprints, and high\n",
    "energy consumption. These challenges pose critical barriers to\n",
    "real-time applications and sustainable deployment.\n",
    "This paper builds on existing pruning techniques, includ-\n",
    "ing unstructured pruning, structured pruning, and approaches\n",
    "inspired by quantization, highlighting their strengths and lim-\n",
    "itations. In response to the challenges identified, we introduce\n",
    "Neuro Prune, a novel pruning approach that integrates princi-\n",
    "ples from reinforcement learning and the Lottery Ticket Hy-\n",
    "pothesis to achieve an optimal balance between accuracy, com-\n",
    "putational efficiency, and energy consumption. By integrating\n",
    "these principles, Neuro Prune provides a robust framework\n",
    "for optimizing deep neural networks for deployment on edge\n",
    "devices without sacrificing performance.\n",
    "Neuro Prune reduces model size and computational re-\n",
    "quirements while maintaining or even improving the model’s\n",
    "accuracy. These findings underscore the potential of Neuro\n",
    "Prune to enable real-time, energy-efficient deep learning on\n",
    "resource-constrained devices.\n",
    "Neuro Prune employs a systematic process to optimize\n",
    "neural networks for edge deployment:\n",
    "• Mask Initialization: Each layer’s weights are paired with\n",
    "a binary mask (original_mask), initialized to ones.\n",
    "• Activation Tracking: During forward passes over the\n",
    "dataset, the magnitudes of weight activations are mon-\n",
    "itored and recorded.\n",
    "• Normalize and Prune: The recorded activations are\n",
    "normalized, and a threshold based on the pruning fraction\n",
    "is computed. Using this threshold, the masks are dynam-\n",
    "ically updated to prune unimportant weights.\n",
    "• Evaluate and Log Metrics: After pruning, metrics such\n",
    "as model sparsity and accuracy are computed. A reward\n",
    "metric is derived to balance accuracy and sparsity using\n",
    "a tunable parameter, λweight.\n",
    "• Retrain the Model: The pruned model undergoes retrain-\n",
    "ing to improve the loss in accuracy while maintaining low\n",
    "computational requirements\n",
    "Neuro Prune’s unique combination of reinforcement learn-\n",
    "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
    "pothesis for identifying optimal subnetworks enables it to\n",
    "achieve remarkable efficiency. This approach ensures substan-\n",
    "tial reduction in model size and energy consumption while\n",
    "preserving accuracy and inference speed, making it an ideal\n",
    "solution for edge-based deep learning applications.\n",
    "Through extensive empirical evaluations and theoretical\n",
    "insights, this paper demonstrates how Neuro Pruning outper-\n",
    "forms traditional pruning methods, setting a new benchmark\n",
    "for sustainable, high-performance AI deployment on edge\n",
    "devices. By situating our work within the broader research\n",
    "context, we aim to offer a critical contribution to the ongoing\n",
    "evolution of neural network optimization techniques.\n",
    "II. BACKGROUND\n",
    "A. Need for Pruning Techniques\n",
    "Pruning is a crucial optimization technique for deploying\n",
    "deep neural networks in resource-constrained environments\n",
    "like healthcare. For example, wearable devices like smart-\n",
    "watches that monitor heart rates or portable EEG systems\n",
    "for brain activity analysis require lightweight models due to\n",
    "their limited computational power and memory. Pruning helps\n",
    "address these constraints by reducing the memory footprint\n",
    "and computational load, enabling efficient model deployment.\n",
    "Additionally, it minimizes energy consumption, which is vital\n",
    "for sustainable and real-time healthcare applications. Faster\n",
    "inference enabled by pruning is particularly beneficial for\n",
    "time-sensitive tasks like seizure detection or emergency di-\n",
    "agnostics. Moreover, pruning enhances model interpretability\n",
    "by retaining only the most critical components, which is\n",
    "essential for gaining trust and ensuring regulatory compliance\n",
    "in healthcare. By reducing the hardware and operational costs,\n",
    "pruning also makes deploying advanced AI solutions on edge\n",
    "devices more cost-effective and accessible.\n",
    "B. Types of Pruning Techniques\n",
    "Pruning methods are broadly classified based on granu-\n",
    "larity, timing, and approach. Granularity-based pruning in-\n",
    "cludes structured pruning, which removes entire components\n",
    "such as neurons or filters to streamline computations, and\n",
    "unstructured pruning, which eliminates individual weights\n",
    "for finer optimization, though it often requires specialized\n",
    "hardware. Timing-based pruning involves pre-training pruning,\n",
    "which optimizes models before training; post-training prun-\n",
    "ing, where redundant components are removed from trained\n",
    "models with possible retraining; and dynamic pruning during\n",
    "training, which adjusts models in real-time for improved\n",
    "adaptability. Metric-based pruning uses parameters like weight\n",
    "magnitude or gradient contribution to identify and remove\n",
    "less critical components, while application-specific pruning\n",
    "tailors strategies to specific tasks, such as optimizing for EEG\n",
    "signal analysis, or hardware constraints like GPUs. Together,\n",
    "these methods enable the development of efficient, scalable,\n",
    "and task-specific models, particularly suited for edge device\n",
    "deployment in healthcare and other domains.\n",
    "III. RELATED WORK\n",
    "Jielei Wang et al.[12] introduced an absorption pruning\n",
    "method for object detection in remote sensing imagery. This\n",
    "study achieves efficient compression with minimal accuracy\n",
    "loss but requires careful layer-wise pruning ratio tuning.\n",
    "Jan Muller et al., in their research[12], propose a neural\n",
    "network pruning method for multi-object tracking (MOT) that\n",
    "reduces model size by up to 70\n",
    "Liang Li et al., in their work[9], introduce a novel pruning\n",
    "method for DNNs that utilizes a self-adaptive mechanism\n",
    "based on weight sparsity ratios and a protective reconstruction\n",
    "mechanism. Their approach improves both model compres-\n",
    "sion and accuracy, outperforming state-of-the-art methods on\n",
    "CIFAR-10 and ImageNet datasets.\n",
    "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
    "learning with edge devices like Google Coral AI and Nvidia\n",
    "Jetson Nano. Their work revolutionizes computer vision and\n",
    "real-time tracking but raises energy concerns due to the\n",
    "computational demands of advanced algorithms.\n",
    "Satoru Koda et al.[7] explore how weight pruning in\n",
    "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
    "larly with Mahalanobis-based approaches, by improving global\n",
    "feature extraction and leveraging weights not critical for\n",
    "classification.\n",
    "Shvetha S Kumar et al. analyze and compare three pruning\n",
    "techniques—L1-norm filter pruning, channel pruning, and\n",
    "weight pruning—on CNNs for accuracy and inference time,\n",
    "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
    "pared to GTX1080 Ti in their study[8].\n",
    "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
    "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
    "on stochastic networks, extending it to related problems and\n",
    "demonstrating its efficiency on real-world networks in their\n",
    "study[13].\n",
    "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
    "Training outperforms Post-Training Quantization, and sparsity\n",
    "training with Optimal Reduction pruning improves YOLOv4’s\n",
    "efficiency and edge-device performance, despite potential\n",
    "high-pruning rate drawbacks.\n",
    "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
    "method for DNNs. This approach optimizes inference speed\n",
    "and resource efficiency on edge devices with minimal accuracy\n",
    "loss but requires careful tuning of layer-specific pruning ratios.\n",
    "Yongqi An et al.[1] introduce a retraining-free structured\n",
    "pruning framework for Large Language Models (LLMs). Their\n",
    "method incorporates a fluctuation-based pruning metric, adap-\n",
    "tive structure search, and a bias compensation mechanism to\n",
    "achieve efficient pruning. FLAP significantly enhances infer-\n",
    "ence speed and reduces model size without requiring retrain-\n",
    "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
    "and Wanda-sp on LLaMA models across various benchmarks.\n",
    "From referring to these research papers, we derived an\n",
    "idea for pruning on edge devices. Additionally, our proposed\n",
    "solution is inspired by the research of Jaron Maene et al.[10],\n",
    "which suggests that sparse subnetworks in dense networks can\n",
    "achieve similar accuracy when retrained. This study demon-\n",
    "strates stable training with linear mode connectivity, support-\n",
    "ing the idea that lottery tickets retrain to similar regions, but\n",
    "questions their independence from dense training and iterative\n",
    "pruning.\n",
    "Eran Malach et al.[11] further strengthen the lottery ticket\n",
    "hypothesis by proving that over-parameterized neural networks\n",
    "with random weights always contain a subnetwork matching\n",
    "the accuracy of a target network, without additional training.\n",
    "IV. NEUROLOGICAL PRUNING APPROACH\n",
    "The Neurological Pruning Algorithm is designed to opti-\n",
    "mize neural networks by iteratively pruning less important\n",
    "weights while maintaining high performance. Unlike tradi-\n",
    "tional methods, this approach incorporates adaptive, layer-\n",
    "specific pruning thresholds that dynamically adjust based\n",
    "on weight activations, ensuring efficient model optimization\n",
    "across various stages of training. Drawing inspiration from\n",
    "the Lottery Ticket Hypothesis, which suggests that sparse\n",
    "subnetworks can perform similarly to their dense counterparts\n",
    "when retrained, the algorithm selectively deactivates weights\n",
    "using binary masks to enhance computational efficiency with-\n",
    "out sacrificing accuracy.\n",
    "The algorithm operates in several epochs, during which\n",
    "weight activations are evaluated. Based on a quantile-based\n",
    "strategy, pruning thresholds are established for each layer, en-\n",
    "suring that only weights contributing minimally to the model’s\n",
    "output are removed. After pruning, the model undergoes fine-\n",
    "tuning to restore any lost accuracy and further optimize the\n",
    "network. This step enhances the adaptability and scalability\n",
    "of the pruning method, making it suitable for deployment in\n",
    "resource-constrained environments\n",
    "A. Key Characteristics and Advantages\n",
    "The Neurological Pruning Algorithm boasts several key\n",
    "characteristics that distinguish it from conventional pruning\n",
    "techniques. One notable feature is its layer-specific pruning,\n",
    "which evaluates the importance of weights individually for\n",
    "each layer, leading to a more nuanced and efficient sparsifi-\n",
    "cation process. This approach prevents the model from losing\n",
    "essential features that may otherwise be pruned by a global\n",
    "approach.\n",
    "The algorithm’s use of quantile-based thresholding en-\n",
    "sures that only weights with the least contribution to the\n",
    "model’s output are pruned. This fine-tuned method avoids\n",
    "performance degradation by carefully targeting weights that\n",
    "are less significant to the network’s overall functionality.\n",
    "Furthermore, the pruning process is iterative, allowing for\n",
    "continuous model evaluation and real-time adjustments based\n",
    "on performance feedback, ensuring that accuracy is retained\n",
    "even as sparsity increases.\n",
    "Additionally, the algorithm maintains a careful balance\n",
    "between sparsity and accuracy, enabling a reduction in the\n",
    "model’s computational complexity while preserving its predic-\n",
    "tive capabilities. This characteristic is particularly important\n",
    "for applications requiring rapid inference times, such as in\n",
    "real-time healthcare systems and edge computing scenarios,\n",
    "where both energy efficiency and computational power are\n",
    "crucial.\n",
    "B. Impact on Model Performance\n",
    "The Neurological Pruning Algorithm significantly improves\n",
    "the efficiency of neural networks, making them more suitable\n",
    "for deployment on edge devices. Key performance metrics\n",
    "such as compression ratio (CR) and accuracy retention\n",
    "(AR) are critical indicators of the algorithm’s effectiveness.\n",
    "For instance, in evaluating a ResNet-18 model, the algorithm\n",
    "achieved a CR of 1, indicating that while the number of\n",
    "parameters remained the same, the model’s efficiency im-\n",
    "proved through selective pruning, reducing computational load\n",
    "without sacrificing performance.\n",
    "Moreover, the algorithm demonstrates the ability to achieve\n",
    "high accuracy retention, with models maintaining or even\n",
    "slightly improving their predictive performance post-pruning.\n",
    "This outcome highlights the algorithm’s capability to effi-\n",
    "ciently remove redundant weights while preserving the core\n",
    "functionality of the network. The result is a model that is\n",
    "not only more computationally efficient but also adaptable\n",
    "across different architectures and datasets, making it suit-\n",
    "able for a wide range of applications, from image classification\n",
    "to natural language processing .\n",
    "In practical terms, the Neurological Pruning Algorithm en-\n",
    "ables faster inference times and lower energy consumption,\n",
    "two crucial aspects for deploying AI models in resource-\n",
    "constrained environments. As such, it holds significant poten-\n",
    "tial for improving the viability of real-time, energy-efficient\n",
    "AI applications, particularly in fields like healthcare, where\n",
    "both speed and sustainability are essential.\n",
    "V. NEUROLOGICAL PRUNING ALGORITHM\n",
    "The Neurological Pruning Algorithm is a structured and\n",
    "adaptive methodology designed to optimize neural networks\n",
    "for resource-constrained environments. This algorithm goes\n",
    "beyond traditional pruning approaches by incorporating layer-\n",
    "specific pruning thresholds that dynamically adjust to the\n",
    "importance of weight activations within each layer. Drawing\n",
    "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
    "inforcement learning, the algorithm strikes an optimal balance\n",
    "between model sparsity and accuracy .\n",
    "A. Algorithm Description\n",
    "The Neurological Pruning Algorithm iteratively refines the\n",
    "neural network through a series of training epochs. During\n",
    "each epoch, weight activations are tracked, normalized, and\n",
    "analyzed to identify low-importance weights. These weights\n",
    "are selectively deactivated using binary masks, ensuring com-\n",
    "putational efficiency without compromising the network’s pre-\n",
    "dictive performance. The algorithm further integrates a fine-\n",
    "tuning phase, enabling the pruned network to recover and\n",
    "enhance its accuracy. The detailed process is presented in\n",
    "Algorithm 1.\n",
    "By dynamically adjusting to layer-specific characteristics\n",
    "and maintaining performance metrics, the Neurological Prun-\n",
    "ing Algorithm ensures a significant reduction in computational\n",
    "and memory requirements, paving the way for efficient deploy-\n",
    "ment of deep learning models on edge devices .\n",
    "B. Input Parameters and Output\n",
    "Inputs:\n",
    "• Pretrained model (M)\n",
    "• Dataset (D)\n",
    "• Prune fraction (p)\n",
    "• Total epochs (E)\n",
    "Output:\n",
    "• Pruned and fine-tuned model (Mpruned)\n",
    "C. Key Features and Advantages\n",
    "The Neurological Pruning Algorithm is designed with sev-\n",
    "eral distinctive features that contribute to its effectiveness\n",
    "in optimizing neural network performance. These features\n",
    "include:\n",
    "Algorithm 1 Simplified Neurological Pruning Algorithm\n",
    "Require: Pretrained model M, Dataset D, Prune fraction p,\n",
    "Total epochs E\n",
    "Ensure: Pruned and fine-tuned model Mpruned\n",
    "1: Initialize binary masks M with all ones\n",
    "2: for each epoch e = 1 to E do\n",
    "3:\n",
    "Compute weight activations A during forward passes\n",
    "4:\n",
    "for each layer i in M do\n",
    "5:\n",
    "Compute pruning threshold Ti as p-quantile of Ai\n",
    "6:\n",
    "Update mask Mi: retain weights with Ai > Ti\n",
    "7:\n",
    "Apply mask to layer: θi ←θi · Mi\n",
    "8:\n",
    "end for\n",
    "9:\n",
    "Evaluate model performance and update metrics\n",
    "10: end for\n",
    "11: Fine-tune Mpruned on D\n",
    "12: return Mpruned\n",
    "• Layer-specific Pruning: The algorithm applies pruning\n",
    "on a per-layer basis, evaluating the weight activations\n",
    "within each layer individually. This approach ensures that\n",
    "the sparsity introduced by pruning is well-distributed and\n",
    "context-dependent, facilitating more nuanced control over\n",
    "model optimization .\n",
    "• Quantile-based Thresholding: A quantile-based strat-\n",
    "egy is employed to determine the pruning threshold for\n",
    "each layer. This method ensures that only weights with\n",
    "minimal contribution to the network’s output are pruned,\n",
    "thereby reducing the risk of adversely affecting model\n",
    "accuracy while enhancing sparsity .\n",
    "• Iterative Pruning and Evaluation: The pruning pro-\n",
    "cess follows an iterative approach, enabling continuous\n",
    "assessment of the model’s performance after each pruning\n",
    "step. This ensures dynamic adjustments to maintain a bal-\n",
    "ance between model sparsity and predictive performance\n",
    "throughout training.\n",
    "• Preservation of Model Accuracy: By adjusting pruning\n",
    "thresholds in response to ongoing performance metrics,\n",
    "the algorithm ensures that model accuracy is not com-\n",
    "promised. The careful tuning of these thresholds allows\n",
    "for significant sparsity while maintaining the robustness\n",
    "of the network’s predictive capabilities .\n",
    "D. Impact on Neural Network Performance\n",
    "The Neurological Pruning Algorithm has been extensively\n",
    "evaluated across various benchmark datasets and model archi-\n",
    "tectures, demonstrating its capacity to achieve the following\n",
    "outcomes:\n",
    "• High Sparsity with Minimal Accuracy Loss: The algo-\n",
    "rithm effectively prunes unnecessary weights, achieving\n",
    "high sparsity levels without causing significant degrada-\n",
    "tion in model accuracy. This outcome is critical for the\n",
    "efficient deployment of models in resource-constrained\n",
    "environments.\n",
    "• Reduced Computational Complexity: By sparsifying\n",
    "the weight matrices, the algorithm reduces the compu-\n",
    "tational load during inference, which translates to faster\n",
    "model execution and lower resource consumption. This\n",
    "characteristic is especially beneficial in edge computing\n",
    "and real-time applications .\n",
    "• Broad Applicability: The algorithm is highly flexible\n",
    "and can be adapted to various neural network architec-\n",
    "tures and datasets, making it suitable for a wide range\n",
    "of tasks, from image recognition to natural language\n",
    "processing .\n",
    "VI. EVALUATION METRICS\n",
    "A. Performance Comparison\n",
    "Fig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\n",
    "TABLE I\n",
    "COMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\n",
    "Metric\n",
    "Unpruned\n",
    "Model\n",
    "Pruned\n",
    "Model\n",
    "Accuracy\n",
    "0.9562\n",
    "0.9583\n",
    "Train Loss\n",
    "0.1414\n",
    "0.1625\n",
    "Inference\n",
    "Time\n",
    "(s)\n",
    "0.0318\n",
    "0.0314\n",
    "No of Parame-\n",
    "ters(million)\n",
    "11.7\n",
    "11.7\n",
    "B. Evaluation Metrics Calculation\n",
    "The key metrics used to evaluate the proposed pruning\n",
    "technique are as follows:\n",
    "Compression Ratio (CR): The Compression Ratio (CR) is\n",
    "calculated as:\n",
    "CR = Total Parameters After Pruning\n",
    "Total Parameters Before Pruning\n",
    "(1)\n",
    "CR = 11.7M\n",
    "11.7M = 1\n",
    "Accuracy Retention (AR): The Accuracy Retention (AR) is\n",
    "calculated as:\n",
    "AR =\n",
    "\u0012Accuracy After Pruning −Baseline Accuracy\n",
    "Baseline Accuracy\n",
    "\u0013\n",
    "×100\n",
    "(2)\n",
    "AR =\n",
    "\u00120.958284 −0.956206\n",
    "0.956206\n",
    "\u0013\n",
    "× 100 = 0.22%\n",
    "This result indicates that the pruned model achieves an ac-\n",
    "curacy improvement of 0.22% when compared to unpruned\n",
    "model.\n",
    "C. Illustration of Results\n",
    "Fig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\n",
    "18.\n",
    "D. Discussion\n",
    "The evaluation metrics highlight the effectiveness of the\n",
    "proposed Neurological Pruning method on the ResNet-18\n",
    "model. The Compression Ratio (CR) remains 1, indicating\n",
    "no reduction in the total number of parameters, which suggests\n",
    "that this approach optimizes the neural network by selectively\n",
    "pruning connections rather than reducing the overall parameter\n",
    "count. The Accuracy Retention (AR), which is approximately\n",
    "0.22%, demonstrates that the Neurological Pruning technique\n",
    "successfully improved the accuracy compared to the baseline\n",
    "model.\n",
    "These results validate the potential of Neurological Pruning\n",
    "as an innovative approach to optimizing deep learning models.\n",
    "By maintaining accuracy while likely enhancing sparsity and\n",
    "computational efficiency, this method presents a promising di-\n",
    "rection for achieving high-performance models under resource\n",
    "constraints.\n",
    "VII. CONCLUSION\n",
    "In summary, the Neurological Pruning Algorithm provides\n",
    "a systematic and principled approach to optimizing neural\n",
    "networks. By balancing the trade-off between sparsity and\n",
    "accuracy, it enables significant reductions in computational\n",
    "complexity without sacrificing the network’s predictive per-\n",
    "formance. Future work will focus on extending the algorithm\n",
    "to dynamic network architectures and incorporating advanced\n",
    "regularization techniques to further enhance its robustness and\n",
    "adaptability in diverse applications.\n",
    "REFERENCES\n",
    "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\n",
    "Fluctuation-based Adaptive Structured Pruning for Large Language\n",
    "Models.\n",
    "[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\n",
    "Simplification of Deep Neural Network-Based Object Detector for Real-\n",
    "Time Edge Computing.\n",
    "[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\n",
    "Devices Object Detection by Filter Pruning.\n",
    "[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\n",
    "and Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\n",
    "Tracking Algorithms in Edge Devices.\n",
    "[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\n",
    "Chen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\n",
    "Compression Method for Object Detection Network for Edge Devices.\n",
    "[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\n",
    "Cao. Absorption Pruning of Deep Neural Network for Object Detection\n",
    "in Remote Sensing Imagery.\n",
    "[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\n",
    "and Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\n",
    "Distribution Detection: An Empirical Survey.\n",
    "[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\n",
    "Sahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\n",
    "[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\n",
    "Compress DNNs.\n",
    "[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\n",
    "derstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\n",
    "[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\n",
    "Proving the Lottery Ticket Hypothesis: Pruning is All You Need.\n",
    "[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\n",
    "Devices via Reconstruction-Based Channel Pruning.\n",
    "[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\n",
    "Determine Reliable Paths on Networks with Random and Correlated\n",
    "Link Travel Times.\n",
    "[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\n",
    "and D. Suganthi. Real-Time Object Detection on Edge Devices Using\n",
    "Mobile Neural Networks.\n",
    "[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\n",
    "Tang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\n",
    "Models.\n",
    "[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\n",
    "Li, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\n",
    "Deng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\n",
    "Detection of Tea Leaves with One Bud and Two Leaves Based on\n",
    "ShuffleNetv2-YOLOv5-Lite-E.\n",
    "Neuro Prune: An Adaptive Approach for Efficient\n",
    "Deep Neural Network Optimization on Edge\n",
    "Devices\n",
    "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
    "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
    "grk.cse@psgtech.ac.in\n",
    "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
    "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
    "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
    "Abstract—The optimization of deep neural networks for de-\n",
    "ployment on edge devices is a significant research area due to\n",
    "the demand for applications such as augmented reality, smart\n",
    "cameras, and autonomous navigation. However, deploying large\n",
    "deep learning models on edge devices poses challenges related to\n",
    "computational power, energy consumption, and latency. Pruning\n",
    "is a method to reduce the model size, accelerate inference, and\n",
    "save power. The objective of the paper is to propose the Neuro\n",
    "Prune algorithm and to apply it for the optimization of deep\n",
    "neural networks on edge devices. Efforts have been made to\n",
    "compare pruned and unpruned models. As a result, the pruned\n",
    "model has an accuracy increase of 0.22%.\n",
    "I. INTRODUCTION\n",
    "The rise of intelligent systems, ranging from autonomous\n",
    "drones to augmented reality (AR) devices and smart surveil-\n",
    "lance cameras, has intensified the need for efficient deep neural\n",
    "network deployment on edge devices with constrained com-\n",
    "putational resources. While advanced deep learning models\n",
    "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
    "their deployment on such devices is hindered by significant\n",
    "computational demands, large memory footprints, and high\n",
    "energy consumption. These challenges pose critical barriers to\n",
    "real-time applications and sustainable deployment.\n",
    "This paper builds on existing pruning techniques, includ-\n",
    "ing unstructured pruning, structured pruning, and approaches\n",
    "inspired by quantization, highlighting their strengths and lim-\n",
    "itations. In response to the challenges identified, we introduce\n",
    "Neuro Prune, a novel pruning approach that integrates princi-\n",
    "ples from reinforcement learning and the Lottery Ticket Hy-\n",
    "pothesis to achieve an optimal balance between accuracy, com-\n",
    "putational efficiency, and energy consumption. By integrating\n",
    "these principles, Neuro Prune provides a robust framework\n",
    "for optimizing deep neural networks for deployment on edge\n",
    "devices without sacrificing performance.\n",
    "Neuro Prune reduces model size and computational re-\n",
    "quirements while maintaining or even improving the model’s\n",
    "accuracy. These findings underscore the potential of Neuro\n",
    "Prune to enable real-time, energy-efficient deep learning on\n",
    "resource-constrained devices.\n",
    "Neuro Prune employs a systematic process to optimize\n",
    "neural networks for edge deployment:\n",
    "• Mask Initialization: Each layer’s weights are paired with\n",
    "a binary mask (original_mask), initialized to ones.\n",
    "• Activation Tracking: During forward passes over the\n",
    "dataset, the magnitudes of weight activations are mon-\n",
    "itored and recorded.\n",
    "• Normalize and Prune: The recorded activations are\n",
    "normalized, and a threshold based on the pruning fraction\n",
    "is computed. Using this threshold, the masks are dynam-\n",
    "ically updated to prune unimportant weights.\n",
    "• Evaluate and Log Metrics: After pruning, metrics such\n",
    "as model sparsity and accuracy are computed. A reward\n",
    "metric is derived to balance accuracy and sparsity using\n",
    "a tunable parameter, λweight.\n",
    "• Retrain the Model: The pruned model undergoes retrain-\n",
    "ing to improve the loss in accuracy while maintaining low\n",
    "computational requirements\n",
    "Neuro Prune’s unique combination of reinforcement learn-\n",
    "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
    "pothesis for identifying optimal subnetworks enables it to\n",
    "achieve remarkable efficiency. This approach ensures substan-\n",
    "tial reduction in model size and energy consumption while\n",
    "preserving accuracy and inference speed, making it an ideal\n",
    "solution for edge-based deep learning applications.\n",
    "Through extensive empirical evaluations and theoretical\n",
    "insights, this paper demonstrates how Neuro Pruning outper-\n",
    "forms traditional pruning methods, setting a new benchmark\n",
    "for sustainable, high-performance AI deployment on edge\n",
    "devices. By situating our work within the broader research\n",
    "context, we aim to offer a critical contribution to the ongoing\n",
    "evolution of neural network optimization techniques.\n",
    "II. BACKGROUND\n",
    "A. Need for Pruning Techniques\n",
    "Pruning is a crucial optimization technique for deploying\n",
    "deep neural networks in resource-constrained environments\n",
    "like healthcare. For example, wearable devices like smart-\n",
    "watches that monitor heart rates or portable EEG systems\n",
    "for brain activity analysis require lightweight models due to\n",
    "their limited computational power and memory. Pruning helps\n",
    "address these constraints by reducing the memory footprint\n",
    "and computational load, enabling efficient model deployment.\n",
    "Additionally, it minimizes energy consumption, which is vital\n",
    "for sustainable and real-time healthcare applications. Faster\n",
    "inference enabled by pruning is particularly beneficial for\n",
    "time-sensitive tasks like seizure detection or emergency di-\n",
    "agnostics. Moreover, pruning enhances model interpretability\n",
    "by retaining only the most critical components, which is\n",
    "essential for gaining trust and ensuring regulatory compliance\n",
    "in healthcare. By reducing the hardware and operational costs,\n",
    "pruning also makes deploying advanced AI solutions on edge\n",
    "devices more cost-effective and accessible.\n",
    "B. Types of Pruning Techniques\n",
    "Pruning methods are broadly classified based on granu-\n",
    "larity, timing, and approach. Granularity-based pruning in-\n",
    "cludes structured pruning, which removes entire components\n",
    "such as neurons or filters to streamline computations, and\n",
    "unstructured pruning, which eliminates individual weights\n",
    "for finer optimization, though it often requires specialized\n",
    "hardware. Timing-based pruning involves pre-training pruning,\n",
    "which optimizes models before training; post-training prun-\n",
    "ing, where redundant components are removed from trained\n",
    "models with possible retraining; and dynamic pruning during\n",
    "training, which adjusts models in real-time for improved\n",
    "adaptability. Metric-based pruning uses parameters like weight\n",
    "magnitude or gradient contribution to identify and remove\n",
    "less critical components, while application-specific pruning\n",
    "tailors strategies to specific tasks, such as optimizing for EEG\n",
    "signal analysis, or hardware constraints like GPUs. Together,\n",
    "these methods enable the development of efficient, scalable,\n",
    "and task-specific models, particularly suited for edge device\n",
    "deployment in healthcare and other domains.\n",
    "III. RELATED WORK\n",
    "Jielei Wang et al.[12] introduced an absorption pruning\n",
    "method for object detection in remote sensing imagery. This\n",
    "study achieves efficient compression with minimal accuracy\n",
    "loss but requires careful layer-wise pruning ratio tuning.\n",
    "Jan Muller et al., in their research[12], propose a neural\n",
    "network pruning method for multi-object tracking (MOT) that\n",
    "reduces model size by up to 70\n",
    "Liang Li et al., in their work[9], introduce a novel pruning\n",
    "method for DNNs that utilizes a self-adaptive mechanism\n",
    "based on weight sparsity ratios and a protective reconstruction\n",
    "mechanism. Their approach improves both model compres-\n",
    "sion and accuracy, outperforming state-of-the-art methods on\n",
    "CIFAR-10 and ImageNet datasets.\n",
    "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
    "learning with edge devices like Google Coral AI and Nvidia\n",
    "Jetson Nano. Their work revolutionizes computer vision and\n",
    "real-time tracking but raises energy concerns due to the\n",
    "computational demands of advanced algorithms.\n",
    "Satoru Koda et al.[7] explore how weight pruning in\n",
    "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
    "larly with Mahalanobis-based approaches, by improving global\n",
    "feature extraction and leveraging weights not critical for\n",
    "classification.\n",
    "Shvetha S Kumar et al. analyze and compare three pruning\n",
    "techniques—L1-norm filter pruning, channel pruning, and\n",
    "weight pruning—on CNNs for accuracy and inference time,\n",
    "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
    "pared to GTX1080 Ti in their study[8].\n",
    "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
    "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
    "on stochastic networks, extending it to related problems and\n",
    "demonstrating its efficiency on real-world networks in their\n",
    "study[13].\n",
    "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
    "Training outperforms Post-Training Quantization, and sparsity\n",
    "training with Optimal Reduction pruning improves YOLOv4’s\n",
    "efficiency and edge-device performance, despite potential\n",
    "high-pruning rate drawbacks.\n",
    "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
    "method for DNNs. This approach optimizes inference speed\n",
    "and resource efficiency on edge devices with minimal accuracy\n",
    "loss but requires careful tuning of layer-specific pruning ratios.\n",
    "Yongqi An et al.[1] introduce a retraining-free structured\n",
    "pruning framework for Large Language Models (LLMs). Their\n",
    "method incorporates a fluctuation-based pruning metric, adap-\n",
    "tive structure search, and a bias compensation mechanism to\n",
    "achieve efficient pruning. FLAP significantly enhances infer-\n",
    "ence speed and reduces model size without requiring retrain-\n",
    "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
    "and Wanda-sp on LLaMA models across various benchmarks.\n",
    "From referring to these research papers, we derived an\n",
    "idea for pruning on edge devices. Additionally, our proposed\n",
    "solution is inspired by the research of Jaron Maene et al.[10],\n",
    "which suggests that sparse subnetworks in dense networks can\n",
    "achieve similar accuracy when retrained. This study demon-\n",
    "strates stable training with linear mode connectivity, support-\n",
    "ing the idea that lottery tickets retrain to similar regions, but\n",
    "questions their independence from dense training and iterative\n",
    "pruning.\n",
    "Eran Malach et al.[11] further strengthen the lottery ticket\n",
    "hypothesis by proving that over-parameterized neural networks\n",
    "with random weights always contain a subnetwork matching\n",
    "the accuracy of a target network, without additional training.\n",
    "IV. NEUROLOGICAL PRUNING APPROACH\n",
    "The Neurological Pruning Algorithm is designed to opti-\n",
    "mize neural networks by iteratively pruning less important\n",
    "weights while maintaining high performance. Unlike tradi-\n",
    "tional methods, this approach incorporates adaptive, layer-\n",
    "specific pruning thresholds that dynamically adjust based\n",
    "on weight activations, ensuring efficient model optimization\n",
    "across various stages of training. Drawing inspiration from\n",
    "the Lottery Ticket Hypothesis, which suggests that sparse\n",
    "subnetworks can perform similarly to their dense counterparts\n",
    "when retrained, the algorithm selectively deactivates weights\n",
    "using binary masks to enhance computational efficiency with-\n",
    "out sacrificing accuracy.\n",
    "The algorithm operates in several epochs, during which\n",
    "weight activations are evaluated. Based on a quantile-based\n",
    "strategy, pruning thresholds are established for each layer, en-\n",
    "suring that only weights contributing minimally to the model’s\n",
    "output are removed. After pruning, the model undergoes fine-\n",
    "tuning to restore any lost accuracy and further optimize the\n",
    "network. This step enhances the adaptability and scalability\n",
    "of the pruning method, making it suitable for deployment in\n",
    "resource-constrained environments\n",
    "A. Key Characteristics and Advantages\n",
    "The Neurological Pruning Algorithm boasts several key\n",
    "characteristics that distinguish it from conventional pruning\n",
    "techniques. One notable feature is its layer-specific pruning,\n",
    "which evaluates the importance of weights individually for\n",
    "each layer, leading to a more nuanced and efficient sparsifi-\n",
    "cation process. This approach prevents the model from losing\n",
    "essential features that may otherwise be pruned by a global\n",
    "approach.\n",
    "The algorithm’s use of quantile-based thresholding en-\n",
    "sures that only weights with the least contribution to the\n",
    "model’s output are pruned. This fine-tuned method avoids\n",
    "performance degradation by carefully targeting weights that\n",
    "are less significant to the network’s overall functionality.\n",
    "Furthermore, the pruning process is iterative, allowing for\n",
    "continuous model evaluation and real-time adjustments based\n",
    "on performance feedback, ensuring that accuracy is retained\n",
    "even as sparsity increases.\n",
    "Additionally, the algorithm maintains a careful balance\n",
    "between sparsity and accuracy, enabling a reduction in the\n",
    "model’s computational complexity while preserving its predic-\n",
    "tive capabilities. This characteristic is particularly important\n",
    "for applications requiring rapid inference times, such as in\n",
    "real-time healthcare systems and edge computing scenarios,\n",
    "where both energy efficiency and computational power are\n",
    "crucial.\n",
    "B. Impact on Model Performance\n",
    "The Neurological Pruning Algorithm significantly improves\n",
    "the efficiency of neural networks, making them more suitable\n",
    "for deployment on edge devices. Key performance metrics\n",
    "such as compression ratio (CR) and accuracy retention\n",
    "(AR) are critical indicators of the algorithm’s effectiveness.\n",
    "For instance, in evaluating a ResNet-18 model, the algorithm\n",
    "achieved a CR of 1, indicating that while the number of\n",
    "parameters remained the same, the model’s efficiency im-\n",
    "proved through selective pruning, reducing computational load\n",
    "without sacrificing performance.\n",
    "Moreover, the algorithm demonstrates the ability to achieve\n",
    "high accuracy retention, with models maintaining or even\n",
    "slightly improving their predictive performance post-pruning.\n",
    "This outcome highlights the algorithm’s capability to effi-\n",
    "ciently remove redundant weights while preserving the core\n",
    "functionality of the network. The result is a model that is\n",
    "not only more computationally efficient but also adaptable\n",
    "across different architectures and datasets, making it suit-\n",
    "able for a wide range of applications, from image classification\n",
    "to natural language processing .\n",
    "In practical terms, the Neurological Pruning Algorithm en-\n",
    "ables faster inference times and lower energy consumption,\n",
    "two crucial aspects for deploying AI models in resource-\n",
    "constrained environments. As such, it holds significant poten-\n",
    "tial for improving the viability of real-time, energy-efficient\n",
    "AI applications, particularly in fields like healthcare, where\n",
    "both speed and sustainability are essential.\n",
    "V. NEUROLOGICAL PRUNING ALGORITHM\n",
    "The Neurological Pruning Algorithm is a structured and\n",
    "adaptive methodology designed to optimize neural networks\n",
    "for resource-constrained environments. This algorithm goes\n",
    "beyond traditional pruning approaches by incorporating layer-\n",
    "specific pruning thresholds that dynamically adjust to the\n",
    "importance of weight activations within each layer. Drawing\n",
    "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
    "inforcement learning, the algorithm strikes an optimal balance\n",
    "between model sparsity and accuracy .\n",
    "A. Algorithm Description\n",
    "The Neurological Pruning Algorithm iteratively refines the\n",
    "neural network through a series of training epochs. During\n",
    "each epoch, weight activations are tracked, normalized, and\n",
    "analyzed to identify low-importance weights. These weights\n",
    "are selectively deactivated using binary masks, ensuring com-\n",
    "putational efficiency without compromising the network’s pre-\n",
    "dictive performance. The algorithm further integrates a fine-\n",
    "tuning phase, enabling the pruned network to recover and\n",
    "enhance its accuracy. The detailed process is presented in\n",
    "Algorithm 1.\n",
    "By dynamically adjusting to layer-specific characteristics\n",
    "and maintaining performance metrics, the Neurological Prun-\n",
    "ing Algorithm ensures a significant reduction in computational\n",
    "and memory requirements, paving the way for efficient deploy-\n",
    "ment of deep learning models on edge devices .\n",
    "B. Input Parameters and Output\n",
    "Inputs:\n",
    "• Pretrained model (M)\n",
    "• Dataset (D)\n",
    "• Prune fraction (p)\n",
    "• Total epochs (E)\n",
    "Output:\n",
    "• Pruned and fine-tuned model (Mpruned)\n",
    "C. Key Features and Advantages\n",
    "The Neurological Pruning Algorithm is designed with sev-\n",
    "eral distinctive features that contribute to its effectiveness\n",
    "in optimizing neural network performance. These features\n",
    "include:\n",
    "Algorithm 1 Simplified Neurological Pruning Algorithm\n",
    "Require: Pretrained model M, Dataset D, Prune fraction p,\n",
    "Total epochs E\n",
    "Ensure: Pruned and fine-tuned model Mpruned\n",
    "1: Initialize binary masks M with all ones\n",
    "2: for each epoch e = 1 to E do\n",
    "3:\n",
    "Compute weight activations A during forward passes\n",
    "4:\n",
    "for each layer i in M do\n",
    "5:\n",
    "Compute pruning threshold Ti as p-quantile of Ai\n",
    "6:\n",
    "Update mask Mi: retain weights with Ai > Ti\n",
    "7:\n",
    "Apply mask to layer: θi ←θi · Mi\n",
    "8:\n",
    "end for\n",
    "9:\n",
    "Evaluate model performance and update metrics\n",
    "10: end for\n",
    "11: Fine-tune Mpruned on D\n",
    "12: return Mpruned\n",
    "• Layer-specific Pruning: The algorithm applies pruning\n",
    "on a per-layer basis, evaluating the weight activations\n",
    "within each layer individually. This approach ensures that\n",
    "the sparsity introduced by pruning is well-distributed and\n",
    "context-dependent, facilitating more nuanced control over\n",
    "model optimization .\n",
    "• Quantile-based Thresholding: A quantile-based strat-\n",
    "egy is employed to determine the pruning threshold for\n",
    "each layer. This method ensures that only weights with\n",
    "minimal contribution to the network’s output are pruned,\n",
    "thereby reducing the risk of adversely affecting model\n",
    "accuracy while enhancing sparsity .\n",
    "• Iterative Pruning and Evaluation: The pruning pro-\n",
    "cess follows an iterative approach, enabling continuous\n",
    "assessment of the model’s performance after each pruning\n",
    "step. This ensures dynamic adjustments to maintain a bal-\n",
    "ance between model sparsity and predictive performance\n",
    "throughout training.\n",
    "• Preservation of Model Accuracy: By adjusting pruning\n",
    "thresholds in response to ongoing performance metrics,\n",
    "the algorithm ensures that model accuracy is not com-\n",
    "promised. The careful tuning of these thresholds allows\n",
    "for significant sparsity while maintaining the robustness\n",
    "of the network’s predictive capabilities .\n",
    "D. Impact on Neural Network Performance\n",
    "The Neurological Pruning Algorithm has been extensively\n",
    "evaluated across various benchmark datasets and model archi-\n",
    "tectures, demonstrating its capacity to achieve the following\n",
    "outcomes:\n",
    "• High Sparsity with Minimal Accuracy Loss: The algo-\n",
    "rithm effectively prunes unnecessary weights, achieving\n",
    "high sparsity levels without causing significant degrada-\n",
    "tion in model accuracy. This outcome is critical for the\n",
    "efficient deployment of models in resource-constrained\n",
    "environments.\n",
    "• Reduced Computational Complexity: By sparsifying\n",
    "the weight matrices, the algorithm reduces the compu-\n",
    "tational load during inference, which translates to faster\n",
    "model execution and lower resource consumption. This\n",
    "characteristic is especially beneficial in edge computing\n",
    "and real-time applications .\n",
    "• Broad Applicability: The algorithm is highly flexible\n",
    "and can be adapted to various neural network architec-\n",
    "tures and datasets, making it suitable for a wide range\n",
    "of tasks, from image recognition to natural language\n",
    "processing .\n",
    "VI. EVALUATION METRICS\n",
    "A. Performance Comparison\n",
    "Fig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\n",
    "TABLE I\n",
    "COMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\n",
    "Metric\n",
    "Unpruned\n",
    "Model\n",
    "Pruned\n",
    "Model\n",
    "Accuracy\n",
    "0.9562\n",
    "0.9583\n",
    "Train Loss\n",
    "0.1414\n",
    "0.1625\n",
    "Inference\n",
    "Time\n",
    "(s)\n",
    "0.0318\n",
    "0.0314\n",
    "No of Parame-\n",
    "ters(million)\n",
    "11.7\n",
    "11.7\n",
    "B. Evaluation Metrics Calculation\n",
    "The key metrics used to evaluate the proposed pruning\n",
    "technique are as follows:\n",
    "Compression Ratio (CR): The Compression Ratio (CR) is\n",
    "calculated as:\n",
    "CR = Total Parameters After Pruning\n",
    "Total Parameters Before Pruning\n",
    "(1)\n",
    "CR = 11.7M\n",
    "11.7M = 1\n",
    "Accuracy Retention (AR): The Accuracy Retention (AR) is\n",
    "calculated as:\n",
    "AR =\n",
    "\u0012Accuracy After Pruning −Baseline Accuracy\n",
    "Baseline Accuracy\n",
    "\u0013\n",
    "×100\n",
    "(2)\n",
    "AR =\n",
    "\u00120.958284 −0.956206\n",
    "0.956206\n",
    "\u0013\n",
    "× 100 = 0.22%\n",
    "This result indicates that the pruned model achieves an ac-\n",
    "curacy improvement of 0.22% when compared to unpruned\n",
    "model.\n",
    "C. Illustration of Results\n",
    "Fig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\n",
    "18.\n",
    "D. Discussion\n",
    "The evaluation metrics highlight the effectiveness of the\n",
    "proposed Neurological Pruning method on the ResNet-18\n",
    "model. The Compression Ratio (CR) remains 1, indicating\n",
    "no reduction in the total number of parameters, which suggests\n",
    "that this approach optimizes the neural network by selectively\n",
    "pruning connections rather than reducing the overall parameter\n",
    "count. The Accuracy Retention (AR), which is approximately\n",
    "0.22%, demonstrates that the Neurological Pruning technique\n",
    "successfully improved the accuracy compared to the baseline\n",
    "model.\n",
    "These results validate the potential of Neurological Pruning\n",
    "as an innovative approach to optimizing deep learning models.\n",
    "By maintaining accuracy while likely enhancing sparsity and\n",
    "computational efficiency, this method presents a promising di-\n",
    "rection for achieving high-performance models under resource\n",
    "constraints.\n",
    "VII. CONCLUSION\n",
    "In summary, the Neurological Pruning Algorithm provides\n",
    "a systematic and principled approach to optimizing neural\n",
    "networks. By balancing the trade-off between sparsity and\n",
    "accuracy, it enables significant reductions in computational\n",
    "complexity without sacrificing the network’s predictive per-\n",
    "formance. Future work will focus on extending the algorithm\n",
    "to dynamic network architectures and incorporating advanced\n",
    "regularization techniques to further enhance its robustness and\n",
    "adaptability in diverse applications.\n",
    "REFERENCES\n",
    "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\n",
    "Fluctuation-based Adaptive Structured Pruning for Large Language\n",
    "Models.\n",
    "[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\n",
    "Simplification of Deep Neural Network-Based Object Detector for Real-\n",
    "Time Edge Computing.\n",
    "[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\n",
    "Devices Object Detection by Filter Pruning.\n",
    "[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\n",
    "and Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\n",
    "Tracking Algorithms in Edge Devices.\n",
    "[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\n",
    "Chen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\n",
    "Compression Method for Object Detection Network for Edge Devices.\n",
    "[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\n",
    "Cao. Absorption Pruning of Deep Neural Network for Object Detection\n",
    "in Remote Sensing Imagery.\n",
    "[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\n",
    "and Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\n",
    "Distribution Detection: An Empirical Survey.\n",
    "[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\n",
    "Sahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\n",
    "[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\n",
    "Compress DNNs.\n",
    "[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\n",
    "derstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\n",
    "[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\n",
    "Proving the Lottery Ticket Hypothesis: Pruning is All You Need.\n",
    "[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\n",
    "Devices via Reconstruction-Based Channel Pruning.\n",
    "[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\n",
    "Determine Reliable Paths on Networks with Random and Correlated\n",
    "Link Travel Times.\n",
    "[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\n",
    "and D. Suganthi. Real-Time Object Detection on Edge Devices Using\n",
    "Mobile Neural Networks.\n",
    "[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\n",
    "Tang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\n",
    "Models.\n",
    "[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\n",
    "Li, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\n",
    "Deng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\n",
    "Detection of Tea Leaves with One Bud and Two Leaves Based on\n",
    "ShuffleNetv2-YOLOv5-Lite-E.\n",
    "\"\"\", \"metadata\": {\"source\": \"geo_facts.txt\", \"page\": 1}},\n",
    "    {\"content\": \"The Great Wall of China is located in China and is a famous landmark.\", \"metadata\": {\"source\": \"world_landmarks.txt\", \"page\": 5}},\n",
    "    {\"content\": \"Paris is known for the Eiffel Tower.\", \"metadata\": {\"source\": \"geo_facts.txt\", \"page\": 2}},\n",
    "]\n",
    "\n",
    "# Convert raw data to LangChain Document objects\n",
    "documents = [Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"]) for doc in documents_data]\n",
    "\n",
    "# Create FAISS vector store directly from documents\n",
    "# This handles embedding the texts and building the index internally\n",
    "try:\n",
    "    vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "    print(\"FAISS vector store created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating FAISS vector store: {e}\")\n",
    "# Exit if vector store creation fails\n",
    "\n",
    "# Step 4: Load Gemini model for text generation\n",
    "\n",
    "# Step 5: Create the Retrieval-Augmented Generation pipeline\n",
    "print(\"Creating RAG chain...\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\", # Other options: \"mmr\", \"similarity_score_threshold\"\n",
    "    search_kwargs={'k': 2} # Retrieve top 2 relevant documents\n",
    ")\n",
    "\n",
    "# Use the recommended from_chain_type method\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # Options: \"stuff\", \"map_reduce\", \"refine\", \"map_rerank\"\n",
    "                        # \"stuff\" puts all retrieved docs into the context\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True # Include source documents in the output\n",
    "    # chain_type_kwargs={\"prompt\": YOUR_CUSTOM_PROMPT} # Optional: customize prompt\n",
    ")\n",
    "print(\"RAG chain created.\")\n",
    "\n",
    "query = \"who is surya narayanaa\"\n",
    "print(f\"\\nQuerying the RAG system with: '{query}'\")\n",
    "\n",
    "try:\n",
    "    result = rag_chain.invoke({\"query\": query})\n",
    "    # Output results\n",
    "    print(\"\\n--- RAG System Output ---\")\n",
    "    print(\"Answer:\", result.get(\"result\", \"No answer found.\"))\n",
    "    print(\"\\nSources:\")\n",
    "    if result.get(\"source_documents\"):\n",
    "        for i, doc in enumerate(result[\"source_documents\"]):\n",
    "            print(f\"  Source {i+1}:\")\n",
    "            print(f\"    Content: {doc.page_content}\")\n",
    "            print(f\"    Metadata: {doc.metadata}\")\n",
    "    else:\n",
    "        print(\"  No source documents found or returned.\")\n",
    "    print(\"--- End of Output ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during RAG chain execution: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
