{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ca4a4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The theory of relativity, introduced by Albert Einstein in 1905 and 1915, is a fundamental concept in modern physics that challenges our traditional understanding of space and time. The theory consists of two main components: special relativity and general relativity.\n",
      "\n",
      "Special Relativity (SR):\n",
      "\n",
      "SR posits that the laws of physics are the same for all observers in uniform motion relative to one another. This means that the passage of time and the length of objects can vary depending on the observer's frame of reference. The theory is based on two postulates:\n",
      "\n",
      "1. The laws of physics are the same for all observers in uniform motion relative to one another.\n",
      "2. The speed of light is constant and unchanging for all observers, regardless of their relative motion.\n",
      "\n",
      "From these postulates, Einstein derived the famous equation E=mc², which shows that mass and energy are equivalent and can be converted into each other. SR also introduced the concept of time dilation, which states that time appears to pass more slowly for an observer in motion relative to a stationary observer. Additionally, SR showed that the length of an object can appear shorter to an observer in motion relative to the object.\n",
      "\n",
      "General Relativity (GR):\n",
      "\n",
      "GR builds upon SR by introducing the concept of gravity as a curvature of spacetime caused by the presence of mass and energy. According to GR, any massive object warps the fabric of spacetime around it, causing other objects to move along curved paths. This theory challenges our classical understanding of gravity as a force that acts between two objects, instead positing that gravity is an inherent property of spacetime itself.\n",
      "\n",
      "GR introduces the concept of gravitational waves, which are ripples in spacetime caused by the acceleration of massive objects. These waves can travel across vast distances and carry information about their source, such as the mass and spin of a star. GR also predicts phenomena such as black holes, where the gravitational pull is so strong that not even light can escape once it falls past the event horizon.\n",
      "\n",
      "Impact of Relativity:\n",
      "\n",
      "The theory of relativity has had a profound impact on our understanding of space and time. It has led to the development of new technologies such as GPS, which relies on the principles of SR to provide accurate location and time information. GR has also influenced the field of astrophysics, helping scientists understand the behavior of celestial objects such as stars and black holes.\n",
      "\n",
      "In addition, relativity has challenged our classical understanding of space and time, leading to a greater appreciation for the nature of reality and the limitations of human perception. It has also inspired new areas of research, such as quantum mechanics and cosmology, which seek to uncover the fundamental laws that govern the universe.\n",
      "\n",
      "In conclusion, the theory of relativity revolutionized our understanding of space and time by introducing the concepts of spacetime curvature and the equivalence of mass and energy. These ideas have had far-reaching implications for physics, astronomy, and our understanding of the universe as a whole.\n"
     ]
    }
   ],
   "source": [
    "# import getpass\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# import os\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "# # if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "# #     os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "# llm = ChatGoogleGenerativeAI(\n",
    "#     model=\"gemini-2.0-flash\",\n",
    "#     temperature=0,\n",
    "#     max_tokens=None,\n",
    "#     timeout=None,\n",
    "#     max_retries=2,\n",
    "# )\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Replace this with your local Ollama model name\n",
    "MODEL_NAME = \"llama2\"\n",
    "\n",
    "# Initialize the Ollama chat model\n",
    "llm = ChatOllama(\n",
    "    model=MODEL_NAME,\n",
    "    base_url=\"http://localhost:11434\",  # Default Ollama server URL\n",
    "    temperature=0,                   # Optional: Adjust sampling temperature\n",
    "    max_retries=3                      # Optional: Number of retries for failed requests\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "response = llm([HumanMessage(content=\"Explain the theory of relativity.\")])\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce1e3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from src.graph_builder._llm import LLMGraphTransformer\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c350687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "text = \"\"\"\n",
    " Neuro Prune: An Adaptive Approach for Efficient\n",
    "Deep Neural Network Optimization on Edge\n",
    "Devices\n",
    "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
    "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
    "grk.cse@psgtech.ac.in\n",
    "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
    "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
    "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
    "Abstract—The optimization of deep neural networks for de-\n",
    "ployment on edge devices is a significant research area due to\n",
    "the demand for applications such as augmented reality, smart\n",
    "cameras, and autonomous navigation. However, deploying large\n",
    "deep learning models on edge devices poses challenges related to\n",
    "computational power, energy consumption, and latency. Pruning\n",
    "is a method to reduce the model size, accelerate inference, and\n",
    "save power. The objective of the paper is to propose the Neuro\n",
    "Prune algorithm and to apply it for the optimization of deep\n",
    "neural networks on edge devices. Efforts have been made to\n",
    "compare pruned and unpruned models. As a result, the pruned\n",
    "model has an accuracy increase of 0.22%.\n",
    "I. INTRODUCTION\n",
    "The rise of intelligent systems, ranging from autonomous\n",
    "drones to augmented reality (AR) devices and smart surveil-\n",
    "lance cameras, has intensified the need for efficient deep neural\n",
    "network deployment on edge devices with constrained com-\n",
    "putational resources. While advanced deep learning models\n",
    "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
    "their deployment on such devices is hindered by significant\n",
    "computational demands, large memory footprints, and high\n",
    "energy consumption. These challenges pose critical barriers to\n",
    "real-time applications and sustainable deployment.\n",
    "This paper builds on existing pruning techniques, includ-\n",
    "ing unstructured pruning, structured pruning, and approaches\n",
    "inspired by quantization, highlighting their strengths and lim-\n",
    "itations. In response to the challenges identified, we introduce\n",
    "Neuro Prune, a novel pruning approach that integrates princi-\n",
    "ples from reinforcement learning and the Lottery Ticket Hy-\n",
    "pothesis to achieve an optimal balance between accuracy, com-\n",
    "putational efficiency, and energy consumption. By integrating\n",
    "these principles, Neuro Prune provides a robust framework\n",
    "for optimizing deep neural networks for deployment on edge\n",
    "devices without sacrificing performance.\n",
    "Neuro Prune reduces model size and computational re-\n",
    "quirements while maintaining or even improving the model’s\n",
    "accuracy. These findings underscore the potential of Neuro\n",
    "Prune to enable real-time, energy-efficient deep learning on\n",
    "resource-constrained devices.\n",
    "Neuro Prune employs a systematic process to optimize\n",
    "neural networks for edge deployment:\n",
    "• Mask Initialization: Each layer’s weights are paired with\n",
    "a binary mask (original_mask), initialized to ones.\n",
    "• Activation Tracking: During forward passes over the\n",
    "dataset, the magnitudes of weight activations are mon-\n",
    "itored and recorded.\n",
    "• Normalize and Prune: The recorded activations are\n",
    "normalized, and a threshold based on the pruning fraction\n",
    "is computed. Using this threshold, the masks are dynam-\n",
    "ically updated to prune unimportant weights.\n",
    "• Evaluate and Log Metrics: After pruning, metrics such\n",
    "as model sparsity and accuracy are computed. A reward\n",
    "metric is derived to balance accuracy and sparsity using\n",
    "a tunable parameter, λweight.\n",
    "• Retrain the Model: The pruned model undergoes retrain-\n",
    "ing to improve the loss in accuracy while maintaining low\n",
    "computational requirements\n",
    "Neuro Prune’s unique combination of reinforcement learn-\n",
    "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
    "pothesis for identifying optimal subnetworks enables it to\n",
    "achieve remarkable efficiency. This approach ensures substan-\n",
    "tial reduction in model size and energy consumption while\n",
    "preserving accuracy and inference speed, making it an ideal\n",
    "solution for edge-based deep learning applications.\n",
    "Through extensive empirical evaluations and theoretical\n",
    "insights, this paper demonstrates how Neuro Pruning outper-\n",
    "forms traditional pruning methods, setting a new benchmark\n",
    "for sustainable, high-performance AI deployment on edge\n",
    "devices. By situating our work within the broader research\n",
    "context, we aim to offer a critical contribution to the ongoing\n",
    "evolution of neural network optimization techniques.\n",
    "II. BACKGROUND\n",
    "A. Need for Pruning Techniques\n",
    "Pruning is a crucial optimization technique for deploying\n",
    "deep neural networks in resource-constrained environments\n",
    "like healthcare. For example, wearable devices like smart-\n",
    "watches that monitor heart rates or portable EEG systems\n",
    "for brain activity analysis require lightweight models due to\n",
    "their limited computational power and memory. Pruning helps\n",
    "address these constraints by reducing the memory footprint\n",
    "and computational load, enabling efficient model deployment.\n",
    "Additionally, it minimizes energy consumption, which is vital\n",
    "for sustainable and real-time healthcare applications. Faster\n",
    "inference enabled by pruning is particularly beneficial for\n",
    "time-sensitive tasks like seizure detection or emergency di-\n",
    "agnostics. Moreover, pruning enhances model interpretability\n",
    "by retaining only the most critical components, which is\n",
    "essential for gaining trust and ensuring regulatory compliance\n",
    "in healthcare. By reducing the hardware and operational costs,\n",
    "pruning also makes deploying advanced AI solutions on edge\n",
    "devices more cost-effective and accessible.\n",
    "B. Types of Pruning Techniques\n",
    "Pruning methods are broadly classified based on granu-\n",
    "larity, timing, and approach. Granularity-based pruning in-\n",
    "cludes structured pruning, which removes entire components\n",
    "such as neurons or filters to streamline computations, and\n",
    "unstructured pruning, which eliminates individual weights\n",
    "for finer optimization, though it often requires specialized\n",
    "hardware. Timing-based pruning involves pre-training pruning,\n",
    "which optimizes models before training; post-training prun-\n",
    "ing, where redundant components are removed from trained\n",
    "models with possible retraining; and dynamic pruning during\n",
    "training, which adjusts models in real-time for improved\n",
    "adaptability. Metric-based pruning uses parameters like weight\n",
    "magnitude or gradient contribution to identify and remove\n",
    "less critical components, while application-specific pruning\n",
    "tailors strategies to specific tasks, such as optimizing for EEG\n",
    "signal analysis, or hardware constraints like GPUs. Together,\n",
    "these methods enable the development of efficient, scalable,\n",
    "and task-specific models, particularly suited for edge device\n",
    "deployment in healthcare and other domains.\n",
    "III. RELATED WORK\n",
    "Jielei Wang et al.[12] introduced an absorption pruning\n",
    "method for object detection in remote sensing imagery. This\n",
    "study achieves efficient compression with minimal accuracy\n",
    "loss but requires careful layer-wise pruning ratio tuning.\n",
    "Jan Muller et al., in their research[12], propose a neural\n",
    "network pruning method for multi-object tracking (MOT) that\n",
    "reduces model size by up to 70\n",
    "Liang Li et al., in their work[9], introduce a novel pruning\n",
    "method for DNNs that utilizes a self-adaptive mechanism\n",
    "based on weight sparsity ratios and a protective reconstruction\n",
    "mechanism. Their approach improves both model compres-\n",
    "sion and accuracy, outperforming state-of-the-art methods on\n",
    "CIFAR-10 and ImageNet datasets.\n",
    "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
    "learning with edge devices like Google Coral AI and Nvidia\n",
    "Jetson Nano. Their work revolutionizes computer vision and\n",
    "real-time tracking but raises energy concerns due to the\n",
    "computational demands of advanced algorithms.\n",
    "Satoru Koda et al.[7] explore how weight pruning in\n",
    "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
    "larly with Mahalanobis-based approaches, by improving global\n",
    "feature extraction and leveraging weights not critical for\n",
    "classification.\n",
    "Shvetha S Kumar et al. analyze and compare three pruning\n",
    "techniques—L1-norm filter pruning, channel pruning, and\n",
    "weight pruning—on CNNs for accuracy and inference time,\n",
    "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
    "pared to GTX1080 Ti in their study[8].\n",
    "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
    "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
    "on stochastic networks, extending it to related problems and\n",
    "demonstrating its efficiency on real-world networks in their\n",
    "study[13].\n",
    "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
    "Training outperforms Post-Training Quantization, and sparsity\n",
    "training with Optimal Reduction pruning improves YOLOv4’s\n",
    "efficiency and edge-device performance, despite potential\n",
    "high-pruning rate drawbacks.\n",
    "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
    "method for DNNs. This approach optimizes inference speed\n",
    "and resource efficiency on edge devices with minimal accuracy\n",
    "loss but requires careful tuning of layer-specific pruning ratios.\n",
    "Yongqi An et al.[1] introduce a retraining-free structured\n",
    "pruning framework for Large Language Models (LLMs). Their\n",
    "method incorporates a fluctuation-based pruning metric, adap-\n",
    "tive structure search, and a bias compensation mechanism to\n",
    "achieve efficient pruning. FLAP significantly enhances infer-\n",
    "ence speed and reduces model size without requiring retrain-\n",
    "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
    "and Wanda-sp on LLaMA models across various benchmarks.\n",
    "From referring to these research papers, we derived an\n",
    "idea for pruning on edge devices. Additionally, our proposed\n",
    "solution is inspired by the research of Jaron Maene et al.[10],\n",
    "which suggests that sparse subnetworks in dense networks can\n",
    "achieve similar accuracy when retrained. This study demon-\n",
    "strates stable training with linear mode connectivity, support-\n",
    "ing the idea that lottery tickets retrain to similar regions, but\n",
    "questions their independence from dense training and iterative\n",
    "pruning.\n",
    "Eran Malach et al.[11] further strengthen the lottery ticket\n",
    "hypothesis by proving that over-parameterized neural networks\n",
    "with random weights always contain a subnetwork matching\n",
    "the accuracy of a target network, without additional training.\n",
    "IV. NEUROLOGICAL PRUNING APPROACH\n",
    "The Neurological Pruning Algorithm is designed to opti-\n",
    "mize neural networks by iteratively pruning less important\n",
    "weights while maintaining high performance. Unlike tradi-\n",
    "tional methods, this approach incorporates adaptive, layer-\n",
    "specific pruning thresholds that dynamically adjust based\n",
    "on weight activations, ensuring efficient model optimization\n",
    "across various stages of training. Drawing inspiration from\n",
    "the Lottery Ticket Hypothesis, which suggests that sparse\n",
    "subnetworks can perform similarly to their dense counterparts\n",
    "when retrained, the algorithm selectively deactivates weights\n",
    "using binary masks to enhance computational efficiency with-\n",
    "out sacrificing accuracy.\n",
    "The algorithm operates in several epochs, during which\n",
    "weight activations are evaluated. Based on a quantile-based\n",
    "strategy, pruning thresholds are established for each layer, en-\n",
    "suring that only weights contributing minimally to the model’s\n",
    "output are removed. After pruning, the model undergoes fine-\n",
    "tuning to restore any lost accuracy and further optimize the\n",
    "network. This step enhances the adaptability and scalability\n",
    "of the pruning method, making it suitable for deployment in\n",
    "resource-constrained environments\n",
    "A. Key Characteristics and Advantages\n",
    "The Neurological Pruning Algorithm boasts several key\n",
    "characteristics that distinguish it from conventional pruning\n",
    "techniques. One notable feature is its layer-specific pruning,\n",
    "which evaluates the importance of weights individually for\n",
    "each layer, leading to a more nuanced and efficient sparsifi-\n",
    "cation process. This approach prevents the model from losing\n",
    "essential features that may otherwise be pruned by a global\n",
    "approach.\n",
    "The algorithm’s use of quantile-based thresholding en-\n",
    "sures that only weights with the least contribution to the\n",
    "model’s output are pruned. This fine-tuned method avoids\n",
    "performance degradation by carefully targeting weights that\n",
    "are less significant to the network’s overall functionality.\n",
    "Furthermore, the pruning process is iterative, allowing for\n",
    "continuous model evaluation and real-time adjustments based\n",
    "on performance feedback, ensuring that accuracy is retained\n",
    "even as sparsity increases.\n",
    "Additionally, the algorithm maintains a careful balance\n",
    "between sparsity and accuracy, enabling a reduction in the\n",
    "model’s computational complexity while preserving its predic-\n",
    "tive capabilities. This characteristic is particularly important\n",
    "for applications requiring rapid inference times, such as in\n",
    "real-time healthcare systems and edge computing scenarios,\n",
    "where both energy efficiency and computational power are\n",
    "crucial.\n",
    "B. Impact on Model Performance\n",
    "The Neurological Pruning Algorithm significantly improves\n",
    "the efficiency of neural networks, making them more suitable\n",
    "for deployment on edge devices. Key performance metrics\n",
    "such as compression ratio (CR) and accuracy retention\n",
    "(AR) are critical indicators of the algorithm’s effectiveness.\n",
    "For instance, in evaluating a ResNet-18 model, the algorithm\n",
    "achieved a CR of 1, indicating that while the number of\n",
    "parameters remained the same, the model’s efficiency im-\n",
    "proved through selective pruning, reducing computational load\n",
    "without sacrificing performance.\n",
    "Moreover, the algorithm demonstrates the ability to achieve\n",
    "high accuracy retention, with models maintaining or even\n",
    "slightly improving their predictive performance post-pruning.\n",
    "This outcome highlights the algorithm’s capability to effi-\n",
    "ciently remove redundant weights while preserving the core\n",
    "functionality of the network. The result is a model that is\n",
    "not only more computationally efficient but also adaptable\n",
    "across different architectures and datasets, making it suit-\n",
    "able for a wide range of applications, from image classification\n",
    "to natural language processing .\n",
    "In practical terms, the Neurological Pruning Algorithm en-\n",
    "ables faster inference times and lower energy consumption,\n",
    "two crucial aspects for deploying AI models in resource-\n",
    "constrained environments. As such, it holds significant poten-\n",
    "tial for improving the viability of real-time, energy-efficient\n",
    "AI applications, particularly in fields like healthcare, where\n",
    "both speed and sustainability are essential.\n",
    "V. NEUROLOGICAL PRUNING ALGORITHM\n",
    "The Neurological Pruning Algorithm is a structured and\n",
    "adaptive methodology designed to optimize neural networks\n",
    "for resource-constrained environments. This algorithm goes\n",
    "beyond traditional pruning approaches by incorporating layer-\n",
    "specific pruning thresholds that dynamically adjust to the\n",
    "importance of weight activations within each layer. Drawing\n",
    "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
    "inforcement learning, the algorithm strikes an optimal balance\n",
    "between model sparsity and accuracy .\n",
    "A. Algorithm Description\n",
    "The Neurological Pruning Algorithm iteratively refines the\n",
    "neural network through a series of training epochs. During\n",
    "each epoch, weight activations are tracked, normalized, and\n",
    "analyzed to identify low-importance weights. These weights\n",
    "are selectively deactivated using binary masks, ensuring com-\n",
    "putational efficiency without compromising the network’s pre-\n",
    "dictive performance. The algorithm further integrates a fine-\n",
    "tuning phase, enabling the pruned network to recover and\n",
    "enhance its accuracy. The detailed process is presented in\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6d708297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 1) Read your full text (e.g. from PDF)\n",
    "full_text = open(\"op.txt\", encoding=\"utf-8\").read()\n",
    "\n",
    "# 2) Wrap in a single Document\n",
    "original_doc = Document(page_content=full_text, metadata={\"doc_id\":\"mydoc\"})\n",
    "\n",
    "# 3) Configure the splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    # target chunk size in characters (not tokens)\n",
    "    chunk_size=500,\n",
    "    # how many characters of overlap between chunks\n",
    "    chunk_overlap=100,\n",
    "    # what to split on first, second, etc.\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "# 4) Produce a list of Documents, each with its own chunk\n",
    "chunked_docs = splitter.split_documents([original_doc])\n",
    "\n",
    "# Inspect\n",
    "# for i, doc in enumerate(chunked_docs[:3]):\n",
    "#     print(f\"--- chunk {i} ({len(doc.page_content)} chars) ---\")\n",
    "#     print(doc.page_content[:200].replace(\"\\n\",\" \"), \"…\\n\")\n",
    "print(len(chunked_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f113e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# documents = [Document(page_content=text)]\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(chunked_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93a40de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:20\n",
      "Relationships:[Relationship(source=Node(id='Karpagam G R', type='Person', properties={}), target=Node(id='Department Of Computer Science And Engineering', type='Department', properties={}), type='PROFESSOR', properties={'source_text': 'Neuro Prune: An Adaptive Approach for Efficient\\n   Deep Neural Network Optimization on Edge\\n   Devices\\n   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n   1Professor, Department of Computer Science and Engineering, PSG College of Technology\\n   grk.cse@psgtech.ac.in\\n   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\n   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\\n   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com'}), Relationship(source=Node(id='Department Of Computer Science And Engineering', type='Department', properties={}), target=Node(id='Psg College Of Technology', type='Organization', properties={}), type='LOCATED_IN', properties={'source_text': 'Neuro Prune: An Adaptive Approach for Efficient\\n   Deep Neural Network Optimization on Edge\\n   Devices\\n   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n   1Professor, Department of Computer Science and Engineering, PSG College of Technology\\n   grk.cse@psgtech.ac.in\\n   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\n   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\\n   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com'}), Relationship(source=Node(id='Adhish Krishna S', type='Person', properties={}), target=Node(id='Psg College Of Technology', type='Organization', properties={}), type='STUDENT', properties={'source_text': 'Neuro Prune: An Adaptive Approach for Efficient\\n   Deep Neural Network Optimization on Edge\\n   Devices\\n   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n   1Professor, Department of Computer Science and Engineering, PSG College of Technology\\n   grk.cse@psgtech.ac.in\\n   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\n   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\\n   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com'}), Relationship(source=Node(id='Mohana Kumar P', type='Person', properties={}), target=Node(id='Psg College Of Technology', type='Organization', properties={}), type='STUDENT', properties={'source_text': 'Neuro Prune: An Adaptive Approach for Efficient\\n   Deep Neural Network Optimization on Edge\\n   Devices\\n   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n   1Professor, Department of Computer Science and Engineering, PSG College of Technology\\n   grk.cse@psgtech.ac.in\\n   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\n   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\\n   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com'}), Relationship(source=Node(id='Sanjay J', type='Person', properties={}), target=Node(id='Psg College Of Technology', type='Organization', properties={}), type='STUDENT', properties={'source_text': 'Neuro Prune: An Adaptive Approach for Efficient\\n   Deep Neural Network Optimization on Edge\\n   Devices\\n   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n   1Professor, Department of Computer Science and Engineering, PSG College of Technology\\n   grk.cse@psgtech.ac.in\\n   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\n   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\\n   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com'}), Relationship(source=Node(id='Surya Narayanaa N T', type='Person', properties={}), target=Node(id='Psg College Of Technology', type='Organization', properties={}), type='STUDENT', properties={'source_text': 'Neuro Prune: An Adaptive Approach for Efficient\\n   Deep Neural Network Optimization on Edge\\n   Devices\\n   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n   1Professor, Department of Computer Science and Engineering, PSG College of Technology\\n   grk.cse@psgtech.ac.in\\n   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\n   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\\n   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com'}), Relationship(source=Node(id='Augmented Reality', type='Application', properties={}), target=Node(id='Edge Devices', type='Device', properties={}), type='DEPLOYMENT', properties={'source_text': 'sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\n   Abstract—The optimization of deep neural networks for de-\\n   ployment on edge devices is a significant research area due to\\n   the demand for applications such as augmented reality, smart\\n   cameras, and autonomous navigation. However, deploying large\\n   deep learning models on edge devices poses challenges related to\\n   computational power, energy consumption, and latency. Pruning'}), Relationship(source=Node(id='Smart Cameras', type='Application', properties={}), target=Node(id='Edge Devices', type='Device', properties={}), type='DEPLOYMENT', properties={'source_text': 'sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\n   Abstract—The optimization of deep neural networks for de-\\n   ployment on edge devices is a significant research area due to\\n   the demand for applications such as augmented reality, smart\\n   cameras, and autonomous navigation. However, deploying large\\n   deep learning models on edge devices poses challenges related to\\n   computational power, energy consumption, and latency. Pruning'}), Relationship(source=Node(id='Autonomous Navigation', type='Application', properties={}), target=Node(id='Edge Devices', type='Device', properties={}), type='DEPLOYMENT', properties={'source_text': 'sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\n   Abstract—The optimization of deep neural networks for de-\\n   ployment on edge devices is a significant research area due to\\n   the demand for applications such as augmented reality, smart\\n   cameras, and autonomous navigation. However, deploying large\\n   deep learning models on edge devices poses challenges related to\\n   computational power, energy consumption, and latency. Pruning'}), Relationship(source=Node(id='Deep Learning Models', type='Model', properties={}), target=Node(id='Edge Devices', type='Device', properties={}), type='DEPLOYMENT', properties={'source_text': 'sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\n   Abstract—The optimization of deep neural networks for de-\\n   ployment on edge devices is a significant research area due to\\n   the demand for applications such as augmented reality, smart\\n   cameras, and autonomous navigation. However, deploying large\\n   deep learning models on edge devices poses challenges related to\\n   computational power, energy consumption, and latency. Pruning'}), Relationship(source=Node(id='Pruning', type='Method', properties={}), target=Node(id='Model Size', type='Concept', properties={}), type='REDUCE', properties={'source_text': 'computational power, energy consumption, and latency. Pruning\\n   is a method to reduce the model size, accelerate inference, and\\n   save power. The objective of the paper is to propose the Neuro\\n   Prune algorithm and to apply it for the optimization of deep\\n   neural networks on edge devices. Efforts have been made to\\n   compare pruned and unpruned models. As a result, the pruned\\n   model has an accuracy increase of 0.22%.'}), Relationship(source=Node(id='Pruning', type='Method', properties={}), target=Node(id='Inference', type='Concept', properties={}), type='ACCELERATE', properties={'source_text': 'computational power, energy consumption, and latency. Pruning\\n   is a method to reduce the model size, accelerate inference, and\\n   save power. The objective of the paper is to propose the Neuro\\n   Prune algorithm and to apply it for the optimization of deep\\n   neural networks on edge devices. Efforts have been made to\\n   compare pruned and unpruned models. As a result, the pruned\\n   model has an accuracy increase of 0.22%.'}), Relationship(source=Node(id='Pruning', type='Method', properties={}), target=Node(id='Power', type='Concept', properties={}), type='SAVE', properties={'source_text': 'computational power, energy consumption, and latency. Pruning\\n   is a method to reduce the model size, accelerate inference, and\\n   save power. The objective of the paper is to propose the Neuro\\n   Prune algorithm and to apply it for the optimization of deep\\n   neural networks on edge devices. Efforts have been made to\\n   compare pruned and unpruned models. As a result, the pruned\\n   model has an accuracy increase of 0.22%.'}), Relationship(source=Node(id='Neuro Prune Algorithm', type='Algorithm', properties={}), target=Node(id='Deep Neural Networks', type='Model', properties={}), type='APPLY', properties={'source_text': 'computational power, energy consumption, and latency. Pruning\\n   is a method to reduce the model size, accelerate inference, and\\n   save power. The objective of the paper is to propose the Neuro\\n   Prune algorithm and to apply it for the optimization of deep\\n   neural networks on edge devices. Efforts have been made to\\n   compare pruned and unpruned models. As a result, the pruned\\n   model has an accuracy increase of 0.22%.'}), Relationship(source=Node(id='Deep Neural Networks', type='Model', properties={}), target=Node(id='Edge Devices', type='Device', properties={}), type='ON', properties={'source_text': 'computational power, energy consumption, and latency. Pruning\\n   is a method to reduce the model size, accelerate inference, and\\n   save power. The objective of the paper is to propose the Neuro\\n   Prune algorithm and to apply it for the optimization of deep\\n   neural networks on edge devices. Efforts have been made to\\n   compare pruned and unpruned models. As a result, the pruned\\n   model has an accuracy increase of 0.22%.'}), Relationship(source=Node(id='Pruned Model', type='Model', properties={}), target=Node(id='Accuracy', type='Concept', properties={}), type='INCREASE', properties={'source_text': 'computational power, energy consumption, and latency. Pruning\\n   is a method to reduce the model size, accelerate inference, and\\n   save power. The objective of the paper is to propose the Neuro\\n   Prune algorithm and to apply it for the optimization of deep\\n   neural networks on edge devices. Efforts have been made to\\n   compare pruned and unpruned models. As a result, the pruned\\n   model has an accuracy increase of 0.22%.'})]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nodes:{len(graph_documents[0].nodes)}\")\n",
    "print(f\"Relationships:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e9a1ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### GraphDocument 1\n",
      "**Nodes:**\n",
      "\n",
      "  1. **Node ID:** Karpagam G R\n",
      "     **Type:** Person\n",
      "     **Properties:**\n",
      "       - **Source_texts:**\n",
      "         ```\n",
      "         Neuro Prune: An Adaptive Approach for Efficient\n",
      "   Deep Neural Network Optimization on Edge\n",
      "   Devices\n",
      "   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
      "   1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
      "   grk.cse@psgtech.ac.in\n",
      "   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
      "   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
      "   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
      "         ```\n",
      "       - **Source_doc_id:** mydoc\n",
      "\n",
      "  2. **Node ID:** Adhish Krishna S\n",
      "     **Type:** Person\n",
      "     **Properties:**\n",
      "       - **Source_texts:**\n",
      "         ```\n",
      "         Neuro Prune: An Adaptive Approach for Efficient\n",
      "   Deep Neural Network Optimization on Edge\n",
      "   Devices\n",
      "   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
      "   1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
      "   grk.cse@psgtech.ac.in\n",
      "   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
      "   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
      "   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
      "         ```\n",
      "       - **Source_doc_id:** mydoc\n",
      "\n",
      "  3. **Node ID:** Mohana Kumar P\n",
      "     **Type:** Person\n",
      "     **Properties:**\n",
      "       - **Source_texts:**\n",
      "         ```\n",
      "         Neuro Prune: An Adaptive Approach for Efficient\n",
      "   Deep Neural Network Optimization on Edge\n",
      "   Devices\n",
      "   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
      "   1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
      "   grk.cse@psgtech.ac.in\n",
      "   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
      "   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
      "   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
      "         ```\n",
      "       - **Source_doc_id:** mydoc\n",
      "\n",
      "  4. **Node ID:** Sanjay J\n",
      "     **Type:** Person\n",
      "     **Properties:**\n",
      "       - **Source_texts:**\n",
      "         ```\n",
      "         Neuro Prune: An Adaptive Approach for Efficient\n",
      "   Deep Neural Network Optimization on Edge\n",
      "   Devices\n",
      "   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
      "   1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
      "   grk.cse@psgtech.ac.in\n",
      "   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
      "   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
      "   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
      "         ```\n",
      "       - **Source_doc_id:** mydoc\n",
      "\n",
      "  5. **Node ID:** Surya Narayanaa N T\n",
      "     **Type:** Person\n",
      "     **Properties:**\n",
      "       - **Source_texts:**\n",
      "         ```\n",
      "         Neuro Prune: An Adaptive Approach for Efficient\n",
      "   Deep Neural Network Optimization on Edge\n",
      "   Devices\n",
      "   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
      "   1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
      "   grk.cse@psgtech.ac.in\n",
      "   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
      "   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
      "   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
      "         ```\n",
      "       - **Source_doc_id:** mydoc\n",
      "\n",
      "  6. **Node ID:** Department Of Computer Science And Engineering\n",
      "     **Type:** Department\n",
      "     **Properties:**\n",
      "       - **Source_texts:**\n",
      "         ```\n",
      "         Neuro Prune: An Adaptive Approach for Efficient\n",
      "   Deep Neural Network Optimization on Edge\n",
      "   Devices\n",
      "   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
      "   1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
      "   grk.cse@psgtech.ac.in\n",
      "   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
      "   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
      "   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
      "         ```\n",
      "       - **Source_doc_id:** mydoc\n",
      "\n",
      "  7. **Node ID:** Psg College Of Technology\n",
      "     **Type:** Organization\n",
      "     **Properties:**\n",
      "       - **Source_texts:**\n",
      "         ```\n",
      "         Neuro Prune: An Adaptive Approach for Efficient\n",
      "   Deep Neural Network Optimization on Edge\n",
      "   Devices\n",
      "   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
      "   1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
      "   grk.cse@psgtech.ac.in\n",
      "   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
      "   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
      "   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
      "         ```\n",
      "       - **Source_doc_id:** mydoc\n",
      "\n",
      "  8. **Node ID:** Augmented Reality\n",
      "     **Type:** Application\n",
      "     **Properties:**\n",
      "       - **Source_texts:**\n",
      "         ```\n",
      "         sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
      "   Abstract—The optimization of deep neural networks for de-\n",
      "   ployment on edge devices is a significant research area due to\n",
      "   the demand for applications such as augmented reality, smart\n",
      "   cameras, and autonomous navigation. However, deploying large\n",
      "   deep learning models on edge devices poses challenges related to\n",
      "   computational power, energy consumption, and latency. Pruning\n",
      "         ```\n",
      "       - **Source_doc_id:** mydoc\n",
      "\n",
      "  9. **Node ID:** Smart Cameras\n",
      "     **Type:** Application\n",
      "     **Properties:**\n",
      "       - **Source_texts:**\n",
      "         ```\n",
      "         sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
      "   Abstract—The optimization of deep neural networks for de-\n",
      "   ployment on edge devices is a significant research area due to\n",
      "   the demand for applications such as augmented reality, smart\n",
      "   cameras, and autonomous navigation. However, deploying large\n",
      "   deep learning models on edge devices poses challenges related to\n",
      "   computational power, energy consumption, and latency. Pruning\n",
      "         ```\n",
      "       - **Source_doc_id:** mydoc\n",
      "\n",
      "  10. **Node ID:** Autonomous Navigation\n",
      "     **Type:** Application\n",
      "     **Properties:**\n",
      "       - **Source_texts:**\n",
      "         ```\n",
      "         sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
      "   Abstract—The optimization of deep neural networks for de-\n",
      "   ployment on edge devices is a significant research area due to\n",
      "   the demand for applications such as augmented reality, smart\n",
      "   cameras, and autonomous navigation. However, deploying large\n",
      "   deep learning models on edge devices poses challenges related to\n",
      "   computational power, energy consumption, and latency. Pruning\n",
      "         ```\n",
      "       - **Source_doc_id:** mydoc\n",
      "\n",
      "  11. **Node ID:** Deep Learning Models\n",
      "     **Type:** Model\n",
      "     **Properties:**\n",
      "       - **Source_texts:**\n",
      "         ```\n",
      "         sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
      "   Abstract—The optimization of deep neural networks for de-\n",
      "   ployment on edge devices is a significant research area due to\n",
      "   the demand for applications such as augmented reality, smart\n",
      "   cameras, and autonomous navigation. However, deploying large\n",
      "   deep learning models on edge devices poses challenges related to\n",
      "   computational power, energy consumption, and latency. Pruning\n",
      "         ```\n",
      "       - **Source_doc_id:** mydoc\n",
      "\n",
      "  12. **Node ID:** Edge Devices\n",
      "     **Type:** Device\n",
      "     **Properties:**\n",
      "       - **Source_texts:**\n",
      "         ```\n",
      "         sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
      "   Abstract—The optimization of deep neural networks for de-\n",
      "   ployment on edge devices is a significant research area due to\n",
      "   the demand for applications such as augmented reality, smart\n",
      "   cameras, and autonomous navigation. However, deploying large\n",
      "   deep learning models on edge devices poses challenges related to\n",
      "   computational power, energy consumption, and latency. Pruning\n",
      "         ```\n",
      "         ```\n",
      "         computational power, energy consumption, and latency. Pruning\n",
      "   is a method to reduce the model size, accelerate inference, and\n",
      "   save power. The objective of the paper is to propose the Neuro\n",
      "   Prune algorithm and to apply it for the optimization of deep\n",
      "   neural networks on edge devices. Efforts have been made to\n",
      "   compare pruned and unpruned models. As a result, the pruned\n",
      "   model has an accuracy increase of 0.22%.\n",
      "         ```\n",
      "       - **Source_doc_id:** mydoc\n",
      "\n",
      "  13. **Node ID:** Computational Power\n",
      "     **Type:** Concept\n",
      "     **Properties:**\n",
      "       - **Source_texts:**\n",
      "         ```\n",
      "         computational power, energy consumption, and latency. Pruning\n",
      "   is a method to reduce the model size, accelerate inference, and\n",
      "   save power. The objective of the paper is to propose the Neuro\n",
      "   Prune algorithm and to apply it for the optimization of deep\n",
      "   neural networks on edge devices. Efforts have been made to\n",
      "   compare pruned and unpruned models. As a result, the pruned\n",
      "   model has an accuracy increase of 0.22%.\n",
      "         ```\n",
      "       - **Source_doc_id:** mydoc\n",
      "\n",
      "  14. **Node ID:** Energy Consumption\n",
      "     **Type:** Concept\n",
      "     **Properties:**\n",
      "       - **Source_texts:**\n",
      "         ```\n",
      "         computational power, energy consumption, and latency. Pruning\n",
      "   is a method to reduce the model size, accelerate inference, and\n",
      "   save power. The objective of the paper is to propose the Neuro\n",
      "   Prune algorithm and to apply it for the optimization of deep\n",
      "   neural networks on edge devices. Efforts have been made to\n",
      "   compare pruned and unpruned models. As a result, the pruned\n",
      "   model has an accuracy increase of 0.22%.\n",
      "         ```\n",
      "       - **Source_doc_id:** mydoc\n",
      "\n",
      "  15. **Node ID:** Latency\n",
      "     **Type:** Concept\n",
      "     **Properties:**\n",
      "       - **Source_texts:**\n",
      "         ```\n",
      "         computational power, energy consumption, and latency. Pruning\n",
      "   is a method to reduce the model size, accelerate inference, and\n",
      "   save power. The objective of the paper is to propose the Neuro\n",
      "   Prune algorithm and to apply it for the optimization of deep\n",
      "   neural networks on edge devices. Efforts have been made to\n",
      "   compare pruned and unpruned models. As a result, the pruned\n",
      "   model has an accuracy increase of 0.22%.\n",
      "         ```\n",
      "       - **Source_doc_id:** mydoc\n",
      "\n",
      "  16. **Node ID:** Pruning\n",
      "     **Type:** Method\n",
      "     **Properties:**\n",
      "       - **Source_texts:**\n",
      "         ```\n",
      "         computational power, energy consumption, and latency. Pruning\n",
      "   is a method to reduce the model size, accelerate inference, and\n",
      "   save power. The objective of the paper is to propose the Neuro\n",
      "   Prune algorithm and to apply it for the optimization of deep\n",
      "   neural networks on edge devices. Efforts have been made to\n",
      "   compare pruned and unpruned models. As a result, the pruned\n",
      "   model has an accuracy increase of 0.22%.\n",
      "         ```\n",
      "       - **Source_doc_id:** mydoc\n",
      "\n",
      "  17. **Node ID:** Neuro Prune Algorithm\n",
      "     **Type:** Algorithm\n",
      "     **Properties:**\n",
      "       - **Source_texts:**\n",
      "         ```\n",
      "         computational power, energy consumption, and latency. Pruning\n",
      "   is a method to reduce the model size, accelerate inference, and\n",
      "   save power. The objective of the paper is to propose the Neuro\n",
      "   Prune algorithm and to apply it for the optimization of deep\n",
      "   neural networks on edge devices. Efforts have been made to\n",
      "   compare pruned and unpruned models. As a result, the pruned\n",
      "   model has an accuracy increase of 0.22%.\n",
      "         ```\n",
      "       - **Source_doc_id:** mydoc\n",
      "\n",
      "  18. **Node ID:** Deep Neural Networks\n",
      "     **Type:** Model\n",
      "     **Properties:**\n",
      "       - **Source_texts:**\n",
      "         ```\n",
      "         computational power, energy consumption, and latency. Pruning\n",
      "   is a method to reduce the model size, accelerate inference, and\n",
      "   save power. The objective of the paper is to propose the Neuro\n",
      "   Prune algorithm and to apply it for the optimization of deep\n",
      "   neural networks on edge devices. Efforts have been made to\n",
      "   compare pruned and unpruned models. As a result, the pruned\n",
      "   model has an accuracy increase of 0.22%.\n",
      "         ```\n",
      "       - **Source_doc_id:** mydoc\n",
      "\n",
      "  19. **Node ID:** Pruned Model\n",
      "     **Type:** Model\n",
      "     **Properties:**\n",
      "       - **Source_texts:**\n",
      "         ```\n",
      "         computational power, energy consumption, and latency. Pruning\n",
      "   is a method to reduce the model size, accelerate inference, and\n",
      "   save power. The objective of the paper is to propose the Neuro\n",
      "   Prune algorithm and to apply it for the optimization of deep\n",
      "   neural networks on edge devices. Efforts have been made to\n",
      "   compare pruned and unpruned models. As a result, the pruned\n",
      "   model has an accuracy increase of 0.22%.\n",
      "         ```\n",
      "       - **Source_doc_id:** mydoc\n",
      "\n",
      "  20. **Node ID:** Unpruned Models\n",
      "     **Type:** Model\n",
      "     **Properties:**\n",
      "       - **Source_texts:**\n",
      "         ```\n",
      "         computational power, energy consumption, and latency. Pruning\n",
      "   is a method to reduce the model size, accelerate inference, and\n",
      "   save power. The objective of the paper is to propose the Neuro\n",
      "   Prune algorithm and to apply it for the optimization of deep\n",
      "   neural networks on edge devices. Efforts have been made to\n",
      "   compare pruned and unpruned models. As a result, the pruned\n",
      "   model has an accuracy increase of 0.22%.\n",
      "         ```\n",
      "       - **Source_doc_id:** mydoc\n"
     ]
    }
   ],
   "source": [
    "def pretty_print_graph_documents(graph_documents):\n",
    "    for idx, graph_doc in enumerate(graph_documents, start=1):\n",
    "        print(f\"\\n### GraphDocument {idx}\")\n",
    "        print(\"**Nodes:**\")\n",
    "        for node_idx, node in enumerate(graph_doc.nodes, start=1):\n",
    "            print(f\"\\n  {node_idx}. **Node ID:** {node.id}\")\n",
    "            print(f\"     **Type:** {node.type}\")\n",
    "            print(f\"     **Properties:**\")\n",
    "            for key, value in node.properties.items():\n",
    "                if key == \"source_texts\":\n",
    "                    print(f\"       - **{key.capitalize()}:**\")\n",
    "                    for text in value:\n",
    "                        print(f\"         ```\\n         {text}\\n         ```\")\n",
    "                else:\n",
    "                    print(f\"       - **{key.capitalize()}:** {value}\")\n",
    "\n",
    "# Assuming `graph_documents` is the variable holding the output\n",
    "pretty_print_graph_documents(graph_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "956bc1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"iambatman\"\n",
    "load_dotenv()\n",
    "graph = Neo4jGraph(refresh_schema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea232a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_graph_documents(graph_documents,\n",
    "                        include_source=True,             # Enable traceability\n",
    "                        baseEntityLabel=True    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d07cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a554b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing query tool...\n",
      "{'n': {'id': 'Viviana Crescitelli', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}, 'r': ({'id': 'Viviana Crescitelli', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}, 'PROPOSED', {'id': 'a filter-level pruning method for DNNs', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}), 'm': {'id': 'a filter-level pruning method for DNNs', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}}\n",
      "{'n': {'id': 'demonstrating', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}, 'r': ({'id': 'demonstrating', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}, 'RELATED_TO', {'id': 'real-world networks', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}), 'm': {'id': 'real-world networks', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}}\n",
      "{'n': {'id': 'Optimal Reduction pruning', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}, 'r': ({'id': 'Optimal Reduction pruning', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}, 'USED_IN', {'id': 'sparsity training with Optimal Reduction pruning improves YOLOv4’s efficiency and edge-device performance', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}), 'm': {'id': 'sparsity training with Optimal Reduction pruning improves YOLOv4’s efficiency and edge-device performance', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}}\n",
      "{'n': {'id': 'subnetworks', 'source_texts': ['subnetworks can perform similarly to their dense counterparts\\n   when retrained, the algorithm selectively deactivates weights\\n   using binary masks to enhance computational efficiency with-\\n   out sacrificing accuracy.\\n   The algorithm operates in several epochs, during which\\n   weight activations are evaluated. Based on a quantile-based\\n   strategy, pruning thresholds are established for each layer, en-\\n   suring that only weights contributing minimally to the model’s'], 'source_doc_id': 'mydoc'}, 'r': ({'id': 'subnetworks', 'source_texts': ['subnetworks can perform similarly to their dense counterparts\\n   when retrained, the algorithm selectively deactivates weights\\n   using binary masks to enhance computational efficiency with-\\n   out sacrificing accuracy.\\n   The algorithm operates in several epochs, during which\\n   weight activations are evaluated. Based on a quantile-based\\n   strategy, pruning thresholds are established for each layer, en-\\n   suring that only weights contributing minimally to the model’s'], 'source_doc_id': 'mydoc'}, 'HAS_FEATURE', {'id': 'algorithm', 'source_texts': ['subnetworks can perform similarly to their dense counterparts\\n   when retrained, the algorithm selectively deactivates weights\\n   using binary masks to enhance computational efficiency with-\\n   out sacrificing accuracy.\\n   The algorithm operates in several epochs, during which\\n   weight activations are evaluated. Based on a quantile-based\\n   strategy, pruning thresholds are established for each layer, en-\\n   suring that only weights contributing minimally to the model’s', 'essential features that may otherwise be pruned by a global\\n   approach.\\n   The algorithm’s use of quantile-based thresholding en-\\n   sures that only weights with the least contribution to the\\n   model’s output are pruned. This fine-tuned method avoids\\n   performance degradation by carefully targeting weights that\\n   are less significant to the network’s overall functionality.\\n   Furthermore, the pruning process is iterative, allowing for'], 'source_doc_id': 'mydoc'}), 'm': {'id': 'algorithm', 'source_texts': ['subnetworks can perform similarly to their dense counterparts\\n   when retrained, the algorithm selectively deactivates weights\\n   using binary masks to enhance computational efficiency with-\\n   out sacrificing accuracy.\\n   The algorithm operates in several epochs, during which\\n   weight activations are evaluated. Based on a quantile-based\\n   strategy, pruning thresholds are established for each layer, en-\\n   suring that only weights contributing minimally to the model’s', 'essential features that may otherwise be pruned by a global\\n   approach.\\n   The algorithm’s use of quantile-based thresholding en-\\n   sures that only weights with the least contribution to the\\n   model’s output are pruned. This fine-tuned method avoids\\n   performance degradation by carefully targeting weights that\\n   are less significant to the network’s overall functionality.\\n   Furthermore, the pruning process is iterative, allowing for'], 'source_doc_id': 'mydoc'}}\n",
      "{'n': {'id': 'algorithm', 'source_texts': ['subnetworks can perform similarly to their dense counterparts\\n   when retrained, the algorithm selectively deactivates weights\\n   using binary masks to enhance computational efficiency with-\\n   out sacrificing accuracy.\\n   The algorithm operates in several epochs, during which\\n   weight activations are evaluated. Based on a quantile-based\\n   strategy, pruning thresholds are established for each layer, en-\\n   suring that only weights contributing minimally to the model’s', 'essential features that may otherwise be pruned by a global\\n   approach.\\n   The algorithm’s use of quantile-based thresholding en-\\n   sures that only weights with the least contribution to the\\n   model’s output are pruned. This fine-tuned method avoids\\n   performance degradation by carefully targeting weights that\\n   are less significant to the network’s overall functionality.\\n   Furthermore, the pruning process is iterative, allowing for'], 'source_doc_id': 'mydoc'}, 'r': ({'id': 'algorithm', 'source_texts': ['subnetworks can perform similarly to their dense counterparts\\n   when retrained, the algorithm selectively deactivates weights\\n   using binary masks to enhance computational efficiency with-\\n   out sacrificing accuracy.\\n   The algorithm operates in several epochs, during which\\n   weight activations are evaluated. Based on a quantile-based\\n   strategy, pruning thresholds are established for each layer, en-\\n   suring that only weights contributing minimally to the model’s', 'essential features that may otherwise be pruned by a global\\n   approach.\\n   The algorithm’s use of quantile-based thresholding en-\\n   sures that only weights with the least contribution to the\\n   model’s output are pruned. This fine-tuned method avoids\\n   performance degradation by carefully targeting weights that\\n   are less significant to the network’s overall functionality.\\n   Furthermore, the pruning process is iterative, allowing for'], 'source_doc_id': 'mydoc'}, 'USES', {'id': 'quantile-based thresholding', 'source_texts': ['essential features that may otherwise be pruned by a global\\n   approach.\\n   The algorithm’s use of quantile-based thresholding en-\\n   sures that only weights with the least contribution to the\\n   model’s output are pruned. This fine-tuned method avoids\\n   performance degradation by carefully targeting weights that\\n   are less significant to the network’s overall functionality.\\n   Furthermore, the pruning process is iterative, allowing for'], 'source_doc_id': 'mydoc'}), 'm': {'id': 'quantile-based thresholding', 'source_texts': ['essential features that may otherwise be pruned by a global\\n   approach.\\n   The algorithm’s use of quantile-based thresholding en-\\n   sures that only weights with the least contribution to the\\n   model’s output are pruned. This fine-tuned method avoids\\n   performance degradation by carefully targeting weights that\\n   are less significant to the network’s overall functionality.\\n   Furthermore, the pruning process is iterative, allowing for'], 'source_doc_id': 'mydoc'}}\n",
      "{'n': {'id': 'b85404dfb4f434847a08e5ac0c17e749', 'text': 'Neuro Prune: An Adaptive Approach for Efficient\\n   Deep Neural Network Optimization on Edge\\n   Devices\\n   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n   1Professor, Department of Computer Science and Engineering, PSG College of Technology\\n   grk.cse@psgtech.ac.in\\n   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\n   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\\n   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com', 'doc_id': 'mydoc'}, 'r': ({'id': 'b85404dfb4f434847a08e5ac0c17e749', 'text': 'Neuro Prune: An Adaptive Approach for Efficient\\n   Deep Neural Network Optimization on Edge\\n   Devices\\n   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n   1Professor, Department of Computer Science and Engineering, PSG College of Technology\\n   grk.cse@psgtech.ac.in\\n   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\n   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\\n   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com', 'doc_id': 'mydoc'}, 'MENTIONS', {'id': 'a filter-level pruning method for DNNs', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}), 'm': {'id': 'a filter-level pruning method for DNNs', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}}\n",
      "{'n': {'id': 'b85404dfb4f434847a08e5ac0c17e749', 'text': 'Neuro Prune: An Adaptive Approach for Efficient\\n   Deep Neural Network Optimization on Edge\\n   Devices\\n   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n   1Professor, Department of Computer Science and Engineering, PSG College of Technology\\n   grk.cse@psgtech.ac.in\\n   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\n   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\\n   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com', 'doc_id': 'mydoc'}, 'r': ({'id': 'b85404dfb4f434847a08e5ac0c17e749', 'text': 'Neuro Prune: An Adaptive Approach for Efficient\\n   Deep Neural Network Optimization on Edge\\n   Devices\\n   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n   1Professor, Department of Computer Science and Engineering, PSG College of Technology\\n   grk.cse@psgtech.ac.in\\n   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\n   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\\n   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com', 'doc_id': 'mydoc'}, 'MENTIONS', {'id': 'Viviana Crescitelli', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}), 'm': {'id': 'Viviana Crescitelli', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}}\n",
      "{'n': {'id': 'b85404dfb4f434847a08e5ac0c17e749', 'text': 'Neuro Prune: An Adaptive Approach for Efficient\\n   Deep Neural Network Optimization on Edge\\n   Devices\\n   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n   1Professor, Department of Computer Science and Engineering, PSG College of Technology\\n   grk.cse@psgtech.ac.in\\n   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\n   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\\n   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com', 'doc_id': 'mydoc'}, 'r': ({'id': 'b85404dfb4f434847a08e5ac0c17e749', 'text': 'Neuro Prune: An Adaptive Approach for Efficient\\n   Deep Neural Network Optimization on Edge\\n   Devices\\n   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n   1Professor, Department of Computer Science and Engineering, PSG College of Technology\\n   grk.cse@psgtech.ac.in\\n   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\n   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\\n   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com', 'doc_id': 'mydoc'}, 'MENTIONS', {'id': 'YOLOv4’s efficiency and edge-device performance', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}), 'm': {'id': 'YOLOv4’s efficiency and edge-device performance', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}}\n",
      "{'n': {'id': 'b85404dfb4f434847a08e5ac0c17e749', 'text': 'Neuro Prune: An Adaptive Approach for Efficient\\n   Deep Neural Network Optimization on Edge\\n   Devices\\n   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n   1Professor, Department of Computer Science and Engineering, PSG College of Technology\\n   grk.cse@psgtech.ac.in\\n   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\n   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\\n   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com', 'doc_id': 'mydoc'}, 'r': ({'id': 'b85404dfb4f434847a08e5ac0c17e749', 'text': 'Neuro Prune: An Adaptive Approach for Efficient\\n   Deep Neural Network Optimization on Edge\\n   Devices\\n   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n   1Professor, Department of Computer Science and Engineering, PSG College of Technology\\n   grk.cse@psgtech.ac.in\\n   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\n   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\\n   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com', 'doc_id': 'mydoc'}, 'MENTIONS', {'id': 'demonstrating', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}), 'm': {'id': 'demonstrating', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}}\n",
      "{'n': {'id': 'b85404dfb4f434847a08e5ac0c17e749', 'text': 'Neuro Prune: An Adaptive Approach for Efficient\\n   Deep Neural Network Optimization on Edge\\n   Devices\\n   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n   1Professor, Department of Computer Science and Engineering, PSG College of Technology\\n   grk.cse@psgtech.ac.in\\n   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\n   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\\n   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com', 'doc_id': 'mydoc'}, 'r': ({'id': 'b85404dfb4f434847a08e5ac0c17e749', 'text': 'Neuro Prune: An Adaptive Approach for Efficient\\n   Deep Neural Network Optimization on Edge\\n   Devices\\n   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n   1Professor, Department of Computer Science and Engineering, PSG College of Technology\\n   grk.cse@psgtech.ac.in\\n   2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\n   adhishthesak@gmail.com, mohanakumarp2828@gmail.com\\n   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com', 'doc_id': 'mydoc'}, 'MENTIONS', {'id': 'sparsity training with Optimal Reduction pruning improves YOLOv4’s efficiency and edge-device performance', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}), 'm': {'id': 'sparsity training with Optimal Reduction pruning improves YOLOv4’s efficiency and edge-device performance', 'source_texts': ['demonstrating its efficiency on real-world networks in their\\n   study[13].\\n   Kyoungtaek Choi et al.[2] show that Quantization-Aware\\n   Training outperforms Post-Training Quantization, and sparsity\\n   training with Optimal Reduction pruning improves YOLOv4’s\\n   efficiency and edge-device performance, despite potential\\n   high-pruning rate drawbacks.\\n   Viviana Crescitelli et al.[3] propose a filter-level pruning\\n   method for DNNs. This approach optimizes inference speed'], 'source_doc_id': 'mydoc'}}\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Neo4j credentials\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"iambatman\"\n",
    "# Create a class to interact with Neo4j\n",
    "class Neo4jQueryTool:\n",
    "    def __init__(self, uri, username, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def run_query(self, query):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(query)\n",
    "            return [record.data() for record in result]\n",
    "\n",
    "# Create an instance of the query tool\n",
    "tool = Neo4jQueryTool(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "\n",
    "# Test the tool with a sample query\n",
    "try:\n",
    "    print(\"Testing query tool...\")\n",
    "    \n",
    "    # Replace with your test query\n",
    "    sample_query = \"\"\"\n",
    "        MATCH (n)-[r]->(m)\n",
    "        RETURN n, r, m\n",
    "        LIMIT 10\n",
    "    \"\"\"\n",
    "    \n",
    "    results = tool.run_query(sample_query)\n",
    "    for record in results:\n",
    "        print(record)\n",
    "finally:\n",
    "    tool.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2522ed43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: name)} {position: line: 3, column: 9, offset: 25} for query: 'cypher\\nMATCH (n)\\nWHERE n.name = \"surya\"\\nRETURN n\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (n)\n",
      "WHERE n.name = \"surya\"\n",
      "RETURN n\n",
      "\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I don't know the answer.\n"
     ]
    }
   ],
   "source": [
    "from langchain_neo4j import GraphCypherQAChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Initialize the Gemini model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "\n",
    "# Assuming 'graph' is your Neo4jGraph instance\n",
    "qa_chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True  # Explicitly allow dangerous requests\n",
    ")\n",
    "\n",
    "# Now you can run your query\n",
    "\n",
    "response = qa_chain.run(\"who is surya\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f079e608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding model...\n",
      "Embedding model initialized.\n",
      "Preparing documents and creating FAISS vector store...\n",
      "FAISS vector store created successfully.\n",
      "Creating RAG chain...\n",
      "RAG chain created.\n"
     ]
    }
   ],
   "source": [
    "# rag_faiss_system.py\n",
    "\n",
    "# --- Dependencies ---\n",
    "# Make sure you have installed the necessary packages:\n",
    "# pip install langchain faiss-cpu sentence-transformers langchain-google-genai python-dotenv\n",
    "# (or faiss-gpu if you have CUDA installed and want GPU acceleration)\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS # Use langchain_community\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings # Use langchain_community\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # Import Gemini\n",
    "from langchain.docstore.document import Document # For structured documents\n",
    "\n",
    "# --- Configuration ---\n",
    "# Load environment variables (especially GOOGLE_API_KEY)\n",
    "load_dotenv() \n",
    "\n",
    "# Check if the API key is available\n",
    "if os.getenv(\"GOOGLE_API_KEY\") is None:\n",
    "    print(\"Error: GOOGLE_API_KEY environment variable not set.\")\n",
    "    print(\"Please get an API key from Google AI Studio (https://aistudio.google.com/app/apikey)\")\n",
    "    print(\"and set it as an environment variable (e.g., in a .env file).\")\n",
    "    exit() # Exit if the key is missing\n",
    "\n",
    "# --- Main Function ---\n",
    "# Step 1: Initialize the embedding model\n",
    "print(\"Initializing embedding model...\")\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'} # Specify device (cpu or cuda)\n",
    "    # Set encode_kwargs={'normalize_embeddings': True} if using cosine similarity,\n",
    "    # but FAISS IndexFlatL2 uses L2 distance (Euclidean), so normalization is optional \n",
    "    # but often helpful. Let's keep it simple for L2.\n",
    ")\n",
    "print(\"Embedding model initialized.\")\n",
    "\n",
    "# Step 2 & 3: Prepare documents and create FAISS index\n",
    "print(\"Preparing documents and creating FAISS vector store...\")\n",
    "documents_data = [\n",
    "    {\"content\": \"\"\"\n",
    "        Neuro Prune: An Adaptive Approach for Efficient\n",
    "Deep Neural Network Optimization on Edge\n",
    "Devices\n",
    "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
    "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
    "grk.cse@psgtech.ac.in\n",
    "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
    "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
    "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
    "Abstract—The optimization of deep neural networks for de-\n",
    "ployment on edge devices is a significant research area due to\n",
    "the demand for applications such as augmented reality, smart\n",
    "cameras, and autonomous navigation. However, deploying large\n",
    "deep learning models on edge devices poses challenges related to\n",
    "computational power, energy consumption, and latency. Pruning\n",
    "is a method to reduce the model size, accelerate inference, and\n",
    "save power. The objective of the paper is to propose the Neuro\n",
    "Prune algorithm and to apply it for the optimization of deep\n",
    "neural networks on edge devices. Efforts have been made to\n",
    "compare pruned and unpruned models. As a result, the pruned\n",
    "model has an accuracy increase of 0.22%.\n",
    "I. INTRODUCTION\n",
    "The rise of intelligent systems, ranging from autonomous\n",
    "drones to augmented reality (AR) devices and smart surveil-\n",
    "lance cameras, has intensified the need for efficient deep neural\n",
    "network deployment on edge devices with constrained com-\n",
    "putational resources. While advanced deep learning models\n",
    "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
    "their deployment on such devices is hindered by significant\n",
    "computational demands, large memory footprints, and high\n",
    "energy consumption. These challenges pose critical barriers to\n",
    "real-time applications and sustainable deployment.\n",
    "This paper builds on existing pruning techniques, includ-\n",
    "ing unstructured pruning, structured pruning, and approaches\n",
    "inspired by quantization, highlighting their strengths and lim-\n",
    "itations. In response to the challenges identified, we introduce\n",
    "Neuro Prune, a novel pruning approach that integrates princi-\n",
    "ples from reinforcement learning and the Lottery Ticket Hy-\n",
    "pothesis to achieve an optimal balance between accuracy, com-\n",
    "putational efficiency, and energy consumption. By integrating\n",
    "these principles, Neuro Prune provides a robust framework\n",
    "for optimizing deep neural networks for deployment on edge\n",
    "devices without sacrificing performance.\n",
    "Neuro Prune reduces model size and computational re-\n",
    "quirements while maintaining or even improving the model’s\n",
    "accuracy. These findings underscore the potential of Neuro\n",
    "Prune to enable real-time, energy-efficient deep learning on\n",
    "resource-constrained devices.\n",
    "Neuro Prune employs a systematic process to optimize\n",
    "neural networks for edge deployment:\n",
    "• Mask Initialization: Each layer’s weights are paired with\n",
    "a binary mask (original_mask), initialized to ones.\n",
    "• Activation Tracking: During forward passes over the\n",
    "dataset, the magnitudes of weight activations are mon-\n",
    "itored and recorded.\n",
    "• Normalize and Prune: The recorded activations are\n",
    "normalized, and a threshold based on the pruning fraction\n",
    "is computed. Using this threshold, the masks are dynam-\n",
    "ically updated to prune unimportant weights.\n",
    "• Evaluate and Log Metrics: After pruning, metrics such\n",
    "as model sparsity and accuracy are computed. A reward\n",
    "metric is derived to balance accuracy and sparsity using\n",
    "a tunable parameter, λweight.\n",
    "• Retrain the Model: The pruned model undergoes retrain-\n",
    "ing to improve the loss in accuracy while maintaining low\n",
    "computational requirements\n",
    "Neuro Prune’s unique combination of reinforcement learn-\n",
    "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
    "pothesis for identifying optimal subnetworks enables it to\n",
    "achieve remarkable efficiency. This approach ensures substan-\n",
    "tial reduction in model size and energy consumption while\n",
    "preserving accuracy and inference speed, making it an ideal\n",
    "solution for edge-based deep learning applications.\n",
    "Through extensive empirical evaluations and theoretical\n",
    "insights, this paper demonstrates how Neuro Pruning outper-\n",
    "forms traditional pruning methods, setting a new benchmark\n",
    "for sustainable, high-performance AI deployment on edge\n",
    "devices. By situating our work within the broader research\n",
    "context, we aim to offer a critical contribution to the ongoing\n",
    "evolution of neural network optimization techniques.\n",
    "II. BACKGROUND\n",
    "A. Need for Pruning Techniques\n",
    "Pruning is a crucial optimization technique for deploying\n",
    "deep neural networks in resource-constrained environments\n",
    "like healthcare. For example, wearable devices like smart-\n",
    "watches that monitor heart rates or portable EEG systems\n",
    "for brain activity analysis require lightweight models due to\n",
    "their limited computational power and memory. Pruning helps\n",
    "address these constraints by reducing the memory footprint\n",
    "and computational load, enabling efficient model deployment.\n",
    "Additionally, it minimizes energy consumption, which is vital\n",
    "for sustainable and real-time healthcare applications. Faster\n",
    "inference enabled by pruning is particularly beneficial for\n",
    "time-sensitive tasks like seizure detection or emergency di-\n",
    "agnostics. Moreover, pruning enhances model interpretability\n",
    "by retaining only the most critical components, which is\n",
    "essential for gaining trust and ensuring regulatory compliance\n",
    "in healthcare. By reducing the hardware and operational costs,\n",
    "pruning also makes deploying advanced AI solutions on edge\n",
    "devices more cost-effective and accessible.\n",
    "B. Types of Pruning Techniques\n",
    "Pruning methods are broadly classified based on granu-\n",
    "larity, timing, and approach. Granularity-based pruning in-\n",
    "cludes structured pruning, which removes entire components\n",
    "such as neurons or filters to streamline computations, and\n",
    "unstructured pruning, which eliminates individual weights\n",
    "for finer optimization, though it often requires specialized\n",
    "hardware. Timing-based pruning involves pre-training pruning,\n",
    "which optimizes models before training; post-training prun-\n",
    "ing, where redundant components are removed from trained\n",
    "models with possible retraining; and dynamic pruning during\n",
    "training, which adjusts models in real-time for improved\n",
    "adaptability. Metric-based pruning uses parameters like weight\n",
    "magnitude or gradient contribution to identify and remove\n",
    "less critical components, while application-specific pruning\n",
    "tailors strategies to specific tasks, such as optimizing for EEG\n",
    "signal analysis, or hardware constraints like GPUs. Together,\n",
    "these methods enable the development of efficient, scalable,\n",
    "and task-specific models, particularly suited for edge device\n",
    "deployment in healthcare and other domains.\n",
    "III. RELATED WORK\n",
    "Jielei Wang et al.[12] introduced an absorption pruning\n",
    "method for object detection in remote sensing imagery. This\n",
    "study achieves efficient compression with minimal accuracy\n",
    "loss but requires careful layer-wise pruning ratio tuning.\n",
    "Jan Muller et al., in their research[12], propose a neural\n",
    "network pruning method for multi-object tracking (MOT) that\n",
    "reduces model size by up to 70\n",
    "Liang Li et al., in their work[9], introduce a novel pruning\n",
    "method for DNNs that utilizes a self-adaptive mechanism\n",
    "based on weight sparsity ratios and a protective reconstruction\n",
    "mechanism. Their approach improves both model compres-\n",
    "sion and accuracy, outperforming state-of-the-art methods on\n",
    "CIFAR-10 and ImageNet datasets.\n",
    "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
    "learning with edge devices like Google Coral AI and Nvidia\n",
    "Jetson Nano. Their work revolutionizes computer vision and\n",
    "real-time tracking but raises energy concerns due to the\n",
    "computational demands of advanced algorithms.\n",
    "Satoru Koda et al.[7] explore how weight pruning in\n",
    "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
    "larly with Mahalanobis-based approaches, by improving global\n",
    "feature extraction and leveraging weights not critical for\n",
    "classification.\n",
    "Shvetha S Kumar et al. analyze and compare three pruning\n",
    "techniques—L1-norm filter pruning, channel pruning, and\n",
    "weight pruning—on CNNs for accuracy and inference time,\n",
    "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
    "pared to GTX1080 Ti in their study[8].\n",
    "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
    "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
    "on stochastic networks, extending it to related problems and\n",
    "demonstrating its efficiency on real-world networks in their\n",
    "study[13].\n",
    "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
    "Training outperforms Post-Training Quantization, and sparsity\n",
    "training with Optimal Reduction pruning improves YOLOv4’s\n",
    "efficiency and edge-device performance, despite potential\n",
    "high-pruning rate drawbacks.\n",
    "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
    "method for DNNs. This approach optimizes inference speed\n",
    "and resource efficiency on edge devices with minimal accuracy\n",
    "loss but requires careful tuning of layer-specific pruning ratios.\n",
    "Yongqi An et al.[1] introduce a retraining-free structured\n",
    "pruning framework for Large Language Models (LLMs). Their\n",
    "method incorporates a fluctuation-based pruning metric, adap-\n",
    "tive structure search, and a bias compensation mechanism to\n",
    "achieve efficient pruning. FLAP significantly enhances infer-\n",
    "ence speed and reduces model size without requiring retrain-\n",
    "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
    "and Wanda-sp on LLaMA models across various benchmarks.\n",
    "From referring to these research papers, we derived an\n",
    "idea for pruning on edge devices. Additionally, our proposed\n",
    "solution is inspired by the research of Jaron Maene et al.[10],\n",
    "which suggests that sparse subnetworks in dense networks can\n",
    "achieve similar accuracy when retrained. This study demon-\n",
    "strates stable training with linear mode connectivity, support-\n",
    "ing the idea that lottery tickets retrain to similar regions, but\n",
    "questions their independence from dense training and iterative\n",
    "pruning.\n",
    "Eran Malach et al.[11] further strengthen the lottery ticket\n",
    "hypothesis by proving that over-parameterized neural networks\n",
    "with random weights always contain a subnetwork matching\n",
    "the accuracy of a target network, without additional training.\n",
    "IV. NEUROLOGICAL PRUNING APPROACH\n",
    "The Neurological Pruning Algorithm is designed to opti-\n",
    "mize neural networks by iteratively pruning less important\n",
    "weights while maintaining high performance. Unlike tradi-\n",
    "tional methods, this approach incorporates adaptive, layer-\n",
    "specific pruning thresholds that dynamically adjust based\n",
    "on weight activations, ensuring efficient model optimization\n",
    "across various stages of training. Drawing inspiration from\n",
    "the Lottery Ticket Hypothesis, which suggests that sparse\n",
    "subnetworks can perform similarly to their dense counterparts\n",
    "when retrained, the algorithm selectively deactivates weights\n",
    "using binary masks to enhance computational efficiency with-\n",
    "out sacrificing accuracy.\n",
    "The algorithm operates in several epochs, during which\n",
    "weight activations are evaluated. Based on a quantile-based\n",
    "strategy, pruning thresholds are established for each layer, en-\n",
    "suring that only weights contributing minimally to the model’s\n",
    "output are removed. After pruning, the model undergoes fine-\n",
    "tuning to restore any lost accuracy and further optimize the\n",
    "network. This step enhances the adaptability and scalability\n",
    "of the pruning method, making it suitable for deployment in\n",
    "resource-constrained environments\n",
    "A. Key Characteristics and Advantages\n",
    "The Neurological Pruning Algorithm boasts several key\n",
    "characteristics that distinguish it from conventional pruning\n",
    "techniques. One notable feature is its layer-specific pruning,\n",
    "which evaluates the importance of weights individually for\n",
    "each layer, leading to a more nuanced and efficient sparsifi-\n",
    "cation process. This approach prevents the model from losing\n",
    "essential features that may otherwise be pruned by a global\n",
    "approach.\n",
    "The algorithm’s use of quantile-based thresholding en-\n",
    "sures that only weights with the least contribution to the\n",
    "model’s output are pruned. This fine-tuned method avoids\n",
    "performance degradation by carefully targeting weights that\n",
    "are less significant to the network’s overall functionality.\n",
    "Furthermore, the pruning process is iterative, allowing for\n",
    "continuous model evaluation and real-time adjustments based\n",
    "on performance feedback, ensuring that accuracy is retained\n",
    "even as sparsity increases.\n",
    "Additionally, the algorithm maintains a careful balance\n",
    "between sparsity and accuracy, enabling a reduction in the\n",
    "model’s computational complexity while preserving its predic-\n",
    "tive capabilities. This characteristic is particularly important\n",
    "for applications requiring rapid inference times, such as in\n",
    "real-time healthcare systems and edge computing scenarios,\n",
    "where both energy efficiency and computational power are\n",
    "crucial.\n",
    "B. Impact on Model Performance\n",
    "The Neurological Pruning Algorithm significantly improves\n",
    "the efficiency of neural networks, making them more suitable\n",
    "for deployment on edge devices. Key performance metrics\n",
    "such as compression ratio (CR) and accuracy retention\n",
    "(AR) are critical indicators of the algorithm’s effectiveness.\n",
    "For instance, in evaluating a ResNet-18 model, the algorithm\n",
    "achieved a CR of 1, indicating that while the number of\n",
    "parameters remained the same, the model’s efficiency im-\n",
    "proved through selective pruning, reducing computational load\n",
    "without sacrificing performance.\n",
    "Moreover, the algorithm demonstrates the ability to achieve\n",
    "high accuracy retention, with models maintaining or even\n",
    "slightly improving their predictive performance post-pruning.\n",
    "This outcome highlights the algorithm’s capability to effi-\n",
    "ciently remove redundant weights while preserving the core\n",
    "functionality of the network. The result is a model that is\n",
    "not only more computationally efficient but also adaptable\n",
    "across different architectures and datasets, making it suit-\n",
    "able for a wide range of applications, from image classification\n",
    "to natural language processing .\n",
    "In practical terms, the Neurological Pruning Algorithm en-\n",
    "ables faster inference times and lower energy consumption,\n",
    "two crucial aspects for deploying AI models in resource-\n",
    "constrained environments. As such, it holds significant poten-\n",
    "tial for improving the viability of real-time, energy-efficient\n",
    "AI applications, particularly in fields like healthcare, where\n",
    "both speed and sustainability are essential.\n",
    "V. NEUROLOGICAL PRUNING ALGORITHM\n",
    "The Neurological Pruning Algorithm is a structured and\n",
    "adaptive methodology designed to optimize neural networks\n",
    "for resource-constrained environments. This algorithm goes\n",
    "beyond traditional pruning approaches by incorporating layer-\n",
    "specific pruning thresholds that dynamically adjust to the\n",
    "importance of weight activations within each layer. Drawing\n",
    "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
    "inforcement learning, the algorithm strikes an optimal balance\n",
    "between model sparsity and accuracy .\n",
    "A. Algorithm Description\n",
    "The Neurological Pruning Algorithm iteratively refines the\n",
    "neural network through a series of training epochs. During\n",
    "each epoch, weight activations are tracked, normalized, and\n",
    "analyzed to identify low-importance weights. These weights\n",
    "are selectively deactivated using binary masks, ensuring com-\n",
    "putational efficiency without compromising the network’s pre-\n",
    "dictive performance. The algorithm further integrates a fine-\n",
    "tuning phase, enabling the pruned network to recover and\n",
    "enhance its accuracy. The detailed process is presented in\n",
    "Algorithm 1.\n",
    "By dynamically adjusting to layer-specific characteristics\n",
    "and maintaining performance metrics, the Neurological Prun-\n",
    "ing Algorithm ensures a significant reduction in computational\n",
    "and memory requirements, paving the way for efficient deploy-\n",
    "ment of deep learning models on edge devices .\n",
    "B. Input Parameters and Output\n",
    "Inputs:\n",
    "• Pretrained model (M)\n",
    "• Dataset (D)\n",
    "• Prune fraction (p)\n",
    "• Total epochs (E)\n",
    "Output:\n",
    "• Pruned and fine-tuned model (Mpruned)\n",
    "C. Key Features and Advantages\n",
    "The Neurological Pruning Algorithm is designed with sev-\n",
    "eral distinctive features that contribute to its effectiveness\n",
    "in optimizing neural network performance. These features\n",
    "include:\n",
    "Algorithm 1 Simplified Neurological Pruning Algorithm\n",
    "Require: Pretrained model M, Dataset D, Prune fraction p,\n",
    "Total epochs E\n",
    "Ensure: Pruned and fine-tuned model Mpruned\n",
    "1: Initialize binary masks M with all ones\n",
    "2: for each epoch e = 1 to E do\n",
    "3:\n",
    "Compute weight activations A during forward passes\n",
    "4:\n",
    "for each layer i in M do\n",
    "5:\n",
    "Compute pruning threshold Ti as p-quantile of Ai\n",
    "6:\n",
    "Update mask Mi: retain weights with Ai > Ti\n",
    "7:\n",
    "Apply mask to layer: θi ←θi · Mi\n",
    "8:\n",
    "end for\n",
    "9:\n",
    "Evaluate model performance and update metrics\n",
    "10: end for\n",
    "11: Fine-tune Mpruned on D\n",
    "12: return Mpruned\n",
    "• Layer-specific Pruning: The algorithm applies pruning\n",
    "on a per-layer basis, evaluating the weight activations\n",
    "within each layer individually. This approach ensures that\n",
    "the sparsity introduced by pruning is well-distributed and\n",
    "context-dependent, facilitating more nuanced control over\n",
    "model optimization .\n",
    "• Quantile-based Thresholding: A quantile-based strat-\n",
    "egy is employed to determine the pruning threshold for\n",
    "each layer. This method ensures that only weights with\n",
    "minimal contribution to the network’s output are pruned,\n",
    "thereby reducing the risk of adversely affecting model\n",
    "accuracy while enhancing sparsity .\n",
    "• Iterative Pruning and Evaluation: The pruning pro-\n",
    "cess follows an iterative approach, enabling continuous\n",
    "assessment of the model’s performance after each pruning\n",
    "step. This ensures dynamic adjustments to maintain a bal-\n",
    "ance between model sparsity and predictive performance\n",
    "throughout training.\n",
    "• Preservation of Model Accuracy: By adjusting pruning\n",
    "thresholds in response to ongoing performance metrics,\n",
    "the algorithm ensures that model accuracy is not com-\n",
    "promised. The careful tuning of these thresholds allows\n",
    "for significant sparsity while maintaining the robustness\n",
    "of the network’s predictive capabilities .\n",
    "D. Impact on Neural Network Performance\n",
    "The Neurological Pruning Algorithm has been extensively\n",
    "evaluated across various benchmark datasets and model archi-\n",
    "tectures, demonstrating its capacity to achieve the following\n",
    "outcomes:\n",
    "• High Sparsity with Minimal Accuracy Loss: The algo-\n",
    "rithm effectively prunes unnecessary weights, achieving\n",
    "high sparsity levels without causing significant degrada-\n",
    "tion in model accuracy. This outcome is critical for the\n",
    "efficient deployment of models in resource-constrained\n",
    "environments.\n",
    "• Reduced Computational Complexity: By sparsifying\n",
    "the weight matrices, the algorithm reduces the compu-\n",
    "tational load during inference, which translates to faster\n",
    "model execution and lower resource consumption. This\n",
    "characteristic is especially beneficial in edge computing\n",
    "and real-time applications .\n",
    "• Broad Applicability: The algorithm is highly flexible\n",
    "and can be adapted to various neural network architec-\n",
    "tures and datasets, making it suitable for a wide range\n",
    "of tasks, from image recognition to natural language\n",
    "processing .\n",
    "VI. EVALUATION METRICS\n",
    "A. Performance Comparison\n",
    "Fig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\n",
    "TABLE I\n",
    "COMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\n",
    "Metric\n",
    "Unpruned\n",
    "Model\n",
    "Pruned\n",
    "Model\n",
    "Accuracy\n",
    "0.9562\n",
    "0.9583\n",
    "Train Loss\n",
    "0.1414\n",
    "0.1625\n",
    "Inference\n",
    "Time\n",
    "(s)\n",
    "0.0318\n",
    "0.0314\n",
    "No of Parame-\n",
    "ters(million)\n",
    "11.7\n",
    "11.7\n",
    "B. Evaluation Metrics Calculation\n",
    "The key metrics used to evaluate the proposed pruning\n",
    "technique are as follows:\n",
    "Compression Ratio (CR): The Compression Ratio (CR) is\n",
    "calculated as:\n",
    "CR = Total Parameters After Pruning\n",
    "Total Parameters Before Pruning\n",
    "(1)\n",
    "CR = 11.7M\n",
    "11.7M = 1\n",
    "Accuracy Retention (AR): The Accuracy Retention (AR) is\n",
    "calculated as:\n",
    "AR =\n",
    "\u0012Accuracy After Pruning −Baseline Accuracy\n",
    "Baseline Accuracy\n",
    "\u0013\n",
    "×100\n",
    "(2)\n",
    "AR =\n",
    "\u00120.958284 −0.956206\n",
    "0.956206\n",
    "\u0013\n",
    "× 100 = 0.22%\n",
    "This result indicates that the pruned model achieves an ac-\n",
    "curacy improvement of 0.22% when compared to unpruned\n",
    "model.\n",
    "C. Illustration of Results\n",
    "Fig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\n",
    "18.\n",
    "D. Discussion\n",
    "The evaluation metrics highlight the effectiveness of the\n",
    "proposed Neurological Pruning method on the ResNet-18\n",
    "model. The Compression Ratio (CR) remains 1, indicating\n",
    "no reduction in the total number of parameters, which suggests\n",
    "that this approach optimizes the neural network by selectively\n",
    "pruning connections rather than reducing the overall parameter\n",
    "count. The Accuracy Retention (AR), which is approximately\n",
    "0.22%, demonstrates that the Neurological Pruning technique\n",
    "successfully improved the accuracy compared to the baseline\n",
    "model.\n",
    "These results validate the potential of Neurological Pruning\n",
    "as an innovative approach to optimizing deep learning models.\n",
    "By maintaining accuracy while likely enhancing sparsity and\n",
    "computational efficiency, this method presents a promising di-\n",
    "rection for achieving high-performance models under resource\n",
    "constraints.\n",
    "VII. CONCLUSION\n",
    "In summary, the Neurological Pruning Algorithm provides\n",
    "a systematic and principled approach to optimizing neural\n",
    "networks. By balancing the trade-off between sparsity and\n",
    "accuracy, it enables significant reductions in computational\n",
    "complexity without sacrificing the network’s predictive per-\n",
    "formance. Future work will focus on extending the algorithm\n",
    "to dynamic network architectures and incorporating advanced\n",
    "regularization techniques to further enhance its robustness and\n",
    "adaptability in diverse applications.\n",
    "REFERENCES\n",
    "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\n",
    "Fluctuation-based Adaptive Structured Pruning for Large Language\n",
    "Models.\n",
    "[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\n",
    "Simplification of Deep Neural Network-Based Object Detector for Real-\n",
    "Time Edge Computing.\n",
    "[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\n",
    "Devices Object Detection by Filter Pruning.\n",
    "[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\n",
    "and Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\n",
    "Tracking Algorithms in Edge Devices.\n",
    "[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\n",
    "Chen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\n",
    "Compression Method for Object Detection Network for Edge Devices.\n",
    "[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\n",
    "Cao. Absorption Pruning of Deep Neural Network for Object Detection\n",
    "in Remote Sensing Imagery.\n",
    "[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\n",
    "and Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\n",
    "Distribution Detection: An Empirical Survey.\n",
    "[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\n",
    "Sahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\n",
    "[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\n",
    "Compress DNNs.\n",
    "[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\n",
    "derstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\n",
    "[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\n",
    "Proving the Lottery Ticket Hypothesis: Pruning is All You Need.\n",
    "[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\n",
    "Devices via Reconstruction-Based Channel Pruning.\n",
    "[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\n",
    "Determine Reliable Paths on Networks with Random and Correlated\n",
    "Link Travel Times.\n",
    "[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\n",
    "and D. Suganthi. Real-Time Object Detection on Edge Devices Using\n",
    "Mobile Neural Networks.\n",
    "[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\n",
    "Tang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\n",
    "Models.\n",
    "[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\n",
    "Li, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\n",
    "Deng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\n",
    "Detection of Tea Leaves with One Bud and Two Leaves Based on\n",
    "ShuffleNetv2-YOLOv5-Lite-E.\n",
    "Neuro Prune: An Adaptive Approach for Efficient\n",
    "Deep Neural Network Optimization on Edge\n",
    "Devices\n",
    "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
    "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
    "grk.cse@psgtech.ac.in\n",
    "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
    "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
    "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
    "Abstract—The optimization of deep neural networks for de-\n",
    "ployment on edge devices is a significant research area due to\n",
    "the demand for applications such as augmented reality, smart\n",
    "cameras, and autonomous navigation. However, deploying large\n",
    "deep learning models on edge devices poses challenges related to\n",
    "computational power, energy consumption, and latency. Pruning\n",
    "is a method to reduce the model size, accelerate inference, and\n",
    "save power. The objective of the paper is to propose the Neuro\n",
    "Prune algorithm and to apply it for the optimization of deep\n",
    "neural networks on edge devices. Efforts have been made to\n",
    "compare pruned and unpruned models. As a result, the pruned\n",
    "model has an accuracy increase of 0.22%.\n",
    "I. INTRODUCTION\n",
    "The rise of intelligent systems, ranging from autonomous\n",
    "drones to augmented reality (AR) devices and smart surveil-\n",
    "lance cameras, has intensified the need for efficient deep neural\n",
    "network deployment on edge devices with constrained com-\n",
    "putational resources. While advanced deep learning models\n",
    "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
    "their deployment on such devices is hindered by significant\n",
    "computational demands, large memory footprints, and high\n",
    "energy consumption. These challenges pose critical barriers to\n",
    "real-time applications and sustainable deployment.\n",
    "This paper builds on existing pruning techniques, includ-\n",
    "ing unstructured pruning, structured pruning, and approaches\n",
    "inspired by quantization, highlighting their strengths and lim-\n",
    "itations. In response to the challenges identified, we introduce\n",
    "Neuro Prune, a novel pruning approach that integrates princi-\n",
    "ples from reinforcement learning and the Lottery Ticket Hy-\n",
    "pothesis to achieve an optimal balance between accuracy, com-\n",
    "putational efficiency, and energy consumption. By integrating\n",
    "these principles, Neuro Prune provides a robust framework\n",
    "for optimizing deep neural networks for deployment on edge\n",
    "devices without sacrificing performance.\n",
    "Neuro Prune reduces model size and computational re-\n",
    "quirements while maintaining or even improving the model’s\n",
    "accuracy. These findings underscore the potential of Neuro\n",
    "Prune to enable real-time, energy-efficient deep learning on\n",
    "resource-constrained devices.\n",
    "Neuro Prune employs a systematic process to optimize\n",
    "neural networks for edge deployment:\n",
    "• Mask Initialization: Each layer’s weights are paired with\n",
    "a binary mask (original_mask), initialized to ones.\n",
    "• Activation Tracking: During forward passes over the\n",
    "dataset, the magnitudes of weight activations are mon-\n",
    "itored and recorded.\n",
    "• Normalize and Prune: The recorded activations are\n",
    "normalized, and a threshold based on the pruning fraction\n",
    "is computed. Using this threshold, the masks are dynam-\n",
    "ically updated to prune unimportant weights.\n",
    "• Evaluate and Log Metrics: After pruning, metrics such\n",
    "as model sparsity and accuracy are computed. A reward\n",
    "metric is derived to balance accuracy and sparsity using\n",
    "a tunable parameter, λweight.\n",
    "• Retrain the Model: The pruned model undergoes retrain-\n",
    "ing to improve the loss in accuracy while maintaining low\n",
    "computational requirements\n",
    "Neuro Prune’s unique combination of reinforcement learn-\n",
    "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
    "pothesis for identifying optimal subnetworks enables it to\n",
    "achieve remarkable efficiency. This approach ensures substan-\n",
    "tial reduction in model size and energy consumption while\n",
    "preserving accuracy and inference speed, making it an ideal\n",
    "solution for edge-based deep learning applications.\n",
    "Through extensive empirical evaluations and theoretical\n",
    "insights, this paper demonstrates how Neuro Pruning outper-\n",
    "forms traditional pruning methods, setting a new benchmark\n",
    "for sustainable, high-performance AI deployment on edge\n",
    "devices. By situating our work within the broader research\n",
    "context, we aim to offer a critical contribution to the ongoing\n",
    "evolution of neural network optimization techniques.\n",
    "II. BACKGROUND\n",
    "A. Need for Pruning Techniques\n",
    "Pruning is a crucial optimization technique for deploying\n",
    "deep neural networks in resource-constrained environments\n",
    "like healthcare. For example, wearable devices like smart-\n",
    "watches that monitor heart rates or portable EEG systems\n",
    "for brain activity analysis require lightweight models due to\n",
    "their limited computational power and memory. Pruning helps\n",
    "address these constraints by reducing the memory footprint\n",
    "and computational load, enabling efficient model deployment.\n",
    "Additionally, it minimizes energy consumption, which is vital\n",
    "for sustainable and real-time healthcare applications. Faster\n",
    "inference enabled by pruning is particularly beneficial for\n",
    "time-sensitive tasks like seizure detection or emergency di-\n",
    "agnostics. Moreover, pruning enhances model interpretability\n",
    "by retaining only the most critical components, which is\n",
    "essential for gaining trust and ensuring regulatory compliance\n",
    "in healthcare. By reducing the hardware and operational costs,\n",
    "pruning also makes deploying advanced AI solutions on edge\n",
    "devices more cost-effective and accessible.\n",
    "B. Types of Pruning Techniques\n",
    "Pruning methods are broadly classified based on granu-\n",
    "larity, timing, and approach. Granularity-based pruning in-\n",
    "cludes structured pruning, which removes entire components\n",
    "such as neurons or filters to streamline computations, and\n",
    "unstructured pruning, which eliminates individual weights\n",
    "for finer optimization, though it often requires specialized\n",
    "hardware. Timing-based pruning involves pre-training pruning,\n",
    "which optimizes models before training; post-training prun-\n",
    "ing, where redundant components are removed from trained\n",
    "models with possible retraining; and dynamic pruning during\n",
    "training, which adjusts models in real-time for improved\n",
    "adaptability. Metric-based pruning uses parameters like weight\n",
    "magnitude or gradient contribution to identify and remove\n",
    "less critical components, while application-specific pruning\n",
    "tailors strategies to specific tasks, such as optimizing for EEG\n",
    "signal analysis, or hardware constraints like GPUs. Together,\n",
    "these methods enable the development of efficient, scalable,\n",
    "and task-specific models, particularly suited for edge device\n",
    "deployment in healthcare and other domains.\n",
    "III. RELATED WORK\n",
    "Jielei Wang et al.[12] introduced an absorption pruning\n",
    "method for object detection in remote sensing imagery. This\n",
    "study achieves efficient compression with minimal accuracy\n",
    "loss but requires careful layer-wise pruning ratio tuning.\n",
    "Jan Muller et al., in their research[12], propose a neural\n",
    "network pruning method for multi-object tracking (MOT) that\n",
    "reduces model size by up to 70\n",
    "Liang Li et al., in their work[9], introduce a novel pruning\n",
    "method for DNNs that utilizes a self-adaptive mechanism\n",
    "based on weight sparsity ratios and a protective reconstruction\n",
    "mechanism. Their approach improves both model compres-\n",
    "sion and accuracy, outperforming state-of-the-art methods on\n",
    "CIFAR-10 and ImageNet datasets.\n",
    "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
    "learning with edge devices like Google Coral AI and Nvidia\n",
    "Jetson Nano. Their work revolutionizes computer vision and\n",
    "real-time tracking but raises energy concerns due to the\n",
    "computational demands of advanced algorithms.\n",
    "Satoru Koda et al.[7] explore how weight pruning in\n",
    "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
    "larly with Mahalanobis-based approaches, by improving global\n",
    "feature extraction and leveraging weights not critical for\n",
    "classification.\n",
    "Shvetha S Kumar et al. analyze and compare three pruning\n",
    "techniques—L1-norm filter pruning, channel pruning, and\n",
    "weight pruning—on CNNs for accuracy and inference time,\n",
    "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
    "pared to GTX1080 Ti in their study[8].\n",
    "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
    "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
    "on stochastic networks, extending it to related problems and\n",
    "demonstrating its efficiency on real-world networks in their\n",
    "study[13].\n",
    "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
    "Training outperforms Post-Training Quantization, and sparsity\n",
    "training with Optimal Reduction pruning improves YOLOv4’s\n",
    "efficiency and edge-device performance, despite potential\n",
    "high-pruning rate drawbacks.\n",
    "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
    "method for DNNs. This approach optimizes inference speed\n",
    "and resource efficiency on edge devices with minimal accuracy\n",
    "loss but requires careful tuning of layer-specific pruning ratios.\n",
    "Yongqi An et al.[1] introduce a retraining-free structured\n",
    "pruning framework for Large Language Models (LLMs). Their\n",
    "method incorporates a fluctuation-based pruning metric, adap-\n",
    "tive structure search, and a bias compensation mechanism to\n",
    "achieve efficient pruning. FLAP significantly enhances infer-\n",
    "ence speed and reduces model size without requiring retrain-\n",
    "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
    "and Wanda-sp on LLaMA models across various benchmarks.\n",
    "From referring to these research papers, we derived an\n",
    "idea for pruning on edge devices. Additionally, our proposed\n",
    "solution is inspired by the research of Jaron Maene et al.[10],\n",
    "which suggests that sparse subnetworks in dense networks can\n",
    "achieve similar accuracy when retrained. This study demon-\n",
    "strates stable training with linear mode connectivity, support-\n",
    "ing the idea that lottery tickets retrain to similar regions, but\n",
    "questions their independence from dense training and iterative\n",
    "pruning.\n",
    "Eran Malach et al.[11] further strengthen the lottery ticket\n",
    "hypothesis by proving that over-parameterized neural networks\n",
    "with random weights always contain a subnetwork matching\n",
    "the accuracy of a target network, without additional training.\n",
    "IV. NEUROLOGICAL PRUNING APPROACH\n",
    "The Neurological Pruning Algorithm is designed to opti-\n",
    "mize neural networks by iteratively pruning less important\n",
    "weights while maintaining high performance. Unlike tradi-\n",
    "tional methods, this approach incorporates adaptive, layer-\n",
    "specific pruning thresholds that dynamically adjust based\n",
    "on weight activations, ensuring efficient model optimization\n",
    "across various stages of training. Drawing inspiration from\n",
    "the Lottery Ticket Hypothesis, which suggests that sparse\n",
    "subnetworks can perform similarly to their dense counterparts\n",
    "when retrained, the algorithm selectively deactivates weights\n",
    "using binary masks to enhance computational efficiency with-\n",
    "out sacrificing accuracy.\n",
    "The algorithm operates in several epochs, during which\n",
    "weight activations are evaluated. Based on a quantile-based\n",
    "strategy, pruning thresholds are established for each layer, en-\n",
    "suring that only weights contributing minimally to the model’s\n",
    "output are removed. After pruning, the model undergoes fine-\n",
    "tuning to restore any lost accuracy and further optimize the\n",
    "network. This step enhances the adaptability and scalability\n",
    "of the pruning method, making it suitable for deployment in\n",
    "resource-constrained environments\n",
    "A. Key Characteristics and Advantages\n",
    "The Neurological Pruning Algorithm boasts several key\n",
    "characteristics that distinguish it from conventional pruning\n",
    "techniques. One notable feature is its layer-specific pruning,\n",
    "which evaluates the importance of weights individually for\n",
    "each layer, leading to a more nuanced and efficient sparsifi-\n",
    "cation process. This approach prevents the model from losing\n",
    "essential features that may otherwise be pruned by a global\n",
    "approach.\n",
    "The algorithm’s use of quantile-based thresholding en-\n",
    "sures that only weights with the least contribution to the\n",
    "model’s output are pruned. This fine-tuned method avoids\n",
    "performance degradation by carefully targeting weights that\n",
    "are less significant to the network’s overall functionality.\n",
    "Furthermore, the pruning process is iterative, allowing for\n",
    "continuous model evaluation and real-time adjustments based\n",
    "on performance feedback, ensuring that accuracy is retained\n",
    "even as sparsity increases.\n",
    "Additionally, the algorithm maintains a careful balance\n",
    "between sparsity and accuracy, enabling a reduction in the\n",
    "model’s computational complexity while preserving its predic-\n",
    "tive capabilities. This characteristic is particularly important\n",
    "for applications requiring rapid inference times, such as in\n",
    "real-time healthcare systems and edge computing scenarios,\n",
    "where both energy efficiency and computational power are\n",
    "crucial.\n",
    "B. Impact on Model Performance\n",
    "The Neurological Pruning Algorithm significantly improves\n",
    "the efficiency of neural networks, making them more suitable\n",
    "for deployment on edge devices. Key performance metrics\n",
    "such as compression ratio (CR) and accuracy retention\n",
    "(AR) are critical indicators of the algorithm’s effectiveness.\n",
    "For instance, in evaluating a ResNet-18 model, the algorithm\n",
    "achieved a CR of 1, indicating that while the number of\n",
    "parameters remained the same, the model’s efficiency im-\n",
    "proved through selective pruning, reducing computational load\n",
    "without sacrificing performance.\n",
    "Moreover, the algorithm demonstrates the ability to achieve\n",
    "high accuracy retention, with models maintaining or even\n",
    "slightly improving their predictive performance post-pruning.\n",
    "This outcome highlights the algorithm’s capability to effi-\n",
    "ciently remove redundant weights while preserving the core\n",
    "functionality of the network. The result is a model that is\n",
    "not only more computationally efficient but also adaptable\n",
    "across different architectures and datasets, making it suit-\n",
    "able for a wide range of applications, from image classification\n",
    "to natural language processing .\n",
    "In practical terms, the Neurological Pruning Algorithm en-\n",
    "ables faster inference times and lower energy consumption,\n",
    "two crucial aspects for deploying AI models in resource-\n",
    "constrained environments. As such, it holds significant poten-\n",
    "tial for improving the viability of real-time, energy-efficient\n",
    "AI applications, particularly in fields like healthcare, where\n",
    "both speed and sustainability are essential.\n",
    "V. NEUROLOGICAL PRUNING ALGORITHM\n",
    "The Neurological Pruning Algorithm is a structured and\n",
    "adaptive methodology designed to optimize neural networks\n",
    "for resource-constrained environments. This algorithm goes\n",
    "beyond traditional pruning approaches by incorporating layer-\n",
    "specific pruning thresholds that dynamically adjust to the\n",
    "importance of weight activations within each layer. Drawing\n",
    "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
    "inforcement learning, the algorithm strikes an optimal balance\n",
    "between model sparsity and accuracy .\n",
    "A. Algorithm Description\n",
    "The Neurological Pruning Algorithm iteratively refines the\n",
    "neural network through a series of training epochs. During\n",
    "each epoch, weight activations are tracked, normalized, and\n",
    "analyzed to identify low-importance weights. These weights\n",
    "are selectively deactivated using binary masks, ensuring com-\n",
    "putational efficiency without compromising the network’s pre-\n",
    "dictive performance. The algorithm further integrates a fine-\n",
    "tuning phase, enabling the pruned network to recover and\n",
    "enhance its accuracy. The detailed process is presented in\n",
    "Algorithm 1.\n",
    "By dynamically adjusting to layer-specific characteristics\n",
    "and maintaining performance metrics, the Neurological Prun-\n",
    "ing Algorithm ensures a significant reduction in computational\n",
    "and memory requirements, paving the way for efficient deploy-\n",
    "ment of deep learning models on edge devices .\n",
    "B. Input Parameters and Output\n",
    "Inputs:\n",
    "• Pretrained model (M)\n",
    "• Dataset (D)\n",
    "• Prune fraction (p)\n",
    "• Total epochs (E)\n",
    "Output:\n",
    "• Pruned and fine-tuned model (Mpruned)\n",
    "C. Key Features and Advantages\n",
    "The Neurological Pruning Algorithm is designed with sev-\n",
    "eral distinctive features that contribute to its effectiveness\n",
    "in optimizing neural network performance. These features\n",
    "include:\n",
    "Algorithm 1 Simplified Neurological Pruning Algorithm\n",
    "Require: Pretrained model M, Dataset D, Prune fraction p,\n",
    "Total epochs E\n",
    "Ensure: Pruned and fine-tuned model Mpruned\n",
    "1: Initialize binary masks M with all ones\n",
    "2: for each epoch e = 1 to E do\n",
    "3:\n",
    "Compute weight activations A during forward passes\n",
    "4:\n",
    "for each layer i in M do\n",
    "5:\n",
    "Compute pruning threshold Ti as p-quantile of Ai\n",
    "6:\n",
    "Update mask Mi: retain weights with Ai > Ti\n",
    "7:\n",
    "Apply mask to layer: θi ←θi · Mi\n",
    "8:\n",
    "end for\n",
    "9:\n",
    "Evaluate model performance and update metrics\n",
    "10: end for\n",
    "11: Fine-tune Mpruned on D\n",
    "12: return Mpruned\n",
    "• Layer-specific Pruning: The algorithm applies pruning\n",
    "on a per-layer basis, evaluating the weight activations\n",
    "within each layer individually. This approach ensures that\n",
    "the sparsity introduced by pruning is well-distributed and\n",
    "context-dependent, facilitating more nuanced control over\n",
    "model optimization .\n",
    "• Quantile-based Thresholding: A quantile-based strat-\n",
    "egy is employed to determine the pruning threshold for\n",
    "each layer. This method ensures that only weights with\n",
    "minimal contribution to the network’s output are pruned,\n",
    "thereby reducing the risk of adversely affecting model\n",
    "accuracy while enhancing sparsity .\n",
    "• Iterative Pruning and Evaluation: The pruning pro-\n",
    "cess follows an iterative approach, enabling continuous\n",
    "assessment of the model’s performance after each pruning\n",
    "step. This ensures dynamic adjustments to maintain a bal-\n",
    "ance between model sparsity and predictive performance\n",
    "throughout training.\n",
    "• Preservation of Model Accuracy: By adjusting pruning\n",
    "thresholds in response to ongoing performance metrics,\n",
    "the algorithm ensures that model accuracy is not com-\n",
    "promised. The careful tuning of these thresholds allows\n",
    "for significant sparsity while maintaining the robustness\n",
    "of the network’s predictive capabilities .\n",
    "D. Impact on Neural Network Performance\n",
    "The Neurological Pruning Algorithm has been extensively\n",
    "evaluated across various benchmark datasets and model archi-\n",
    "tectures, demonstrating its capacity to achieve the following\n",
    "outcomes:\n",
    "• High Sparsity with Minimal Accuracy Loss: The algo-\n",
    "rithm effectively prunes unnecessary weights, achieving\n",
    "high sparsity levels without causing significant degrada-\n",
    "tion in model accuracy. This outcome is critical for the\n",
    "efficient deployment of models in resource-constrained\n",
    "environments.\n",
    "• Reduced Computational Complexity: By sparsifying\n",
    "the weight matrices, the algorithm reduces the compu-\n",
    "tational load during inference, which translates to faster\n",
    "model execution and lower resource consumption. This\n",
    "characteristic is especially beneficial in edge computing\n",
    "and real-time applications .\n",
    "• Broad Applicability: The algorithm is highly flexible\n",
    "and can be adapted to various neural network architec-\n",
    "tures and datasets, making it suitable for a wide range\n",
    "of tasks, from image recognition to natural language\n",
    "processing .\n",
    "VI. EVALUATION METRICS\n",
    "A. Performance Comparison\n",
    "Fig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\n",
    "TABLE I\n",
    "COMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\n",
    "Metric\n",
    "Unpruned\n",
    "Model\n",
    "Pruned\n",
    "Model\n",
    "Accuracy\n",
    "0.9562\n",
    "0.9583\n",
    "Train Loss\n",
    "0.1414\n",
    "0.1625\n",
    "Inference\n",
    "Time\n",
    "(s)\n",
    "0.0318\n",
    "0.0314\n",
    "No of Parame-\n",
    "ters(million)\n",
    "11.7\n",
    "11.7\n",
    "B. Evaluation Metrics Calculation\n",
    "The key metrics used to evaluate the proposed pruning\n",
    "technique are as follows:\n",
    "Compression Ratio (CR): The Compression Ratio (CR) is\n",
    "calculated as:\n",
    "CR = Total Parameters After Pruning\n",
    "Total Parameters Before Pruning\n",
    "(1)\n",
    "CR = 11.7M\n",
    "11.7M = 1\n",
    "Accuracy Retention (AR): The Accuracy Retention (AR) is\n",
    "calculated as:\n",
    "AR =\n",
    "\u0012Accuracy After Pruning −Baseline Accuracy\n",
    "Baseline Accuracy\n",
    "\u0013\n",
    "×100\n",
    "(2)\n",
    "AR =\n",
    "\u00120.958284 −0.956206\n",
    "0.956206\n",
    "\u0013\n",
    "× 100 = 0.22%\n",
    "This result indicates that the pruned model achieves an ac-\n",
    "curacy improvement of 0.22% when compared to unpruned\n",
    "model.\n",
    "C. Illustration of Results\n",
    "Fig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\n",
    "18.\n",
    "D. Discussion\n",
    "The evaluation metrics highlight the effectiveness of the\n",
    "proposed Neurological Pruning method on the ResNet-18\n",
    "model. The Compression Ratio (CR) remains 1, indicating\n",
    "no reduction in the total number of parameters, which suggests\n",
    "that this approach optimizes the neural network by selectively\n",
    "pruning connections rather than reducing the overall parameter\n",
    "count. The Accuracy Retention (AR), which is approximately\n",
    "0.22%, demonstrates that the Neurological Pruning technique\n",
    "successfully improved the accuracy compared to the baseline\n",
    "model.\n",
    "These results validate the potential of Neurological Pruning\n",
    "as an innovative approach to optimizing deep learning models.\n",
    "By maintaining accuracy while likely enhancing sparsity and\n",
    "computational efficiency, this method presents a promising di-\n",
    "rection for achieving high-performance models under resource\n",
    "constraints.\n",
    "VII. CONCLUSION\n",
    "In summary, the Neurological Pruning Algorithm provides\n",
    "a systematic and principled approach to optimizing neural\n",
    "networks. By balancing the trade-off between sparsity and\n",
    "accuracy, it enables significant reductions in computational\n",
    "complexity without sacrificing the network’s predictive per-\n",
    "formance. Future work will focus on extending the algorithm\n",
    "to dynamic network architectures and incorporating advanced\n",
    "regularization techniques to further enhance its robustness and\n",
    "adaptability in diverse applications.\n",
    "REFERENCES\n",
    "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\n",
    "Fluctuation-based Adaptive Structured Pruning for Large Language\n",
    "Models.\n",
    "[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\n",
    "Simplification of Deep Neural Network-Based Object Detector for Real-\n",
    "Time Edge Computing.\n",
    "[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\n",
    "Devices Object Detection by Filter Pruning.\n",
    "[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\n",
    "and Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\n",
    "Tracking Algorithms in Edge Devices.\n",
    "[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\n",
    "Chen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\n",
    "Compression Method for Object Detection Network for Edge Devices.\n",
    "[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\n",
    "Cao. Absorption Pruning of Deep Neural Network for Object Detection\n",
    "in Remote Sensing Imagery.\n",
    "[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\n",
    "and Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\n",
    "Distribution Detection: An Empirical Survey.\n",
    "[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\n",
    "Sahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\n",
    "[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\n",
    "Compress DNNs.\n",
    "[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\n",
    "derstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\n",
    "[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\n",
    "Proving the Lottery Ticket Hypothesis: Pruning is All You Need.\n",
    "[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\n",
    "Devices via Reconstruction-Based Channel Pruning.\n",
    "[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\n",
    "Determine Reliable Paths on Networks with Random and Correlated\n",
    "Link Travel Times.\n",
    "[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\n",
    "and D. Suganthi. Real-Time Object Detection on Edge Devices Using\n",
    "Mobile Neural Networks.\n",
    "[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\n",
    "Tang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\n",
    "Models.\n",
    "[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\n",
    "Li, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\n",
    "Deng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\n",
    "Detection of Tea Leaves with One Bud and Two Leaves Based on\n",
    "ShuffleNetv2-YOLOv5-Lite-E.\n",
    "\"\"\", \"metadata\": {\"source\": \"geo_facts.txt\", \"page\": 1}},\n",
    "    {\"content\": \"The Great Wall of China is located in China and is a famous landmark.\", \"metadata\": {\"source\": \"world_landmarks.txt\", \"page\": 5}},\n",
    "    {\"content\": \"Paris is known for the Eiffel Tower.\", \"metadata\": {\"source\": \"geo_facts.txt\", \"page\": 2}},\n",
    "]\n",
    "\n",
    "# Convert raw data to LangChain Document objects\n",
    "documents = [Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"]) for doc in documents_data]\n",
    "\n",
    "# Create FAISS vector store directly from documents\n",
    "# This handles embedding the texts and building the index internally\n",
    "try:\n",
    "    vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "    print(\"FAISS vector store created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating FAISS vector store: {e}\")\n",
    "# Exit if vector store creation fails\n",
    "\n",
    "# Step 4: Load Gemini model for text generation\n",
    "\n",
    "# Step 5: Create the Retrieval-Augmented Generation pipeline\n",
    "print(\"Creating RAG chain...\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\", # Other options: \"mmr\", \"similarity_score_threshold\"\n",
    "    search_kwargs={'k': 2} # Retrieve top 2 relevant documents\n",
    ")\n",
    "\n",
    "# Use the recommended from_chain_type method\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # Options: \"stuff\", \"map_reduce\", \"refine\", \"map_rerank\"\n",
    "                        # \"stuff\" puts all retrieved docs into the context\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True # Include source documents in the output\n",
    "    # chain_type_kwargs={\"prompt\": YOUR_CUSTOM_PROMPT} # Optional: customize prompt\n",
    ")\n",
    "print(\"RAG chain created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c4dcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying the RAG system with: 'who is surya narayanaa'\n",
      "\n",
      "--- RAG System Output ---\n",
      "Answer: Surya Narayanaa N T is listed as a co-author (5th author) of the research paper \"Neuro Prune: An Adaptive Approach for Efficient Deep Neural Network Optimization on Edge Devices.\" He is a student (BE-CSE (AI&ML)) at PSG College of Technology. His email address is suryanarayanaant@gmail.com.\n",
      "\n",
      "Sources:\n",
      "  Source 1:\n",
      "    Content: \n",
      "        Neuro Prune: An Adaptive Approach for Efficient\n",
      "Deep Neural Network Optimization on Edge\n",
      "Devices\n",
      "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
      "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
      "grk.cse@psgtech.ac.in\n",
      "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
      "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
      "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
      "Abstract—The optimization of deep neural networks for de-\n",
      "ployment on edge devices is a significant research area due to\n",
      "the demand for applications such as augmented reality, smart\n",
      "cameras, and autonomous navigation. However, deploying large\n",
      "deep learning models on edge devices poses challenges related to\n",
      "computational power, energy consumption, and latency. Pruning\n",
      "is a method to reduce the model size, accelerate inference, and\n",
      "save power. The objective of the paper is to propose the Neuro\n",
      "Prune algorithm and to apply it for the optimization of deep\n",
      "neural networks on edge devices. Efforts have been made to\n",
      "compare pruned and unpruned models. As a result, the pruned\n",
      "model has an accuracy increase of 0.22%.\n",
      "I. INTRODUCTION\n",
      "The rise of intelligent systems, ranging from autonomous\n",
      "drones to augmented reality (AR) devices and smart surveil-\n",
      "lance cameras, has intensified the need for efficient deep neural\n",
      "network deployment on edge devices with constrained com-\n",
      "putational resources. While advanced deep learning models\n",
      "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
      "their deployment on such devices is hindered by significant\n",
      "computational demands, large memory footprints, and high\n",
      "energy consumption. These challenges pose critical barriers to\n",
      "real-time applications and sustainable deployment.\n",
      "This paper builds on existing pruning techniques, includ-\n",
      "ing unstructured pruning, structured pruning, and approaches\n",
      "inspired by quantization, highlighting their strengths and lim-\n",
      "itations. In response to the challenges identified, we introduce\n",
      "Neuro Prune, a novel pruning approach that integrates princi-\n",
      "ples from reinforcement learning and the Lottery Ticket Hy-\n",
      "pothesis to achieve an optimal balance between accuracy, com-\n",
      "putational efficiency, and energy consumption. By integrating\n",
      "these principles, Neuro Prune provides a robust framework\n",
      "for optimizing deep neural networks for deployment on edge\n",
      "devices without sacrificing performance.\n",
      "Neuro Prune reduces model size and computational re-\n",
      "quirements while maintaining or even improving the model’s\n",
      "accuracy. These findings underscore the potential of Neuro\n",
      "Prune to enable real-time, energy-efficient deep learning on\n",
      "resource-constrained devices.\n",
      "Neuro Prune employs a systematic process to optimize\n",
      "neural networks for edge deployment:\n",
      "• Mask Initialization: Each layer’s weights are paired with\n",
      "a binary mask (original_mask), initialized to ones.\n",
      "• Activation Tracking: During forward passes over the\n",
      "dataset, the magnitudes of weight activations are mon-\n",
      "itored and recorded.\n",
      "• Normalize and Prune: The recorded activations are\n",
      "normalized, and a threshold based on the pruning fraction\n",
      "is computed. Using this threshold, the masks are dynam-\n",
      "ically updated to prune unimportant weights.\n",
      "• Evaluate and Log Metrics: After pruning, metrics such\n",
      "as model sparsity and accuracy are computed. A reward\n",
      "metric is derived to balance accuracy and sparsity using\n",
      "a tunable parameter, λweight.\n",
      "• Retrain the Model: The pruned model undergoes retrain-\n",
      "ing to improve the loss in accuracy while maintaining low\n",
      "computational requirements\n",
      "Neuro Prune’s unique combination of reinforcement learn-\n",
      "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
      "pothesis for identifying optimal subnetworks enables it to\n",
      "achieve remarkable efficiency. This approach ensures substan-\n",
      "tial reduction in model size and energy consumption while\n",
      "preserving accuracy and inference speed, making it an ideal\n",
      "solution for edge-based deep learning applications.\n",
      "Through extensive empirical evaluations and theoretical\n",
      "insights, this paper demonstrates how Neuro Pruning outper-\n",
      "forms traditional pruning methods, setting a new benchmark\n",
      "for sustainable, high-performance AI deployment on edge\n",
      "devices. By situating our work within the broader research\n",
      "context, we aim to offer a critical contribution to the ongoing\n",
      "evolution of neural network optimization techniques.\n",
      "II. BACKGROUND\n",
      "A. Need for Pruning Techniques\n",
      "Pruning is a crucial optimization technique for deploying\n",
      "deep neural networks in resource-constrained environments\n",
      "like healthcare. For example, wearable devices like smart-\n",
      "watches that monitor heart rates or portable EEG systems\n",
      "for brain activity analysis require lightweight models due to\n",
      "their limited computational power and memory. Pruning helps\n",
      "address these constraints by reducing the memory footprint\n",
      "and computational load, enabling efficient model deployment.\n",
      "Additionally, it minimizes energy consumption, which is vital\n",
      "for sustainable and real-time healthcare applications. Faster\n",
      "inference enabled by pruning is particularly beneficial for\n",
      "time-sensitive tasks like seizure detection or emergency di-\n",
      "agnostics. Moreover, pruning enhances model interpretability\n",
      "by retaining only the most critical components, which is\n",
      "essential for gaining trust and ensuring regulatory compliance\n",
      "in healthcare. By reducing the hardware and operational costs,\n",
      "pruning also makes deploying advanced AI solutions on edge\n",
      "devices more cost-effective and accessible.\n",
      "B. Types of Pruning Techniques\n",
      "Pruning methods are broadly classified based on granu-\n",
      "larity, timing, and approach. Granularity-based pruning in-\n",
      "cludes structured pruning, which removes entire components\n",
      "such as neurons or filters to streamline computations, and\n",
      "unstructured pruning, which eliminates individual weights\n",
      "for finer optimization, though it often requires specialized\n",
      "hardware. Timing-based pruning involves pre-training pruning,\n",
      "which optimizes models before training; post-training prun-\n",
      "ing, where redundant components are removed from trained\n",
      "models with possible retraining; and dynamic pruning during\n",
      "training, which adjusts models in real-time for improved\n",
      "adaptability. Metric-based pruning uses parameters like weight\n",
      "magnitude or gradient contribution to identify and remove\n",
      "less critical components, while application-specific pruning\n",
      "tailors strategies to specific tasks, such as optimizing for EEG\n",
      "signal analysis, or hardware constraints like GPUs. Together,\n",
      "these methods enable the development of efficient, scalable,\n",
      "and task-specific models, particularly suited for edge device\n",
      "deployment in healthcare and other domains.\n",
      "III. RELATED WORK\n",
      "Jielei Wang et al.[12] introduced an absorption pruning\n",
      "method for object detection in remote sensing imagery. This\n",
      "study achieves efficient compression with minimal accuracy\n",
      "loss but requires careful layer-wise pruning ratio tuning.\n",
      "Jan Muller et al., in their research[12], propose a neural\n",
      "network pruning method for multi-object tracking (MOT) that\n",
      "reduces model size by up to 70\n",
      "Liang Li et al., in their work[9], introduce a novel pruning\n",
      "method for DNNs that utilizes a self-adaptive mechanism\n",
      "based on weight sparsity ratios and a protective reconstruction\n",
      "mechanism. Their approach improves both model compres-\n",
      "sion and accuracy, outperforming state-of-the-art methods on\n",
      "CIFAR-10 and ImageNet datasets.\n",
      "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
      "learning with edge devices like Google Coral AI and Nvidia\n",
      "Jetson Nano. Their work revolutionizes computer vision and\n",
      "real-time tracking but raises energy concerns due to the\n",
      "computational demands of advanced algorithms.\n",
      "Satoru Koda et al.[7] explore how weight pruning in\n",
      "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
      "larly with Mahalanobis-based approaches, by improving global\n",
      "feature extraction and leveraging weights not critical for\n",
      "classification.\n",
      "Shvetha S Kumar et al. analyze and compare three pruning\n",
      "techniques—L1-norm filter pruning, channel pruning, and\n",
      "weight pruning—on CNNs for accuracy and inference time,\n",
      "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
      "pared to GTX1080 Ti in their study[8].\n",
      "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
      "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
      "on stochastic networks, extending it to related problems and\n",
      "demonstrating its efficiency on real-world networks in their\n",
      "study[13].\n",
      "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
      "Training outperforms Post-Training Quantization, and sparsity\n",
      "training with Optimal Reduction pruning improves YOLOv4’s\n",
      "efficiency and edge-device performance, despite potential\n",
      "high-pruning rate drawbacks.\n",
      "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
      "method for DNNs. This approach optimizes inference speed\n",
      "and resource efficiency on edge devices with minimal accuracy\n",
      "loss but requires careful tuning of layer-specific pruning ratios.\n",
      "Yongqi An et al.[1] introduce a retraining-free structured\n",
      "pruning framework for Large Language Models (LLMs). Their\n",
      "method incorporates a fluctuation-based pruning metric, adap-\n",
      "tive structure search, and a bias compensation mechanism to\n",
      "achieve efficient pruning. FLAP significantly enhances infer-\n",
      "ence speed and reduces model size without requiring retrain-\n",
      "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
      "and Wanda-sp on LLaMA models across various benchmarks.\n",
      "From referring to these research papers, we derived an\n",
      "idea for pruning on edge devices. Additionally, our proposed\n",
      "solution is inspired by the research of Jaron Maene et al.[10],\n",
      "which suggests that sparse subnetworks in dense networks can\n",
      "achieve similar accuracy when retrained. This study demon-\n",
      "strates stable training with linear mode connectivity, support-\n",
      "ing the idea that lottery tickets retrain to similar regions, but\n",
      "questions their independence from dense training and iterative\n",
      "pruning.\n",
      "Eran Malach et al.[11] further strengthen the lottery ticket\n",
      "hypothesis by proving that over-parameterized neural networks\n",
      "with random weights always contain a subnetwork matching\n",
      "the accuracy of a target network, without additional training.\n",
      "IV. NEUROLOGICAL PRUNING APPROACH\n",
      "The Neurological Pruning Algorithm is designed to opti-\n",
      "mize neural networks by iteratively pruning less important\n",
      "weights while maintaining high performance. Unlike tradi-\n",
      "tional methods, this approach incorporates adaptive, layer-\n",
      "specific pruning thresholds that dynamically adjust based\n",
      "on weight activations, ensuring efficient model optimization\n",
      "across various stages of training. Drawing inspiration from\n",
      "the Lottery Ticket Hypothesis, which suggests that sparse\n",
      "subnetworks can perform similarly to their dense counterparts\n",
      "when retrained, the algorithm selectively deactivates weights\n",
      "using binary masks to enhance computational efficiency with-\n",
      "out sacrificing accuracy.\n",
      "The algorithm operates in several epochs, during which\n",
      "weight activations are evaluated. Based on a quantile-based\n",
      "strategy, pruning thresholds are established for each layer, en-\n",
      "suring that only weights contributing minimally to the model’s\n",
      "output are removed. After pruning, the model undergoes fine-\n",
      "tuning to restore any lost accuracy and further optimize the\n",
      "network. This step enhances the adaptability and scalability\n",
      "of the pruning method, making it suitable for deployment in\n",
      "resource-constrained environments\n",
      "A. Key Characteristics and Advantages\n",
      "The Neurological Pruning Algorithm boasts several key\n",
      "characteristics that distinguish it from conventional pruning\n",
      "techniques. One notable feature is its layer-specific pruning,\n",
      "which evaluates the importance of weights individually for\n",
      "each layer, leading to a more nuanced and efficient sparsifi-\n",
      "cation process. This approach prevents the model from losing\n",
      "essential features that may otherwise be pruned by a global\n",
      "approach.\n",
      "The algorithm’s use of quantile-based thresholding en-\n",
      "sures that only weights with the least contribution to the\n",
      "model’s output are pruned. This fine-tuned method avoids\n",
      "performance degradation by carefully targeting weights that\n",
      "are less significant to the network’s overall functionality.\n",
      "Furthermore, the pruning process is iterative, allowing for\n",
      "continuous model evaluation and real-time adjustments based\n",
      "on performance feedback, ensuring that accuracy is retained\n",
      "even as sparsity increases.\n",
      "Additionally, the algorithm maintains a careful balance\n",
      "between sparsity and accuracy, enabling a reduction in the\n",
      "model’s computational complexity while preserving its predic-\n",
      "tive capabilities. This characteristic is particularly important\n",
      "for applications requiring rapid inference times, such as in\n",
      "real-time healthcare systems and edge computing scenarios,\n",
      "where both energy efficiency and computational power are\n",
      "crucial.\n",
      "B. Impact on Model Performance\n",
      "The Neurological Pruning Algorithm significantly improves\n",
      "the efficiency of neural networks, making them more suitable\n",
      "for deployment on edge devices. Key performance metrics\n",
      "such as compression ratio (CR) and accuracy retention\n",
      "(AR) are critical indicators of the algorithm’s effectiveness.\n",
      "For instance, in evaluating a ResNet-18 model, the algorithm\n",
      "achieved a CR of 1, indicating that while the number of\n",
      "parameters remained the same, the model’s efficiency im-\n",
      "proved through selective pruning, reducing computational load\n",
      "without sacrificing performance.\n",
      "Moreover, the algorithm demonstrates the ability to achieve\n",
      "high accuracy retention, with models maintaining or even\n",
      "slightly improving their predictive performance post-pruning.\n",
      "This outcome highlights the algorithm’s capability to effi-\n",
      "ciently remove redundant weights while preserving the core\n",
      "functionality of the network. The result is a model that is\n",
      "not only more computationally efficient but also adaptable\n",
      "across different architectures and datasets, making it suit-\n",
      "able for a wide range of applications, from image classification\n",
      "to natural language processing .\n",
      "In practical terms, the Neurological Pruning Algorithm en-\n",
      "ables faster inference times and lower energy consumption,\n",
      "two crucial aspects for deploying AI models in resource-\n",
      "constrained environments. As such, it holds significant poten-\n",
      "tial for improving the viability of real-time, energy-efficient\n",
      "AI applications, particularly in fields like healthcare, where\n",
      "both speed and sustainability are essential.\n",
      "V. NEUROLOGICAL PRUNING ALGORITHM\n",
      "The Neurological Pruning Algorithm is a structured and\n",
      "adaptive methodology designed to optimize neural networks\n",
      "for resource-constrained environments. This algorithm goes\n",
      "beyond traditional pruning approaches by incorporating layer-\n",
      "specific pruning thresholds that dynamically adjust to the\n",
      "importance of weight activations within each layer. Drawing\n",
      "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
      "inforcement learning, the algorithm strikes an optimal balance\n",
      "between model sparsity and accuracy .\n",
      "A. Algorithm Description\n",
      "The Neurological Pruning Algorithm iteratively refines the\n",
      "neural network through a series of training epochs. During\n",
      "each epoch, weight activations are tracked, normalized, and\n",
      "analyzed to identify low-importance weights. These weights\n",
      "are selectively deactivated using binary masks, ensuring com-\n",
      "putational efficiency without compromising the network’s pre-\n",
      "dictive performance. The algorithm further integrates a fine-\n",
      "tuning phase, enabling the pruned network to recover and\n",
      "enhance its accuracy. The detailed process is presented in\n",
      "Algorithm 1.\n",
      "By dynamically adjusting to layer-specific characteristics\n",
      "and maintaining performance metrics, the Neurological Prun-\n",
      "ing Algorithm ensures a significant reduction in computational\n",
      "and memory requirements, paving the way for efficient deploy-\n",
      "ment of deep learning models on edge devices .\n",
      "B. Input Parameters and Output\n",
      "Inputs:\n",
      "• Pretrained model (M)\n",
      "• Dataset (D)\n",
      "• Prune fraction (p)\n",
      "• Total epochs (E)\n",
      "Output:\n",
      "• Pruned and fine-tuned model (Mpruned)\n",
      "C. Key Features and Advantages\n",
      "The Neurological Pruning Algorithm is designed with sev-\n",
      "eral distinctive features that contribute to its effectiveness\n",
      "in optimizing neural network performance. These features\n",
      "include:\n",
      "Algorithm 1 Simplified Neurological Pruning Algorithm\n",
      "Require: Pretrained model M, Dataset D, Prune fraction p,\n",
      "Total epochs E\n",
      "Ensure: Pruned and fine-tuned model Mpruned\n",
      "1: Initialize binary masks M with all ones\n",
      "2: for each epoch e = 1 to E do\n",
      "3:\n",
      "Compute weight activations A during forward passes\n",
      "4:\n",
      "for each layer i in M do\n",
      "5:\n",
      "Compute pruning threshold Ti as p-quantile of Ai\n",
      "6:\n",
      "Update mask Mi: retain weights with Ai > Ti\n",
      "7:\n",
      "Apply mask to layer: θi ←θi · Mi\n",
      "8:\n",
      "end for\n",
      "9:\n",
      "Evaluate model performance and update metrics\n",
      "10: end for\n",
      "11: Fine-tune Mpruned on D\n",
      "12: return Mpruned\n",
      "• Layer-specific Pruning: The algorithm applies pruning\n",
      "on a per-layer basis, evaluating the weight activations\n",
      "within each layer individually. This approach ensures that\n",
      "the sparsity introduced by pruning is well-distributed and\n",
      "context-dependent, facilitating more nuanced control over\n",
      "model optimization .\n",
      "• Quantile-based Thresholding: A quantile-based strat-\n",
      "egy is employed to determine the pruning threshold for\n",
      "each layer. This method ensures that only weights with\n",
      "minimal contribution to the network’s output are pruned,\n",
      "thereby reducing the risk of adversely affecting model\n",
      "accuracy while enhancing sparsity .\n",
      "• Iterative Pruning and Evaluation: The pruning pro-\n",
      "cess follows an iterative approach, enabling continuous\n",
      "assessment of the model’s performance after each pruning\n",
      "step. This ensures dynamic adjustments to maintain a bal-\n",
      "ance between model sparsity and predictive performance\n",
      "throughout training.\n",
      "• Preservation of Model Accuracy: By adjusting pruning\n",
      "thresholds in response to ongoing performance metrics,\n",
      "the algorithm ensures that model accuracy is not com-\n",
      "promised. The careful tuning of these thresholds allows\n",
      "for significant sparsity while maintaining the robustness\n",
      "of the network’s predictive capabilities .\n",
      "D. Impact on Neural Network Performance\n",
      "The Neurological Pruning Algorithm has been extensively\n",
      "evaluated across various benchmark datasets and model archi-\n",
      "tectures, demonstrating its capacity to achieve the following\n",
      "outcomes:\n",
      "• High Sparsity with Minimal Accuracy Loss: The algo-\n",
      "rithm effectively prunes unnecessary weights, achieving\n",
      "high sparsity levels without causing significant degrada-\n",
      "tion in model accuracy. This outcome is critical for the\n",
      "efficient deployment of models in resource-constrained\n",
      "environments.\n",
      "• Reduced Computational Complexity: By sparsifying\n",
      "the weight matrices, the algorithm reduces the compu-\n",
      "tational load during inference, which translates to faster\n",
      "model execution and lower resource consumption. This\n",
      "characteristic is especially beneficial in edge computing\n",
      "and real-time applications .\n",
      "• Broad Applicability: The algorithm is highly flexible\n",
      "and can be adapted to various neural network architec-\n",
      "tures and datasets, making it suitable for a wide range\n",
      "of tasks, from image recognition to natural language\n",
      "processing .\n",
      "VI. EVALUATION METRICS\n",
      "A. Performance Comparison\n",
      "Fig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\n",
      "TABLE I\n",
      "COMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\n",
      "Metric\n",
      "Unpruned\n",
      "Model\n",
      "Pruned\n",
      "Model\n",
      "Accuracy\n",
      "0.9562\n",
      "0.9583\n",
      "Train Loss\n",
      "0.1414\n",
      "0.1625\n",
      "Inference\n",
      "Time\n",
      "(s)\n",
      "0.0318\n",
      "0.0314\n",
      "No of Parame-\n",
      "ters(million)\n",
      "11.7\n",
      "11.7\n",
      "B. Evaluation Metrics Calculation\n",
      "The key metrics used to evaluate the proposed pruning\n",
      "technique are as follows:\n",
      "Compression Ratio (CR): The Compression Ratio (CR) is\n",
      "calculated as:\n",
      "CR = Total Parameters After Pruning\n",
      "Total Parameters Before Pruning\n",
      "(1)\n",
      "CR = 11.7M\n",
      "11.7M = 1\n",
      "Accuracy Retention (AR): The Accuracy Retention (AR) is\n",
      "calculated as:\n",
      "AR =\n",
      "\u0012Accuracy After Pruning −Baseline Accuracy\n",
      "Baseline Accuracy\n",
      "\u0013\n",
      "×100\n",
      "(2)\n",
      "AR =\n",
      "\u00120.958284 −0.956206\n",
      "0.956206\n",
      "\u0013\n",
      "× 100 = 0.22%\n",
      "This result indicates that the pruned model achieves an ac-\n",
      "curacy improvement of 0.22% when compared to unpruned\n",
      "model.\n",
      "C. Illustration of Results\n",
      "Fig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\n",
      "18.\n",
      "D. Discussion\n",
      "The evaluation metrics highlight the effectiveness of the\n",
      "proposed Neurological Pruning method on the ResNet-18\n",
      "model. The Compression Ratio (CR) remains 1, indicating\n",
      "no reduction in the total number of parameters, which suggests\n",
      "that this approach optimizes the neural network by selectively\n",
      "pruning connections rather than reducing the overall parameter\n",
      "count. The Accuracy Retention (AR), which is approximately\n",
      "0.22%, demonstrates that the Neurological Pruning technique\n",
      "successfully improved the accuracy compared to the baseline\n",
      "model.\n",
      "These results validate the potential of Neurological Pruning\n",
      "as an innovative approach to optimizing deep learning models.\n",
      "By maintaining accuracy while likely enhancing sparsity and\n",
      "computational efficiency, this method presents a promising di-\n",
      "rection for achieving high-performance models under resource\n",
      "constraints.\n",
      "VII. CONCLUSION\n",
      "In summary, the Neurological Pruning Algorithm provides\n",
      "a systematic and principled approach to optimizing neural\n",
      "networks. By balancing the trade-off between sparsity and\n",
      "accuracy, it enables significant reductions in computational\n",
      "complexity without sacrificing the network’s predictive per-\n",
      "formance. Future work will focus on extending the algorithm\n",
      "to dynamic network architectures and incorporating advanced\n",
      "regularization techniques to further enhance its robustness and\n",
      "adaptability in diverse applications.\n",
      "REFERENCES\n",
      "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\n",
      "Fluctuation-based Adaptive Structured Pruning for Large Language\n",
      "Models.\n",
      "[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\n",
      "Simplification of Deep Neural Network-Based Object Detector for Real-\n",
      "Time Edge Computing.\n",
      "[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\n",
      "Devices Object Detection by Filter Pruning.\n",
      "[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\n",
      "and Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\n",
      "Tracking Algorithms in Edge Devices.\n",
      "[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\n",
      "Chen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\n",
      "Compression Method for Object Detection Network for Edge Devices.\n",
      "[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\n",
      "Cao. Absorption Pruning of Deep Neural Network for Object Detection\n",
      "in Remote Sensing Imagery.\n",
      "[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\n",
      "and Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\n",
      "Distribution Detection: An Empirical Survey.\n",
      "[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\n",
      "Sahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\n",
      "[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\n",
      "Compress DNNs.\n",
      "[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\n",
      "derstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\n",
      "[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\n",
      "Proving the Lottery Ticket Hypothesis: Pruning is All You Need.\n",
      "[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\n",
      "Devices via Reconstruction-Based Channel Pruning.\n",
      "[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\n",
      "Determine Reliable Paths on Networks with Random and Correlated\n",
      "Link Travel Times.\n",
      "[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\n",
      "and D. Suganthi. Real-Time Object Detection on Edge Devices Using\n",
      "Mobile Neural Networks.\n",
      "[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\n",
      "Tang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\n",
      "Models.\n",
      "[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\n",
      "Li, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\n",
      "Deng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\n",
      "Detection of Tea Leaves with One Bud and Two Leaves Based on\n",
      "ShuffleNetv2-YOLOv5-Lite-E.\n",
      "Neuro Prune: An Adaptive Approach for Efficient\n",
      "Deep Neural Network Optimization on Edge\n",
      "Devices\n",
      "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
      "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
      "grk.cse@psgtech.ac.in\n",
      "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
      "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
      "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
      "Abstract—The optimization of deep neural networks for de-\n",
      "ployment on edge devices is a significant research area due to\n",
      "the demand for applications such as augmented reality, smart\n",
      "cameras, and autonomous navigation. However, deploying large\n",
      "deep learning models on edge devices poses challenges related to\n",
      "computational power, energy consumption, and latency. Pruning\n",
      "is a method to reduce the model size, accelerate inference, and\n",
      "save power. The objective of the paper is to propose the Neuro\n",
      "Prune algorithm and to apply it for the optimization of deep\n",
      "neural networks on edge devices. Efforts have been made to\n",
      "compare pruned and unpruned models. As a result, the pruned\n",
      "model has an accuracy increase of 0.22%.\n",
      "I. INTRODUCTION\n",
      "The rise of intelligent systems, ranging from autonomous\n",
      "drones to augmented reality (AR) devices and smart surveil-\n",
      "lance cameras, has intensified the need for efficient deep neural\n",
      "network deployment on edge devices with constrained com-\n",
      "putational resources. While advanced deep learning models\n",
      "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
      "their deployment on such devices is hindered by significant\n",
      "computational demands, large memory footprints, and high\n",
      "energy consumption. These challenges pose critical barriers to\n",
      "real-time applications and sustainable deployment.\n",
      "This paper builds on existing pruning techniques, includ-\n",
      "ing unstructured pruning, structured pruning, and approaches\n",
      "inspired by quantization, highlighting their strengths and lim-\n",
      "itations. In response to the challenges identified, we introduce\n",
      "Neuro Prune, a novel pruning approach that integrates princi-\n",
      "ples from reinforcement learning and the Lottery Ticket Hy-\n",
      "pothesis to achieve an optimal balance between accuracy, com-\n",
      "putational efficiency, and energy consumption. By integrating\n",
      "these principles, Neuro Prune provides a robust framework\n",
      "for optimizing deep neural networks for deployment on edge\n",
      "devices without sacrificing performance.\n",
      "Neuro Prune reduces model size and computational re-\n",
      "quirements while maintaining or even improving the model’s\n",
      "accuracy. These findings underscore the potential of Neuro\n",
      "Prune to enable real-time, energy-efficient deep learning on\n",
      "resource-constrained devices.\n",
      "Neuro Prune employs a systematic process to optimize\n",
      "neural networks for edge deployment:\n",
      "• Mask Initialization: Each layer’s weights are paired with\n",
      "a binary mask (original_mask), initialized to ones.\n",
      "• Activation Tracking: During forward passes over the\n",
      "dataset, the magnitudes of weight activations are mon-\n",
      "itored and recorded.\n",
      "• Normalize and Prune: The recorded activations are\n",
      "normalized, and a threshold based on the pruning fraction\n",
      "is computed. Using this threshold, the masks are dynam-\n",
      "ically updated to prune unimportant weights.\n",
      "• Evaluate and Log Metrics: After pruning, metrics such\n",
      "as model sparsity and accuracy are computed. A reward\n",
      "metric is derived to balance accuracy and sparsity using\n",
      "a tunable parameter, λweight.\n",
      "• Retrain the Model: The pruned model undergoes retrain-\n",
      "ing to improve the loss in accuracy while maintaining low\n",
      "computational requirements\n",
      "Neuro Prune’s unique combination of reinforcement learn-\n",
      "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
      "pothesis for identifying optimal subnetworks enables it to\n",
      "achieve remarkable efficiency. This approach ensures substan-\n",
      "tial reduction in model size and energy consumption while\n",
      "preserving accuracy and inference speed, making it an ideal\n",
      "solution for edge-based deep learning applications.\n",
      "Through extensive empirical evaluations and theoretical\n",
      "insights, this paper demonstrates how Neuro Pruning outper-\n",
      "forms traditional pruning methods, setting a new benchmark\n",
      "for sustainable, high-performance AI deployment on edge\n",
      "devices. By situating our work within the broader research\n",
      "context, we aim to offer a critical contribution to the ongoing\n",
      "evolution of neural network optimization techniques.\n",
      "II. BACKGROUND\n",
      "A. Need for Pruning Techniques\n",
      "Pruning is a crucial optimization technique for deploying\n",
      "deep neural networks in resource-constrained environments\n",
      "like healthcare. For example, wearable devices like smart-\n",
      "watches that monitor heart rates or portable EEG systems\n",
      "for brain activity analysis require lightweight models due to\n",
      "their limited computational power and memory. Pruning helps\n",
      "address these constraints by reducing the memory footprint\n",
      "and computational load, enabling efficient model deployment.\n",
      "Additionally, it minimizes energy consumption, which is vital\n",
      "for sustainable and real-time healthcare applications. Faster\n",
      "inference enabled by pruning is particularly beneficial for\n",
      "time-sensitive tasks like seizure detection or emergency di-\n",
      "agnostics. Moreover, pruning enhances model interpretability\n",
      "by retaining only the most critical components, which is\n",
      "essential for gaining trust and ensuring regulatory compliance\n",
      "in healthcare. By reducing the hardware and operational costs,\n",
      "pruning also makes deploying advanced AI solutions on edge\n",
      "devices more cost-effective and accessible.\n",
      "B. Types of Pruning Techniques\n",
      "Pruning methods are broadly classified based on granu-\n",
      "larity, timing, and approach. Granularity-based pruning in-\n",
      "cludes structured pruning, which removes entire components\n",
      "such as neurons or filters to streamline computations, and\n",
      "unstructured pruning, which eliminates individual weights\n",
      "for finer optimization, though it often requires specialized\n",
      "hardware. Timing-based pruning involves pre-training pruning,\n",
      "which optimizes models before training; post-training prun-\n",
      "ing, where redundant components are removed from trained\n",
      "models with possible retraining; and dynamic pruning during\n",
      "training, which adjusts models in real-time for improved\n",
      "adaptability. Metric-based pruning uses parameters like weight\n",
      "magnitude or gradient contribution to identify and remove\n",
      "less critical components, while application-specific pruning\n",
      "tailors strategies to specific tasks, such as optimizing for EEG\n",
      "signal analysis, or hardware constraints like GPUs. Together,\n",
      "these methods enable the development of efficient, scalable,\n",
      "and task-specific models, particularly suited for edge device\n",
      "deployment in healthcare and other domains.\n",
      "III. RELATED WORK\n",
      "Jielei Wang et al.[12] introduced an absorption pruning\n",
      "method for object detection in remote sensing imagery. This\n",
      "study achieves efficient compression with minimal accuracy\n",
      "loss but requires careful layer-wise pruning ratio tuning.\n",
      "Jan Muller et al., in their research[12], propose a neural\n",
      "network pruning method for multi-object tracking (MOT) that\n",
      "reduces model size by up to 70\n",
      "Liang Li et al., in their work[9], introduce a novel pruning\n",
      "method for DNNs that utilizes a self-adaptive mechanism\n",
      "based on weight sparsity ratios and a protective reconstruction\n",
      "mechanism. Their approach improves both model compres-\n",
      "sion and accuracy, outperforming state-of-the-art methods on\n",
      "CIFAR-10 and ImageNet datasets.\n",
      "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
      "learning with edge devices like Google Coral AI and Nvidia\n",
      "Jetson Nano. Their work revolutionizes computer vision and\n",
      "real-time tracking but raises energy concerns due to the\n",
      "computational demands of advanced algorithms.\n",
      "Satoru Koda et al.[7] explore how weight pruning in\n",
      "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
      "larly with Mahalanobis-based approaches, by improving global\n",
      "feature extraction and leveraging weights not critical for\n",
      "classification.\n",
      "Shvetha S Kumar et al. analyze and compare three pruning\n",
      "techniques—L1-norm filter pruning, channel pruning, and\n",
      "weight pruning—on CNNs for accuracy and inference time,\n",
      "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
      "pared to GTX1080 Ti in their study[8].\n",
      "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
      "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
      "on stochastic networks, extending it to related problems and\n",
      "demonstrating its efficiency on real-world networks in their\n",
      "study[13].\n",
      "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
      "Training outperforms Post-Training Quantization, and sparsity\n",
      "training with Optimal Reduction pruning improves YOLOv4’s\n",
      "efficiency and edge-device performance, despite potential\n",
      "high-pruning rate drawbacks.\n",
      "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
      "method for DNNs. This approach optimizes inference speed\n",
      "and resource efficiency on edge devices with minimal accuracy\n",
      "loss but requires careful tuning of layer-specific pruning ratios.\n",
      "Yongqi An et al.[1] introduce a retraining-free structured\n",
      "pruning framework for Large Language Models (LLMs). Their\n",
      "method incorporates a fluctuation-based pruning metric, adap-\n",
      "tive structure search, and a bias compensation mechanism to\n",
      "achieve efficient pruning. FLAP significantly enhances infer-\n",
      "ence speed and reduces model size without requiring retrain-\n",
      "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
      "and Wanda-sp on LLaMA models across various benchmarks.\n",
      "From referring to these research papers, we derived an\n",
      "idea for pruning on edge devices. Additionally, our proposed\n",
      "solution is inspired by the research of Jaron Maene et al.[10],\n",
      "which suggests that sparse subnetworks in dense networks can\n",
      "achieve similar accuracy when retrained. This study demon-\n",
      "strates stable training with linear mode connectivity, support-\n",
      "ing the idea that lottery tickets retrain to similar regions, but\n",
      "questions their independence from dense training and iterative\n",
      "pruning.\n",
      "Eran Malach et al.[11] further strengthen the lottery ticket\n",
      "hypothesis by proving that over-parameterized neural networks\n",
      "with random weights always contain a subnetwork matching\n",
      "the accuracy of a target network, without additional training.\n",
      "IV. NEUROLOGICAL PRUNING APPROACH\n",
      "The Neurological Pruning Algorithm is designed to opti-\n",
      "mize neural networks by iteratively pruning less important\n",
      "weights while maintaining high performance. Unlike tradi-\n",
      "tional methods, this approach incorporates adaptive, layer-\n",
      "specific pruning thresholds that dynamically adjust based\n",
      "on weight activations, ensuring efficient model optimization\n",
      "across various stages of training. Drawing inspiration from\n",
      "the Lottery Ticket Hypothesis, which suggests that sparse\n",
      "subnetworks can perform similarly to their dense counterparts\n",
      "when retrained, the algorithm selectively deactivates weights\n",
      "using binary masks to enhance computational efficiency with-\n",
      "out sacrificing accuracy.\n",
      "The algorithm operates in several epochs, during which\n",
      "weight activations are evaluated. Based on a quantile-based\n",
      "strategy, pruning thresholds are established for each layer, en-\n",
      "suring that only weights contributing minimally to the model’s\n",
      "output are removed. After pruning, the model undergoes fine-\n",
      "tuning to restore any lost accuracy and further optimize the\n",
      "network. This step enhances the adaptability and scalability\n",
      "of the pruning method, making it suitable for deployment in\n",
      "resource-constrained environments\n",
      "A. Key Characteristics and Advantages\n",
      "The Neurological Pruning Algorithm boasts several key\n",
      "characteristics that distinguish it from conventional pruning\n",
      "techniques. One notable feature is its layer-specific pruning,\n",
      "which evaluates the importance of weights individually for\n",
      "each layer, leading to a more nuanced and efficient sparsifi-\n",
      "cation process. This approach prevents the model from losing\n",
      "essential features that may otherwise be pruned by a global\n",
      "approach.\n",
      "The algorithm’s use of quantile-based thresholding en-\n",
      "sures that only weights with the least contribution to the\n",
      "model’s output are pruned. This fine-tuned method avoids\n",
      "performance degradation by carefully targeting weights that\n",
      "are less significant to the network’s overall functionality.\n",
      "Furthermore, the pruning process is iterative, allowing for\n",
      "continuous model evaluation and real-time adjustments based\n",
      "on performance feedback, ensuring that accuracy is retained\n",
      "even as sparsity increases.\n",
      "Additionally, the algorithm maintains a careful balance\n",
      "between sparsity and accuracy, enabling a reduction in the\n",
      "model’s computational complexity while preserving its predic-\n",
      "tive capabilities. This characteristic is particularly important\n",
      "for applications requiring rapid inference times, such as in\n",
      "real-time healthcare systems and edge computing scenarios,\n",
      "where both energy efficiency and computational power are\n",
      "crucial.\n",
      "B. Impact on Model Performance\n",
      "The Neurological Pruning Algorithm significantly improves\n",
      "the efficiency of neural networks, making them more suitable\n",
      "for deployment on edge devices. Key performance metrics\n",
      "such as compression ratio (CR) and accuracy retention\n",
      "(AR) are critical indicators of the algorithm’s effectiveness.\n",
      "For instance, in evaluating a ResNet-18 model, the algorithm\n",
      "achieved a CR of 1, indicating that while the number of\n",
      "parameters remained the same, the model’s efficiency im-\n",
      "proved through selective pruning, reducing computational load\n",
      "without sacrificing performance.\n",
      "Moreover, the algorithm demonstrates the ability to achieve\n",
      "high accuracy retention, with models maintaining or even\n",
      "slightly improving their predictive performance post-pruning.\n",
      "This outcome highlights the algorithm’s capability to effi-\n",
      "ciently remove redundant weights while preserving the core\n",
      "functionality of the network. The result is a model that is\n",
      "not only more computationally efficient but also adaptable\n",
      "across different architectures and datasets, making it suit-\n",
      "able for a wide range of applications, from image classification\n",
      "to natural language processing .\n",
      "In practical terms, the Neurological Pruning Algorithm en-\n",
      "ables faster inference times and lower energy consumption,\n",
      "two crucial aspects for deploying AI models in resource-\n",
      "constrained environments. As such, it holds significant poten-\n",
      "tial for improving the viability of real-time, energy-efficient\n",
      "AI applications, particularly in fields like healthcare, where\n",
      "both speed and sustainability are essential.\n",
      "V. NEUROLOGICAL PRUNING ALGORITHM\n",
      "The Neurological Pruning Algorithm is a structured and\n",
      "adaptive methodology designed to optimize neural networks\n",
      "for resource-constrained environments. This algorithm goes\n",
      "beyond traditional pruning approaches by incorporating layer-\n",
      "specific pruning thresholds that dynamically adjust to the\n",
      "importance of weight activations within each layer. Drawing\n",
      "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
      "inforcement learning, the algorithm strikes an optimal balance\n",
      "between model sparsity and accuracy .\n",
      "A. Algorithm Description\n",
      "The Neurological Pruning Algorithm iteratively refines the\n",
      "neural network through a series of training epochs. During\n",
      "each epoch, weight activations are tracked, normalized, and\n",
      "analyzed to identify low-importance weights. These weights\n",
      "are selectively deactivated using binary masks, ensuring com-\n",
      "putational efficiency without compromising the network’s pre-\n",
      "dictive performance. The algorithm further integrates a fine-\n",
      "tuning phase, enabling the pruned network to recover and\n",
      "enhance its accuracy. The detailed process is presented in\n",
      "Algorithm 1.\n",
      "By dynamically adjusting to layer-specific characteristics\n",
      "and maintaining performance metrics, the Neurological Prun-\n",
      "ing Algorithm ensures a significant reduction in computational\n",
      "and memory requirements, paving the way for efficient deploy-\n",
      "ment of deep learning models on edge devices .\n",
      "B. Input Parameters and Output\n",
      "Inputs:\n",
      "• Pretrained model (M)\n",
      "• Dataset (D)\n",
      "• Prune fraction (p)\n",
      "• Total epochs (E)\n",
      "Output:\n",
      "• Pruned and fine-tuned model (Mpruned)\n",
      "C. Key Features and Advantages\n",
      "The Neurological Pruning Algorithm is designed with sev-\n",
      "eral distinctive features that contribute to its effectiveness\n",
      "in optimizing neural network performance. These features\n",
      "include:\n",
      "Algorithm 1 Simplified Neurological Pruning Algorithm\n",
      "Require: Pretrained model M, Dataset D, Prune fraction p,\n",
      "Total epochs E\n",
      "Ensure: Pruned and fine-tuned model Mpruned\n",
      "1: Initialize binary masks M with all ones\n",
      "2: for each epoch e = 1 to E do\n",
      "3:\n",
      "Compute weight activations A during forward passes\n",
      "4:\n",
      "for each layer i in M do\n",
      "5:\n",
      "Compute pruning threshold Ti as p-quantile of Ai\n",
      "6:\n",
      "Update mask Mi: retain weights with Ai > Ti\n",
      "7:\n",
      "Apply mask to layer: θi ←θi · Mi\n",
      "8:\n",
      "end for\n",
      "9:\n",
      "Evaluate model performance and update metrics\n",
      "10: end for\n",
      "11: Fine-tune Mpruned on D\n",
      "12: return Mpruned\n",
      "• Layer-specific Pruning: The algorithm applies pruning\n",
      "on a per-layer basis, evaluating the weight activations\n",
      "within each layer individually. This approach ensures that\n",
      "the sparsity introduced by pruning is well-distributed and\n",
      "context-dependent, facilitating more nuanced control over\n",
      "model optimization .\n",
      "• Quantile-based Thresholding: A quantile-based strat-\n",
      "egy is employed to determine the pruning threshold for\n",
      "each layer. This method ensures that only weights with\n",
      "minimal contribution to the network’s output are pruned,\n",
      "thereby reducing the risk of adversely affecting model\n",
      "accuracy while enhancing sparsity .\n",
      "• Iterative Pruning and Evaluation: The pruning pro-\n",
      "cess follows an iterative approach, enabling continuous\n",
      "assessment of the model’s performance after each pruning\n",
      "step. This ensures dynamic adjustments to maintain a bal-\n",
      "ance between model sparsity and predictive performance\n",
      "throughout training.\n",
      "• Preservation of Model Accuracy: By adjusting pruning\n",
      "thresholds in response to ongoing performance metrics,\n",
      "the algorithm ensures that model accuracy is not com-\n",
      "promised. The careful tuning of these thresholds allows\n",
      "for significant sparsity while maintaining the robustness\n",
      "of the network’s predictive capabilities .\n",
      "D. Impact on Neural Network Performance\n",
      "The Neurological Pruning Algorithm has been extensively\n",
      "evaluated across various benchmark datasets and model archi-\n",
      "tectures, demonstrating its capacity to achieve the following\n",
      "outcomes:\n",
      "• High Sparsity with Minimal Accuracy Loss: The algo-\n",
      "rithm effectively prunes unnecessary weights, achieving\n",
      "high sparsity levels without causing significant degrada-\n",
      "tion in model accuracy. This outcome is critical for the\n",
      "efficient deployment of models in resource-constrained\n",
      "environments.\n",
      "• Reduced Computational Complexity: By sparsifying\n",
      "the weight matrices, the algorithm reduces the compu-\n",
      "tational load during inference, which translates to faster\n",
      "model execution and lower resource consumption. This\n",
      "characteristic is especially beneficial in edge computing\n",
      "and real-time applications .\n",
      "• Broad Applicability: The algorithm is highly flexible\n",
      "and can be adapted to various neural network architec-\n",
      "tures and datasets, making it suitable for a wide range\n",
      "of tasks, from image recognition to natural language\n",
      "processing .\n",
      "VI. EVALUATION METRICS\n",
      "A. Performance Comparison\n",
      "Fig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\n",
      "TABLE I\n",
      "COMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\n",
      "Metric\n",
      "Unpruned\n",
      "Model\n",
      "Pruned\n",
      "Model\n",
      "Accuracy\n",
      "0.9562\n",
      "0.9583\n",
      "Train Loss\n",
      "0.1414\n",
      "0.1625\n",
      "Inference\n",
      "Time\n",
      "(s)\n",
      "0.0318\n",
      "0.0314\n",
      "No of Parame-\n",
      "ters(million)\n",
      "11.7\n",
      "11.7\n",
      "B. Evaluation Metrics Calculation\n",
      "The key metrics used to evaluate the proposed pruning\n",
      "technique are as follows:\n",
      "Compression Ratio (CR): The Compression Ratio (CR) is\n",
      "calculated as:\n",
      "CR = Total Parameters After Pruning\n",
      "Total Parameters Before Pruning\n",
      "(1)\n",
      "CR = 11.7M\n",
      "11.7M = 1\n",
      "Accuracy Retention (AR): The Accuracy Retention (AR) is\n",
      "calculated as:\n",
      "AR =\n",
      "\u0012Accuracy After Pruning −Baseline Accuracy\n",
      "Baseline Accuracy\n",
      "\u0013\n",
      "×100\n",
      "(2)\n",
      "AR =\n",
      "\u00120.958284 −0.956206\n",
      "0.956206\n",
      "\u0013\n",
      "× 100 = 0.22%\n",
      "This result indicates that the pruned model achieves an ac-\n",
      "curacy improvement of 0.22% when compared to unpruned\n",
      "model.\n",
      "C. Illustration of Results\n",
      "Fig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\n",
      "18.\n",
      "D. Discussion\n",
      "The evaluation metrics highlight the effectiveness of the\n",
      "proposed Neurological Pruning method on the ResNet-18\n",
      "model. The Compression Ratio (CR) remains 1, indicating\n",
      "no reduction in the total number of parameters, which suggests\n",
      "that this approach optimizes the neural network by selectively\n",
      "pruning connections rather than reducing the overall parameter\n",
      "count. The Accuracy Retention (AR), which is approximately\n",
      "0.22%, demonstrates that the Neurological Pruning technique\n",
      "successfully improved the accuracy compared to the baseline\n",
      "model.\n",
      "These results validate the potential of Neurological Pruning\n",
      "as an innovative approach to optimizing deep learning models.\n",
      "By maintaining accuracy while likely enhancing sparsity and\n",
      "computational efficiency, this method presents a promising di-\n",
      "rection for achieving high-performance models under resource\n",
      "constraints.\n",
      "VII. CONCLUSION\n",
      "In summary, the Neurological Pruning Algorithm provides\n",
      "a systematic and principled approach to optimizing neural\n",
      "networks. By balancing the trade-off between sparsity and\n",
      "accuracy, it enables significant reductions in computational\n",
      "complexity without sacrificing the network’s predictive per-\n",
      "formance. Future work will focus on extending the algorithm\n",
      "to dynamic network architectures and incorporating advanced\n",
      "regularization techniques to further enhance its robustness and\n",
      "adaptability in diverse applications.\n",
      "REFERENCES\n",
      "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\n",
      "Fluctuation-based Adaptive Structured Pruning for Large Language\n",
      "Models.\n",
      "[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\n",
      "Simplification of Deep Neural Network-Based Object Detector for Real-\n",
      "Time Edge Computing.\n",
      "[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\n",
      "Devices Object Detection by Filter Pruning.\n",
      "[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\n",
      "and Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\n",
      "Tracking Algorithms in Edge Devices.\n",
      "[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\n",
      "Chen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\n",
      "Compression Method for Object Detection Network for Edge Devices.\n",
      "[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\n",
      "Cao. Absorption Pruning of Deep Neural Network for Object Detection\n",
      "in Remote Sensing Imagery.\n",
      "[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\n",
      "and Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\n",
      "Distribution Detection: An Empirical Survey.\n",
      "[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\n",
      "Sahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\n",
      "[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\n",
      "Compress DNNs.\n",
      "[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\n",
      "derstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\n",
      "[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\n",
      "Proving the Lottery Ticket Hypothesis: Pruning is All You Need.\n",
      "[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\n",
      "Devices via Reconstruction-Based Channel Pruning.\n",
      "[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\n",
      "Determine Reliable Paths on Networks with Random and Correlated\n",
      "Link Travel Times.\n",
      "[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\n",
      "and D. Suganthi. Real-Time Object Detection on Edge Devices Using\n",
      "Mobile Neural Networks.\n",
      "[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\n",
      "Tang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\n",
      "Models.\n",
      "[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\n",
      "Li, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\n",
      "Deng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\n",
      "Detection of Tea Leaves with One Bud and Two Leaves Based on\n",
      "ShuffleNetv2-YOLOv5-Lite-E.\n",
      "\n",
      "    Metadata: {'source': 'geo_facts.txt', 'page': 1}\n",
      "  Source 2:\n",
      "    Content: The Great Wall of China is located in China and is a famous landmark.\n",
      "    Metadata: {'source': 'world_landmarks.txt', 'page': 5}\n",
      "--- End of Output ---\n"
     ]
    }
   ],
   "source": [
    "# rag_faiss_system.py\n",
    "\n",
    "# --- Dependencies ---\n",
    "# Make sure you have installed the necessary packages:\n",
    "# pip install langchain faiss-cpu sentence-transformers langchain-google-genai python-dotenv\n",
    "# (or faiss-gpu if you have CUDA installed and want GPU acceleration)\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS # Use langchain_community\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings # Use langchain_community\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # Import Gemini\n",
    "from langchain.docstore.document import Document # For structured documents\n",
    "\n",
    "# --- Configuration ---\n",
    "# Load environment variables (especially GOOGLE_API_KEY)\n",
    "load_dotenv() \n",
    "\n",
    "# Check if the API key is available\n",
    "if os.getenv(\"GOOGLE_API_KEY\") is None:\n",
    "    print(\"Error: GOOGLE_API_KEY environment variable not set.\")\n",
    "    print(\"Please get an API key from Google AI Studio (https://aistudio.google.com/app/apikey)\")\n",
    "    print(\"and set it as an environment variable (e.g., in a .env file).\")\n",
    "    exit() # Exit if the key is missing\n",
    "\n",
    "# --- Main Function ---\n",
    "# Step 1: Initialize the embedding model\n",
    "print(\"Initializing embedding model...\")\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'} # Specify device (cpu or cuda)\n",
    "    # Set encode_kwargs={'normalize_embeddings': True} if using cosine similarity,\n",
    "    # but FAISS IndexFlatL2 uses L2 distance (Euclidean), so normalization is optional \n",
    "    # but often helpful. Let's keep it simple for L2.\n",
    ")\n",
    "print(\"Embedding model initialized.\")\n",
    "\n",
    "# Step 2 & 3: Prepare documents and create FAISS index\n",
    "print(\"Preparing documents and creating FAISS vector store...\")\n",
    "documents_data = [\n",
    "    {\"content\": \"\"\"\n",
    "        Neuro Prune: An Adaptive Approach for Efficient\n",
    "Deep Neural Network Optimization on Edge\n",
    "Devices\n",
    "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
    "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
    "grk.cse@psgtech.ac.in\n",
    "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
    "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
    "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
    "Abstract—The optimization of deep neural networks for de-\n",
    "ployment on edge devices is a significant research area due to\n",
    "the demand for applications such as augmented reality, smart\n",
    "cameras, and autonomous navigation. However, deploying large\n",
    "deep learning models on edge devices poses challenges related to\n",
    "computational power, energy consumption, and latency. Pruning\n",
    "is a method to reduce the model size, accelerate inference, and\n",
    "save power. The objective of the paper is to propose the Neuro\n",
    "Prune algorithm and to apply it for the optimization of deep\n",
    "neural networks on edge devices. Efforts have been made to\n",
    "compare pruned and unpruned models. As a result, the pruned\n",
    "model has an accuracy increase of 0.22%.\n",
    "I. INTRODUCTION\n",
    "The rise of intelligent systems, ranging from autonomous\n",
    "drones to augmented reality (AR) devices and smart surveil-\n",
    "lance cameras, has intensified the need for efficient deep neural\n",
    "network deployment on edge devices with constrained com-\n",
    "putational resources. While advanced deep learning models\n",
    "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
    "their deployment on such devices is hindered by significant\n",
    "computational demands, large memory footprints, and high\n",
    "energy consumption. These challenges pose critical barriers to\n",
    "real-time applications and sustainable deployment.\n",
    "This paper builds on existing pruning techniques, includ-\n",
    "ing unstructured pruning, structured pruning, and approaches\n",
    "inspired by quantization, highlighting their strengths and lim-\n",
    "itations. In response to the challenges identified, we introduce\n",
    "Neuro Prune, a novel pruning approach that integrates princi-\n",
    "ples from reinforcement learning and the Lottery Ticket Hy-\n",
    "pothesis to achieve an optimal balance between accuracy, com-\n",
    "putational efficiency, and energy consumption. By integrating\n",
    "these principles, Neuro Prune provides a robust framework\n",
    "for optimizing deep neural networks for deployment on edge\n",
    "devices without sacrificing performance.\n",
    "Neuro Prune reduces model size and computational re-\n",
    "quirements while maintaining or even improving the model’s\n",
    "accuracy. These findings underscore the potential of Neuro\n",
    "Prune to enable real-time, energy-efficient deep learning on\n",
    "resource-constrained devices.\n",
    "Neuro Prune employs a systematic process to optimize\n",
    "neural networks for edge deployment:\n",
    "• Mask Initialization: Each layer’s weights are paired with\n",
    "a binary mask (original_mask), initialized to ones.\n",
    "• Activation Tracking: During forward passes over the\n",
    "dataset, the magnitudes of weight activations are mon-\n",
    "itored and recorded.\n",
    "• Normalize and Prune: The recorded activations are\n",
    "normalized, and a threshold based on the pruning fraction\n",
    "is computed. Using this threshold, the masks are dynam-\n",
    "ically updated to prune unimportant weights.\n",
    "• Evaluate and Log Metrics: After pruning, metrics such\n",
    "as model sparsity and accuracy are computed. A reward\n",
    "metric is derived to balance accuracy and sparsity using\n",
    "a tunable parameter, λweight.\n",
    "• Retrain the Model: The pruned model undergoes retrain-\n",
    "ing to improve the loss in accuracy while maintaining low\n",
    "computational requirements\n",
    "Neuro Prune’s unique combination of reinforcement learn-\n",
    "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
    "pothesis for identifying optimal subnetworks enables it to\n",
    "achieve remarkable efficiency. This approach ensures substan-\n",
    "tial reduction in model size and energy consumption while\n",
    "preserving accuracy and inference speed, making it an ideal\n",
    "solution for edge-based deep learning applications.\n",
    "Through extensive empirical evaluations and theoretical\n",
    "insights, this paper demonstrates how Neuro Pruning outper-\n",
    "forms traditional pruning methods, setting a new benchmark\n",
    "for sustainable, high-performance AI deployment on edge\n",
    "devices. By situating our work within the broader research\n",
    "context, we aim to offer a critical contribution to the ongoing\n",
    "evolution of neural network optimization techniques.\n",
    "II. BACKGROUND\n",
    "A. Need for Pruning Techniques\n",
    "Pruning is a crucial optimization technique for deploying\n",
    "deep neural networks in resource-constrained environments\n",
    "like healthcare. For example, wearable devices like smart-\n",
    "watches that monitor heart rates or portable EEG systems\n",
    "for brain activity analysis require lightweight models due to\n",
    "their limited computational power and memory. Pruning helps\n",
    "address these constraints by reducing the memory footprint\n",
    "and computational load, enabling efficient model deployment.\n",
    "Additionally, it minimizes energy consumption, which is vital\n",
    "for sustainable and real-time healthcare applications. Faster\n",
    "inference enabled by pruning is particularly beneficial for\n",
    "time-sensitive tasks like seizure detection or emergency di-\n",
    "agnostics. Moreover, pruning enhances model interpretability\n",
    "by retaining only the most critical components, which is\n",
    "essential for gaining trust and ensuring regulatory compliance\n",
    "in healthcare. By reducing the hardware and operational costs,\n",
    "pruning also makes deploying advanced AI solutions on edge\n",
    "devices more cost-effective and accessible.\n",
    "B. Types of Pruning Techniques\n",
    "Pruning methods are broadly classified based on granu-\n",
    "larity, timing, and approach. Granularity-based pruning in-\n",
    "cludes structured pruning, which removes entire components\n",
    "such as neurons or filters to streamline computations, and\n",
    "unstructured pruning, which eliminates individual weights\n",
    "for finer optimization, though it often requires specialized\n",
    "hardware. Timing-based pruning involves pre-training pruning,\n",
    "which optimizes models before training; post-training prun-\n",
    "ing, where redundant components are removed from trained\n",
    "models with possible retraining; and dynamic pruning during\n",
    "training, which adjusts models in real-time for improved\n",
    "adaptability. Metric-based pruning uses parameters like weight\n",
    "magnitude or gradient contribution to identify and remove\n",
    "less critical components, while application-specific pruning\n",
    "tailors strategies to specific tasks, such as optimizing for EEG\n",
    "signal analysis, or hardware constraints like GPUs. Together,\n",
    "these methods enable the development of efficient, scalable,\n",
    "and task-specific models, particularly suited for edge device\n",
    "deployment in healthcare and other domains.\n",
    "III. RELATED WORK\n",
    "Jielei Wang et al.[12] introduced an absorption pruning\n",
    "method for object detection in remote sensing imagery. This\n",
    "study achieves efficient compression with minimal accuracy\n",
    "loss but requires careful layer-wise pruning ratio tuning.\n",
    "Jan Muller et al., in their research[12], propose a neural\n",
    "network pruning method for multi-object tracking (MOT) that\n",
    "reduces model size by up to 70\n",
    "Liang Li et al., in their work[9], introduce a novel pruning\n",
    "method for DNNs that utilizes a self-adaptive mechanism\n",
    "based on weight sparsity ratios and a protective reconstruction\n",
    "mechanism. Their approach improves both model compres-\n",
    "sion and accuracy, outperforming state-of-the-art methods on\n",
    "CIFAR-10 and ImageNet datasets.\n",
    "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
    "learning with edge devices like Google Coral AI and Nvidia\n",
    "Jetson Nano. Their work revolutionizes computer vision and\n",
    "real-time tracking but raises energy concerns due to the\n",
    "computational demands of advanced algorithms.\n",
    "Satoru Koda et al.[7] explore how weight pruning in\n",
    "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
    "larly with Mahalanobis-based approaches, by improving global\n",
    "feature extraction and leveraging weights not critical for\n",
    "classification.\n",
    "Shvetha S Kumar et al. analyze and compare three pruning\n",
    "techniques—L1-norm filter pruning, channel pruning, and\n",
    "weight pruning—on CNNs for accuracy and inference time,\n",
    "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
    "pared to GTX1080 Ti in their study[8].\n",
    "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
    "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
    "on stochastic networks, extending it to related problems and\n",
    "demonstrating its efficiency on real-world networks in their\n",
    "study[13].\n",
    "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
    "Training outperforms Post-Training Quantization, and sparsity\n",
    "training with Optimal Reduction pruning improves YOLOv4’s\n",
    "efficiency and edge-device performance, despite potential\n",
    "high-pruning rate drawbacks.\n",
    "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
    "method for DNNs. This approach optimizes inference speed\n",
    "and resource efficiency on edge devices with minimal accuracy\n",
    "loss but requires careful tuning of layer-specific pruning ratios.\n",
    "Yongqi An et al.[1] introduce a retraining-free structured\n",
    "pruning framework for Large Language Models (LLMs). Their\n",
    "method incorporates a fluctuation-based pruning metric, adap-\n",
    "tive structure search, and a bias compensation mechanism to\n",
    "achieve efficient pruning. FLAP significantly enhances infer-\n",
    "ence speed and reduces model size without requiring retrain-\n",
    "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
    "and Wanda-sp on LLaMA models across various benchmarks.\n",
    "From referring to these research papers, we derived an\n",
    "idea for pruning on edge devices. Additionally, our proposed\n",
    "solution is inspired by the research of Jaron Maene et al.[10],\n",
    "which suggests that sparse subnetworks in dense networks can\n",
    "achieve similar accuracy when retrained. This study demon-\n",
    "strates stable training with linear mode connectivity, support-\n",
    "ing the idea that lottery tickets retrain to similar regions, but\n",
    "questions their independence from dense training and iterative\n",
    "pruning.\n",
    "Eran Malach et al.[11] further strengthen the lottery ticket\n",
    "hypothesis by proving that over-parameterized neural networks\n",
    "with random weights always contain a subnetwork matching\n",
    "the accuracy of a target network, without additional training.\n",
    "IV. NEUROLOGICAL PRUNING APPROACH\n",
    "The Neurological Pruning Algorithm is designed to opti-\n",
    "mize neural networks by iteratively pruning less important\n",
    "weights while maintaining high performance. Unlike tradi-\n",
    "tional methods, this approach incorporates adaptive, layer-\n",
    "specific pruning thresholds that dynamically adjust based\n",
    "on weight activations, ensuring efficient model optimization\n",
    "across various stages of training. Drawing inspiration from\n",
    "the Lottery Ticket Hypothesis, which suggests that sparse\n",
    "subnetworks can perform similarly to their dense counterparts\n",
    "when retrained, the algorithm selectively deactivates weights\n",
    "using binary masks to enhance computational efficiency with-\n",
    "out sacrificing accuracy.\n",
    "The algorithm operates in several epochs, during which\n",
    "weight activations are evaluated. Based on a quantile-based\n",
    "strategy, pruning thresholds are established for each layer, en-\n",
    "suring that only weights contributing minimally to the model’s\n",
    "output are removed. After pruning, the model undergoes fine-\n",
    "tuning to restore any lost accuracy and further optimize the\n",
    "network. This step enhances the adaptability and scalability\n",
    "of the pruning method, making it suitable for deployment in\n",
    "resource-constrained environments\n",
    "A. Key Characteristics and Advantages\n",
    "The Neurological Pruning Algorithm boasts several key\n",
    "characteristics that distinguish it from conventional pruning\n",
    "techniques. One notable feature is its layer-specific pruning,\n",
    "which evaluates the importance of weights individually for\n",
    "each layer, leading to a more nuanced and efficient sparsifi-\n",
    "cation process. This approach prevents the model from losing\n",
    "essential features that may otherwise be pruned by a global\n",
    "approach.\n",
    "The algorithm’s use of quantile-based thresholding en-\n",
    "sures that only weights with the least contribution to the\n",
    "model’s output are pruned. This fine-tuned method avoids\n",
    "performance degradation by carefully targeting weights that\n",
    "are less significant to the network’s overall functionality.\n",
    "Furthermore, the pruning process is iterative, allowing for\n",
    "continuous model evaluation and real-time adjustments based\n",
    "on performance feedback, ensuring that accuracy is retained\n",
    "even as sparsity increases.\n",
    "Additionally, the algorithm maintains a careful balance\n",
    "between sparsity and accuracy, enabling a reduction in the\n",
    "model’s computational complexity while preserving its predic-\n",
    "tive capabilities. This characteristic is particularly important\n",
    "for applications requiring rapid inference times, such as in\n",
    "real-time healthcare systems and edge computing scenarios,\n",
    "where both energy efficiency and computational power are\n",
    "crucial.\n",
    "B. Impact on Model Performance\n",
    "The Neurological Pruning Algorithm significantly improves\n",
    "the efficiency of neural networks, making them more suitable\n",
    "for deployment on edge devices. Key performance metrics\n",
    "such as compression ratio (CR) and accuracy retention\n",
    "(AR) are critical indicators of the algorithm’s effectiveness.\n",
    "For instance, in evaluating a ResNet-18 model, the algorithm\n",
    "achieved a CR of 1, indicating that while the number of\n",
    "parameters remained the same, the model’s efficiency im-\n",
    "proved through selective pruning, reducing computational load\n",
    "without sacrificing performance.\n",
    "Moreover, the algorithm demonstrates the ability to achieve\n",
    "high accuracy retention, with models maintaining or even\n",
    "slightly improving their predictive performance post-pruning.\n",
    "This outcome highlights the algorithm’s capability to effi-\n",
    "ciently remove redundant weights while preserving the core\n",
    "functionality of the network. The result is a model that is\n",
    "not only more computationally efficient but also adaptable\n",
    "across different architectures and datasets, making it suit-\n",
    "able for a wide range of applications, from image classification\n",
    "to natural language processing .\n",
    "In practical terms, the Neurological Pruning Algorithm en-\n",
    "ables faster inference times and lower energy consumption,\n",
    "two crucial aspects for deploying AI models in resource-\n",
    "constrained environments. As such, it holds significant poten-\n",
    "tial for improving the viability of real-time, energy-efficient\n",
    "AI applications, particularly in fields like healthcare, where\n",
    "both speed and sustainability are essential.\n",
    "V. NEUROLOGICAL PRUNING ALGORITHM\n",
    "The Neurological Pruning Algorithm is a structured and\n",
    "adaptive methodology designed to optimize neural networks\n",
    "for resource-constrained environments. This algorithm goes\n",
    "beyond traditional pruning approaches by incorporating layer-\n",
    "specific pruning thresholds that dynamically adjust to the\n",
    "importance of weight activations within each layer. Drawing\n",
    "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
    "inforcement learning, the algorithm strikes an optimal balance\n",
    "between model sparsity and accuracy .\n",
    "A. Algorithm Description\n",
    "The Neurological Pruning Algorithm iteratively refines the\n",
    "neural network through a series of training epochs. During\n",
    "each epoch, weight activations are tracked, normalized, and\n",
    "analyzed to identify low-importance weights. These weights\n",
    "are selectively deactivated using binary masks, ensuring com-\n",
    "putational efficiency without compromising the network’s pre-\n",
    "dictive performance. The algorithm further integrates a fine-\n",
    "tuning phase, enabling the pruned network to recover and\n",
    "enhance its accuracy. The detailed process is presented in\n",
    "Algorithm 1.\n",
    "By dynamically adjusting to layer-specific characteristics\n",
    "and maintaining performance metrics, the Neurological Prun-\n",
    "ing Algorithm ensures a significant reduction in computational\n",
    "and memory requirements, paving the way for efficient deploy-\n",
    "ment of deep learning models on edge devices .\n",
    "B. Input Parameters and Output\n",
    "Inputs:\n",
    "• Pretrained model (M)\n",
    "• Dataset (D)\n",
    "• Prune fraction (p)\n",
    "• Total epochs (E)\n",
    "Output:\n",
    "• Pruned and fine-tuned model (Mpruned)\n",
    "C. Key Features and Advantages\n",
    "The Neurological Pruning Algorithm is designed with sev-\n",
    "eral distinctive features that contribute to its effectiveness\n",
    "in optimizing neural network performance. These features\n",
    "include:\n",
    "Algorithm 1 Simplified Neurological Pruning Algorithm\n",
    "Require: Pretrained model M, Dataset D, Prune fraction p,\n",
    "Total epochs E\n",
    "Ensure: Pruned and fine-tuned model Mpruned\n",
    "1: Initialize binary masks M with all ones\n",
    "2: for each epoch e = 1 to E do\n",
    "3:\n",
    "Compute weight activations A during forward passes\n",
    "4:\n",
    "for each layer i in M do\n",
    "5:\n",
    "Compute pruning threshold Ti as p-quantile of Ai\n",
    "6:\n",
    "Update mask Mi: retain weights with Ai > Ti\n",
    "7:\n",
    "Apply mask to layer: θi ←θi · Mi\n",
    "8:\n",
    "end for\n",
    "9:\n",
    "Evaluate model performance and update metrics\n",
    "10: end for\n",
    "11: Fine-tune Mpruned on D\n",
    "12: return Mpruned\n",
    "• Layer-specific Pruning: The algorithm applies pruning\n",
    "on a per-layer basis, evaluating the weight activations\n",
    "within each layer individually. This approach ensures that\n",
    "the sparsity introduced by pruning is well-distributed and\n",
    "context-dependent, facilitating more nuanced control over\n",
    "model optimization .\n",
    "• Quantile-based Thresholding: A quantile-based strat-\n",
    "egy is employed to determine the pruning threshold for\n",
    "each layer. This method ensures that only weights with\n",
    "minimal contribution to the network’s output are pruned,\n",
    "thereby reducing the risk of adversely affecting model\n",
    "accuracy while enhancing sparsity .\n",
    "• Iterative Pruning and Evaluation: The pruning pro-\n",
    "cess follows an iterative approach, enabling continuous\n",
    "assessment of the model’s performance after each pruning\n",
    "step. This ensures dynamic adjustments to maintain a bal-\n",
    "ance between model sparsity and predictive performance\n",
    "throughout training.\n",
    "• Preservation of Model Accuracy: By adjusting pruning\n",
    "thresholds in response to ongoing performance metrics,\n",
    "the algorithm ensures that model accuracy is not com-\n",
    "promised. The careful tuning of these thresholds allows\n",
    "for significant sparsity while maintaining the robustness\n",
    "of the network’s predictive capabilities .\n",
    "D. Impact on Neural Network Performance\n",
    "The Neurological Pruning Algorithm has been extensively\n",
    "evaluated across various benchmark datasets and model archi-\n",
    "tectures, demonstrating its capacity to achieve the following\n",
    "outcomes:\n",
    "• High Sparsity with Minimal Accuracy Loss: The algo-\n",
    "rithm effectively prunes unnecessary weights, achieving\n",
    "high sparsity levels without causing significant degrada-\n",
    "tion in model accuracy. This outcome is critical for the\n",
    "efficient deployment of models in resource-constrained\n",
    "environments.\n",
    "• Reduced Computational Complexity: By sparsifying\n",
    "the weight matrices, the algorithm reduces the compu-\n",
    "tational load during inference, which translates to faster\n",
    "model execution and lower resource consumption. This\n",
    "characteristic is especially beneficial in edge computing\n",
    "and real-time applications .\n",
    "• Broad Applicability: The algorithm is highly flexible\n",
    "and can be adapted to various neural network architec-\n",
    "tures and datasets, making it suitable for a wide range\n",
    "of tasks, from image recognition to natural language\n",
    "processing .\n",
    "VI. EVALUATION METRICS\n",
    "A. Performance Comparison\n",
    "Fig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\n",
    "TABLE I\n",
    "COMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\n",
    "Metric\n",
    "Unpruned\n",
    "Model\n",
    "Pruned\n",
    "Model\n",
    "Accuracy\n",
    "0.9562\n",
    "0.9583\n",
    "Train Loss\n",
    "0.1414\n",
    "0.1625\n",
    "Inference\n",
    "Time\n",
    "(s)\n",
    "0.0318\n",
    "0.0314\n",
    "No of Parame-\n",
    "ters(million)\n",
    "11.7\n",
    "11.7\n",
    "B. Evaluation Metrics Calculation\n",
    "The key metrics used to evaluate the proposed pruning\n",
    "technique are as follows:\n",
    "Compression Ratio (CR): The Compression Ratio (CR) is\n",
    "calculated as:\n",
    "CR = Total Parameters After Pruning\n",
    "Total Parameters Before Pruning\n",
    "(1)\n",
    "CR = 11.7M\n",
    "11.7M = 1\n",
    "Accuracy Retention (AR): The Accuracy Retention (AR) is\n",
    "calculated as:\n",
    "AR =\n",
    "\u0012Accuracy After Pruning −Baseline Accuracy\n",
    "Baseline Accuracy\n",
    "\u0013\n",
    "×100\n",
    "(2)\n",
    "AR =\n",
    "\u00120.958284 −0.956206\n",
    "0.956206\n",
    "\u0013\n",
    "× 100 = 0.22%\n",
    "This result indicates that the pruned model achieves an ac-\n",
    "curacy improvement of 0.22% when compared to unpruned\n",
    "model.\n",
    "C. Illustration of Results\n",
    "Fig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\n",
    "18.\n",
    "D. Discussion\n",
    "The evaluation metrics highlight the effectiveness of the\n",
    "proposed Neurological Pruning method on the ResNet-18\n",
    "model. The Compression Ratio (CR) remains 1, indicating\n",
    "no reduction in the total number of parameters, which suggests\n",
    "that this approach optimizes the neural network by selectively\n",
    "pruning connections rather than reducing the overall parameter\n",
    "count. The Accuracy Retention (AR), which is approximately\n",
    "0.22%, demonstrates that the Neurological Pruning technique\n",
    "successfully improved the accuracy compared to the baseline\n",
    "model.\n",
    "These results validate the potential of Neurological Pruning\n",
    "as an innovative approach to optimizing deep learning models.\n",
    "By maintaining accuracy while likely enhancing sparsity and\n",
    "computational efficiency, this method presents a promising di-\n",
    "rection for achieving high-performance models under resource\n",
    "constraints.\n",
    "VII. CONCLUSION\n",
    "In summary, the Neurological Pruning Algorithm provides\n",
    "a systematic and principled approach to optimizing neural\n",
    "networks. By balancing the trade-off between sparsity and\n",
    "accuracy, it enables significant reductions in computational\n",
    "complexity without sacrificing the network’s predictive per-\n",
    "formance. Future work will focus on extending the algorithm\n",
    "to dynamic network architectures and incorporating advanced\n",
    "regularization techniques to further enhance its robustness and\n",
    "adaptability in diverse applications.\n",
    "REFERENCES\n",
    "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\n",
    "Fluctuation-based Adaptive Structured Pruning for Large Language\n",
    "Models.\n",
    "[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\n",
    "Simplification of Deep Neural Network-Based Object Detector for Real-\n",
    "Time Edge Computing.\n",
    "[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\n",
    "Devices Object Detection by Filter Pruning.\n",
    "[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\n",
    "and Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\n",
    "Tracking Algorithms in Edge Devices.\n",
    "[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\n",
    "Chen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\n",
    "Compression Method for Object Detection Network for Edge Devices.\n",
    "[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\n",
    "Cao. Absorption Pruning of Deep Neural Network for Object Detection\n",
    "in Remote Sensing Imagery.\n",
    "[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\n",
    "and Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\n",
    "Distribution Detection: An Empirical Survey.\n",
    "[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\n",
    "Sahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\n",
    "[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\n",
    "Compress DNNs.\n",
    "[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\n",
    "derstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\n",
    "[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\n",
    "Proving the Lottery Ticket Hypothesis: Pruning is All You Need.\n",
    "[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\n",
    "Devices via Reconstruction-Based Channel Pruning.\n",
    "[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\n",
    "Determine Reliable Paths on Networks with Random and Correlated\n",
    "Link Travel Times.\n",
    "[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\n",
    "and D. Suganthi. Real-Time Object Detection on Edge Devices Using\n",
    "Mobile Neural Networks.\n",
    "[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\n",
    "Tang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\n",
    "Models.\n",
    "[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\n",
    "Li, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\n",
    "Deng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\n",
    "Detection of Tea Leaves with One Bud and Two Leaves Based on\n",
    "ShuffleNetv2-YOLOv5-Lite-E.\n",
    "Neuro Prune: An Adaptive Approach for Efficient\n",
    "Deep Neural Network Optimization on Edge\n",
    "Devices\n",
    "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
    "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
    "grk.cse@psgtech.ac.in\n",
    "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
    "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
    "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
    "Abstract—The optimization of deep neural networks for de-\n",
    "ployment on edge devices is a significant research area due to\n",
    "the demand for applications such as augmented reality, smart\n",
    "cameras, and autonomous navigation. However, deploying large\n",
    "deep learning models on edge devices poses challenges related to\n",
    "computational power, energy consumption, and latency. Pruning\n",
    "is a method to reduce the model size, accelerate inference, and\n",
    "save power. The objective of the paper is to propose the Neuro\n",
    "Prune algorithm and to apply it for the optimization of deep\n",
    "neural networks on edge devices. Efforts have been made to\n",
    "compare pruned and unpruned models. As a result, the pruned\n",
    "model has an accuracy increase of 0.22%.\n",
    "I. INTRODUCTION\n",
    "The rise of intelligent systems, ranging from autonomous\n",
    "drones to augmented reality (AR) devices and smart surveil-\n",
    "lance cameras, has intensified the need for efficient deep neural\n",
    "network deployment on edge devices with constrained com-\n",
    "putational resources. While advanced deep learning models\n",
    "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
    "their deployment on such devices is hindered by significant\n",
    "computational demands, large memory footprints, and high\n",
    "energy consumption. These challenges pose critical barriers to\n",
    "real-time applications and sustainable deployment.\n",
    "This paper builds on existing pruning techniques, includ-\n",
    "ing unstructured pruning, structured pruning, and approaches\n",
    "inspired by quantization, highlighting their strengths and lim-\n",
    "itations. In response to the challenges identified, we introduce\n",
    "Neuro Prune, a novel pruning approach that integrates princi-\n",
    "ples from reinforcement learning and the Lottery Ticket Hy-\n",
    "pothesis to achieve an optimal balance between accuracy, com-\n",
    "putational efficiency, and energy consumption. By integrating\n",
    "these principles, Neuro Prune provides a robust framework\n",
    "for optimizing deep neural networks for deployment on edge\n",
    "devices without sacrificing performance.\n",
    "Neuro Prune reduces model size and computational re-\n",
    "quirements while maintaining or even improving the model’s\n",
    "accuracy. These findings underscore the potential of Neuro\n",
    "Prune to enable real-time, energy-efficient deep learning on\n",
    "resource-constrained devices.\n",
    "Neuro Prune employs a systematic process to optimize\n",
    "neural networks for edge deployment:\n",
    "• Mask Initialization: Each layer’s weights are paired with\n",
    "a binary mask (original_mask), initialized to ones.\n",
    "• Activation Tracking: During forward passes over the\n",
    "dataset, the magnitudes of weight activations are mon-\n",
    "itored and recorded.\n",
    "• Normalize and Prune: The recorded activations are\n",
    "normalized, and a threshold based on the pruning fraction\n",
    "is computed. Using this threshold, the masks are dynam-\n",
    "ically updated to prune unimportant weights.\n",
    "• Evaluate and Log Metrics: After pruning, metrics such\n",
    "as model sparsity and accuracy are computed. A reward\n",
    "metric is derived to balance accuracy and sparsity using\n",
    "a tunable parameter, λweight.\n",
    "• Retrain the Model: The pruned model undergoes retrain-\n",
    "ing to improve the loss in accuracy while maintaining low\n",
    "computational requirements\n",
    "Neuro Prune’s unique combination of reinforcement learn-\n",
    "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
    "pothesis for identifying optimal subnetworks enables it to\n",
    "achieve remarkable efficiency. This approach ensures substan-\n",
    "tial reduction in model size and energy consumption while\n",
    "preserving accuracy and inference speed, making it an ideal\n",
    "solution for edge-based deep learning applications.\n",
    "Through extensive empirical evaluations and theoretical\n",
    "insights, this paper demonstrates how Neuro Pruning outper-\n",
    "forms traditional pruning methods, setting a new benchmark\n",
    "for sustainable, high-performance AI deployment on edge\n",
    "devices. By situating our work within the broader research\n",
    "context, we aim to offer a critical contribution to the ongoing\n",
    "evolution of neural network optimization techniques.\n",
    "II. BACKGROUND\n",
    "A. Need for Pruning Techniques\n",
    "Pruning is a crucial optimization technique for deploying\n",
    "deep neural networks in resource-constrained environments\n",
    "like healthcare. For example, wearable devices like smart-\n",
    "watches that monitor heart rates or portable EEG systems\n",
    "for brain activity analysis require lightweight models due to\n",
    "their limited computational power and memory. Pruning helps\n",
    "address these constraints by reducing the memory footprint\n",
    "and computational load, enabling efficient model deployment.\n",
    "Additionally, it minimizes energy consumption, which is vital\n",
    "for sustainable and real-time healthcare applications. Faster\n",
    "inference enabled by pruning is particularly beneficial for\n",
    "time-sensitive tasks like seizure detection or emergency di-\n",
    "agnostics. Moreover, pruning enhances model interpretability\n",
    "by retaining only the most critical components, which is\n",
    "essential for gaining trust and ensuring regulatory compliance\n",
    "in healthcare. By reducing the hardware and operational costs,\n",
    "pruning also makes deploying advanced AI solutions on edge\n",
    "devices more cost-effective and accessible.\n",
    "B. Types of Pruning Techniques\n",
    "Pruning methods are broadly classified based on granu-\n",
    "larity, timing, and approach. Granularity-based pruning in-\n",
    "cludes structured pruning, which removes entire components\n",
    "such as neurons or filters to streamline computations, and\n",
    "unstructured pruning, which eliminates individual weights\n",
    "for finer optimization, though it often requires specialized\n",
    "hardware. Timing-based pruning involves pre-training pruning,\n",
    "which optimizes models before training; post-training prun-\n",
    "ing, where redundant components are removed from trained\n",
    "models with possible retraining; and dynamic pruning during\n",
    "training, which adjusts models in real-time for improved\n",
    "adaptability. Metric-based pruning uses parameters like weight\n",
    "magnitude or gradient contribution to identify and remove\n",
    "less critical components, while application-specific pruning\n",
    "tailors strategies to specific tasks, such as optimizing for EEG\n",
    "signal analysis, or hardware constraints like GPUs. Together,\n",
    "these methods enable the development of efficient, scalable,\n",
    "and task-specific models, particularly suited for edge device\n",
    "deployment in healthcare and other domains.\n",
    "III. RELATED WORK\n",
    "Jielei Wang et al.[12] introduced an absorption pruning\n",
    "method for object detection in remote sensing imagery. This\n",
    "study achieves efficient compression with minimal accuracy\n",
    "loss but requires careful layer-wise pruning ratio tuning.\n",
    "Jan Muller et al., in their research[12], propose a neural\n",
    "network pruning method for multi-object tracking (MOT) that\n",
    "reduces model size by up to 70\n",
    "Liang Li et al., in their work[9], introduce a novel pruning\n",
    "method for DNNs that utilizes a self-adaptive mechanism\n",
    "based on weight sparsity ratios and a protective reconstruction\n",
    "mechanism. Their approach improves both model compres-\n",
    "sion and accuracy, outperforming state-of-the-art methods on\n",
    "CIFAR-10 and ImageNet datasets.\n",
    "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
    "learning with edge devices like Google Coral AI and Nvidia\n",
    "Jetson Nano. Their work revolutionizes computer vision and\n",
    "real-time tracking but raises energy concerns due to the\n",
    "computational demands of advanced algorithms.\n",
    "Satoru Koda et al.[7] explore how weight pruning in\n",
    "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
    "larly with Mahalanobis-based approaches, by improving global\n",
    "feature extraction and leveraging weights not critical for\n",
    "classification.\n",
    "Shvetha S Kumar et al. analyze and compare three pruning\n",
    "techniques—L1-norm filter pruning, channel pruning, and\n",
    "weight pruning—on CNNs for accuracy and inference time,\n",
    "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
    "pared to GTX1080 Ti in their study[8].\n",
    "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
    "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
    "on stochastic networks, extending it to related problems and\n",
    "demonstrating its efficiency on real-world networks in their\n",
    "study[13].\n",
    "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
    "Training outperforms Post-Training Quantization, and sparsity\n",
    "training with Optimal Reduction pruning improves YOLOv4’s\n",
    "efficiency and edge-device performance, despite potential\n",
    "high-pruning rate drawbacks.\n",
    "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
    "method for DNNs. This approach optimizes inference speed\n",
    "and resource efficiency on edge devices with minimal accuracy\n",
    "loss but requires careful tuning of layer-specific pruning ratios.\n",
    "Yongqi An et al.[1] introduce a retraining-free structured\n",
    "pruning framework for Large Language Models (LLMs). Their\n",
    "method incorporates a fluctuation-based pruning metric, adap-\n",
    "tive structure search, and a bias compensation mechanism to\n",
    "achieve efficient pruning. FLAP significantly enhances infer-\n",
    "ence speed and reduces model size without requiring retrain-\n",
    "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
    "and Wanda-sp on LLaMA models across various benchmarks.\n",
    "From referring to these research papers, we derived an\n",
    "idea for pruning on edge devices. Additionally, our proposed\n",
    "solution is inspired by the research of Jaron Maene et al.[10],\n",
    "which suggests that sparse subnetworks in dense networks can\n",
    "achieve similar accuracy when retrained. This study demon-\n",
    "strates stable training with linear mode connectivity, support-\n",
    "ing the idea that lottery tickets retrain to similar regions, but\n",
    "questions their independence from dense training and iterative\n",
    "pruning.\n",
    "Eran Malach et al.[11] further strengthen the lottery ticket\n",
    "hypothesis by proving that over-parameterized neural networks\n",
    "with random weights always contain a subnetwork matching\n",
    "the accuracy of a target network, without additional training.\n",
    "IV. NEUROLOGICAL PRUNING APPROACH\n",
    "The Neurological Pruning Algorithm is designed to opti-\n",
    "mize neural networks by iteratively pruning less important\n",
    "weights while maintaining high performance. Unlike tradi-\n",
    "tional methods, this approach incorporates adaptive, layer-\n",
    "specific pruning thresholds that dynamically adjust based\n",
    "on weight activations, ensuring efficient model optimization\n",
    "across various stages of training. Drawing inspiration from\n",
    "the Lottery Ticket Hypothesis, which suggests that sparse\n",
    "subnetworks can perform similarly to their dense counterparts\n",
    "when retrained, the algorithm selectively deactivates weights\n",
    "using binary masks to enhance computational efficiency with-\n",
    "out sacrificing accuracy.\n",
    "The algorithm operates in several epochs, during which\n",
    "weight activations are evaluated. Based on a quantile-based\n",
    "strategy, pruning thresholds are established for each layer, en-\n",
    "suring that only weights contributing minimally to the model’s\n",
    "output are removed. After pruning, the model undergoes fine-\n",
    "tuning to restore any lost accuracy and further optimize the\n",
    "network. This step enhances the adaptability and scalability\n",
    "of the pruning method, making it suitable for deployment in\n",
    "resource-constrained environments\n",
    "A. Key Characteristics and Advantages\n",
    "The Neurological Pruning Algorithm boasts several key\n",
    "characteristics that distinguish it from conventional pruning\n",
    "techniques. One notable feature is its layer-specific pruning,\n",
    "which evaluates the importance of weights individually for\n",
    "each layer, leading to a more nuanced and efficient sparsifi-\n",
    "cation process. This approach prevents the model from losing\n",
    "essential features that may otherwise be pruned by a global\n",
    "approach.\n",
    "The algorithm’s use of quantile-based thresholding en-\n",
    "sures that only weights with the least contribution to the\n",
    "model’s output are pruned. This fine-tuned method avoids\n",
    "performance degradation by carefully targeting weights that\n",
    "are less significant to the network’s overall functionality.\n",
    "Furthermore, the pruning process is iterative, allowing for\n",
    "continuous model evaluation and real-time adjustments based\n",
    "on performance feedback, ensuring that accuracy is retained\n",
    "even as sparsity increases.\n",
    "Additionally, the algorithm maintains a careful balance\n",
    "between sparsity and accuracy, enabling a reduction in the\n",
    "model’s computational complexity while preserving its predic-\n",
    "tive capabilities. This characteristic is particularly important\n",
    "for applications requiring rapid inference times, such as in\n",
    "real-time healthcare systems and edge computing scenarios,\n",
    "where both energy efficiency and computational power are\n",
    "crucial.\n",
    "B. Impact on Model Performance\n",
    "The Neurological Pruning Algorithm significantly improves\n",
    "the efficiency of neural networks, making them more suitable\n",
    "for deployment on edge devices. Key performance metrics\n",
    "such as compression ratio (CR) and accuracy retention\n",
    "(AR) are critical indicators of the algorithm’s effectiveness.\n",
    "For instance, in evaluating a ResNet-18 model, the algorithm\n",
    "achieved a CR of 1, indicating that while the number of\n",
    "parameters remained the same, the model’s efficiency im-\n",
    "proved through selective pruning, reducing computational load\n",
    "without sacrificing performance.\n",
    "Moreover, the algorithm demonstrates the ability to achieve\n",
    "high accuracy retention, with models maintaining or even\n",
    "slightly improving their predictive performance post-pruning.\n",
    "This outcome highlights the algorithm’s capability to effi-\n",
    "ciently remove redundant weights while preserving the core\n",
    "functionality of the network. The result is a model that is\n",
    "not only more computationally efficient but also adaptable\n",
    "across different architectures and datasets, making it suit-\n",
    "able for a wide range of applications, from image classification\n",
    "to natural language processing .\n",
    "In practical terms, the Neurological Pruning Algorithm en-\n",
    "ables faster inference times and lower energy consumption,\n",
    "two crucial aspects for deploying AI models in resource-\n",
    "constrained environments. As such, it holds significant poten-\n",
    "tial for improving the viability of real-time, energy-efficient\n",
    "AI applications, particularly in fields like healthcare, where\n",
    "both speed and sustainability are essential.\n",
    "V. NEUROLOGICAL PRUNING ALGORITHM\n",
    "The Neurological Pruning Algorithm is a structured and\n",
    "adaptive methodology designed to optimize neural networks\n",
    "for resource-constrained environments. This algorithm goes\n",
    "beyond traditional pruning approaches by incorporating layer-\n",
    "specific pruning thresholds that dynamically adjust to the\n",
    "importance of weight activations within each layer. Drawing\n",
    "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
    "inforcement learning, the algorithm strikes an optimal balance\n",
    "between model sparsity and accuracy .\n",
    "A. Algorithm Description\n",
    "The Neurological Pruning Algorithm iteratively refines the\n",
    "neural network through a series of training epochs. During\n",
    "each epoch, weight activations are tracked, normalized, and\n",
    "analyzed to identify low-importance weights. These weights\n",
    "are selectively deactivated using binary masks, ensuring com-\n",
    "putational efficiency without compromising the network’s pre-\n",
    "dictive performance. The algorithm further integrates a fine-\n",
    "tuning phase, enabling the pruned network to recover and\n",
    "enhance its accuracy. The detailed process is presented in\n",
    "Algorithm 1.\n",
    "By dynamically adjusting to layer-specific characteristics\n",
    "and maintaining performance metrics, the Neurological Prun-\n",
    "ing Algorithm ensures a significant reduction in computational\n",
    "and memory requirements, paving the way for efficient deploy-\n",
    "ment of deep learning models on edge devices .\n",
    "B. Input Parameters and Output\n",
    "Inputs:\n",
    "• Pretrained model (M)\n",
    "• Dataset (D)\n",
    "• Prune fraction (p)\n",
    "• Total epochs (E)\n",
    "Output:\n",
    "• Pruned and fine-tuned model (Mpruned)\n",
    "C. Key Features and Advantages\n",
    "The Neurological Pruning Algorithm is designed with sev-\n",
    "eral distinctive features that contribute to its effectiveness\n",
    "in optimizing neural network performance. These features\n",
    "include:\n",
    "Algorithm 1 Simplified Neurological Pruning Algorithm\n",
    "Require: Pretrained model M, Dataset D, Prune fraction p,\n",
    "Total epochs E\n",
    "Ensure: Pruned and fine-tuned model Mpruned\n",
    "1: Initialize binary masks M with all ones\n",
    "2: for each epoch e = 1 to E do\n",
    "3:\n",
    "Compute weight activations A during forward passes\n",
    "4:\n",
    "for each layer i in M do\n",
    "5:\n",
    "Compute pruning threshold Ti as p-quantile of Ai\n",
    "6:\n",
    "Update mask Mi: retain weights with Ai > Ti\n",
    "7:\n",
    "Apply mask to layer: θi ←θi · Mi\n",
    "8:\n",
    "end for\n",
    "9:\n",
    "Evaluate model performance and update metrics\n",
    "10: end for\n",
    "11: Fine-tune Mpruned on D\n",
    "12: return Mpruned\n",
    "• Layer-specific Pruning: The algorithm applies pruning\n",
    "on a per-layer basis, evaluating the weight activations\n",
    "within each layer individually. This approach ensures that\n",
    "the sparsity introduced by pruning is well-distributed and\n",
    "context-dependent, facilitating more nuanced control over\n",
    "model optimization .\n",
    "• Quantile-based Thresholding: A quantile-based strat-\n",
    "egy is employed to determine the pruning threshold for\n",
    "each layer. This method ensures that only weights with\n",
    "minimal contribution to the network’s output are pruned,\n",
    "thereby reducing the risk of adversely affecting model\n",
    "accuracy while enhancing sparsity .\n",
    "• Iterative Pruning and Evaluation: The pruning pro-\n",
    "cess follows an iterative approach, enabling continuous\n",
    "assessment of the model’s performance after each pruning\n",
    "step. This ensures dynamic adjustments to maintain a bal-\n",
    "ance between model sparsity and predictive performance\n",
    "throughout training.\n",
    "• Preservation of Model Accuracy: By adjusting pruning\n",
    "thresholds in response to ongoing performance metrics,\n",
    "the algorithm ensures that model accuracy is not com-\n",
    "promised. The careful tuning of these thresholds allows\n",
    "for significant sparsity while maintaining the robustness\n",
    "of the network’s predictive capabilities .\n",
    "D. Impact on Neural Network Performance\n",
    "The Neurological Pruning Algorithm has been extensively\n",
    "evaluated across various benchmark datasets and model archi-\n",
    "tectures, demonstrating its capacity to achieve the following\n",
    "outcomes:\n",
    "• High Sparsity with Minimal Accuracy Loss: The algo-\n",
    "rithm effectively prunes unnecessary weights, achieving\n",
    "high sparsity levels without causing significant degrada-\n",
    "tion in model accuracy. This outcome is critical for the\n",
    "efficient deployment of models in resource-constrained\n",
    "environments.\n",
    "• Reduced Computational Complexity: By sparsifying\n",
    "the weight matrices, the algorithm reduces the compu-\n",
    "tational load during inference, which translates to faster\n",
    "model execution and lower resource consumption. This\n",
    "characteristic is especially beneficial in edge computing\n",
    "and real-time applications .\n",
    "• Broad Applicability: The algorithm is highly flexible\n",
    "and can be adapted to various neural network architec-\n",
    "tures and datasets, making it suitable for a wide range\n",
    "of tasks, from image recognition to natural language\n",
    "processing .\n",
    "VI. EVALUATION METRICS\n",
    "A. Performance Comparison\n",
    "Fig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\n",
    "TABLE I\n",
    "COMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\n",
    "Metric\n",
    "Unpruned\n",
    "Model\n",
    "Pruned\n",
    "Model\n",
    "Accuracy\n",
    "0.9562\n",
    "0.9583\n",
    "Train Loss\n",
    "0.1414\n",
    "0.1625\n",
    "Inference\n",
    "Time\n",
    "(s)\n",
    "0.0318\n",
    "0.0314\n",
    "No of Parame-\n",
    "ters(million)\n",
    "11.7\n",
    "11.7\n",
    "B. Evaluation Metrics Calculation\n",
    "The key metrics used to evaluate the proposed pruning\n",
    "technique are as follows:\n",
    "Compression Ratio (CR): The Compression Ratio (CR) is\n",
    "calculated as:\n",
    "CR = Total Parameters After Pruning\n",
    "Total Parameters Before Pruning\n",
    "(1)\n",
    "CR = 11.7M\n",
    "11.7M = 1\n",
    "Accuracy Retention (AR): The Accuracy Retention (AR) is\n",
    "calculated as:\n",
    "AR =\n",
    "\u0012Accuracy After Pruning −Baseline Accuracy\n",
    "Baseline Accuracy\n",
    "\u0013\n",
    "×100\n",
    "(2)\n",
    "AR =\n",
    "\u00120.958284 −0.956206\n",
    "0.956206\n",
    "\u0013\n",
    "× 100 = 0.22%\n",
    "This result indicates that the pruned model achieves an ac-\n",
    "curacy improvement of 0.22% when compared to unpruned\n",
    "model.\n",
    "C. Illustration of Results\n",
    "Fig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\n",
    "18.\n",
    "D. Discussion\n",
    "The evaluation metrics highlight the effectiveness of the\n",
    "proposed Neurological Pruning method on the ResNet-18\n",
    "model. The Compression Ratio (CR) remains 1, indicating\n",
    "no reduction in the total number of parameters, which suggests\n",
    "that this approach optimizes the neural network by selectively\n",
    "pruning connections rather than reducing the overall parameter\n",
    "count. The Accuracy Retention (AR), which is approximately\n",
    "0.22%, demonstrates that the Neurological Pruning technique\n",
    "successfully improved the accuracy compared to the baseline\n",
    "model.\n",
    "These results validate the potential of Neurological Pruning\n",
    "as an innovative approach to optimizing deep learning models.\n",
    "By maintaining accuracy while likely enhancing sparsity and\n",
    "computational efficiency, this method presents a promising di-\n",
    "rection for achieving high-performance models under resource\n",
    "constraints.\n",
    "VII. CONCLUSION\n",
    "In summary, the Neurological Pruning Algorithm provides\n",
    "a systematic and principled approach to optimizing neural\n",
    "networks. By balancing the trade-off between sparsity and\n",
    "accuracy, it enables significant reductions in computational\n",
    "complexity without sacrificing the network’s predictive per-\n",
    "formance. Future work will focus on extending the algorithm\n",
    "to dynamic network architectures and incorporating advanced\n",
    "regularization techniques to further enhance its robustness and\n",
    "adaptability in diverse applications.\n",
    "REFERENCES\n",
    "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\n",
    "Fluctuation-based Adaptive Structured Pruning for Large Language\n",
    "Models.\n",
    "[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\n",
    "Simplification of Deep Neural Network-Based Object Detector for Real-\n",
    "Time Edge Computing.\n",
    "[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\n",
    "Devices Object Detection by Filter Pruning.\n",
    "[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\n",
    "and Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\n",
    "Tracking Algorithms in Edge Devices.\n",
    "[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\n",
    "Chen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\n",
    "Compression Method for Object Detection Network for Edge Devices.\n",
    "[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\n",
    "Cao. Absorption Pruning of Deep Neural Network for Object Detection\n",
    "in Remote Sensing Imagery.\n",
    "[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\n",
    "and Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\n",
    "Distribution Detection: An Empirical Survey.\n",
    "[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\n",
    "Sahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\n",
    "[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\n",
    "Compress DNNs.\n",
    "[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\n",
    "derstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\n",
    "[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\n",
    "Proving the Lottery Ticket Hypothesis: Pruning is All You Need.\n",
    "[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\n",
    "Devices via Reconstruction-Based Channel Pruning.\n",
    "[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\n",
    "Determine Reliable Paths on Networks with Random and Correlated\n",
    "Link Travel Times.\n",
    "[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\n",
    "and D. Suganthi. Real-Time Object Detection on Edge Devices Using\n",
    "Mobile Neural Networks.\n",
    "[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\n",
    "Tang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\n",
    "Models.\n",
    "[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\n",
    "Li, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\n",
    "Deng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\n",
    "Detection of Tea Leaves with One Bud and Two Leaves Based on\n",
    "ShuffleNetv2-YOLOv5-Lite-E.\n",
    "\"\"\", \"metadata\": {\"source\": \"geo_facts.txt\", \"page\": 1}},\n",
    "    {\"content\": \"The Great Wall of China is located in China and is a famous landmark.\", \"metadata\": {\"source\": \"world_landmarks.txt\", \"page\": 5}},\n",
    "    {\"content\": \"Paris is known for the Eiffel Tower.\", \"metadata\": {\"source\": \"geo_facts.txt\", \"page\": 2}},\n",
    "]\n",
    "\n",
    "# Convert raw data to LangChain Document objects\n",
    "documents = [Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"]) for doc in documents_data]\n",
    "\n",
    "# Create FAISS vector store directly from documents\n",
    "# This handles embedding the texts and building the index internally\n",
    "try:\n",
    "    vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "    print(\"FAISS vector store created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating FAISS vector store: {e}\")\n",
    "# Exit if vector store creation fails\n",
    "\n",
    "# Step 4: Load Gemini model for text generation\n",
    "\n",
    "# Step 5: Create the Retrieval-Augmented Generation pipeline\n",
    "print(\"Creating RAG chain...\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\", # Other options: \"mmr\", \"similarity_score_threshold\"\n",
    "    search_kwargs={'k': 2} # Retrieve top 2 relevant documents\n",
    ")\n",
    "\n",
    "# Use the recommended from_chain_type method\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # Options: \"stuff\", \"map_reduce\", \"refine\", \"map_rerank\"\n",
    "                        # \"stuff\" puts all retrieved docs into the context\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True # Include source documents in the output\n",
    "    # chain_type_kwargs={\"prompt\": YOUR_CUSTOM_PROMPT} # Optional: customize prompt\n",
    ")\n",
    "print(\"RAG chain created.\")\n",
    "\n",
    "query = \"who is surya narayanaa\"\n",
    "print(f\"\\nQuerying the RAG system with: '{query}'\")\n",
    "\n",
    "try:\n",
    "    result = rag_chain.invoke({\"query\": query})\n",
    "    # Output results\n",
    "    print(\"\\n--- RAG System Output ---\")\n",
    "    print(\"Answer:\", result.get(\"result\", \"No answer found.\"))\n",
    "    print(\"\\nSources:\")\n",
    "    if result.get(\"source_documents\"):\n",
    "        for i, doc in enumerate(result[\"source_documents\"]):\n",
    "            print(f\"  Source {i+1}:\")\n",
    "            print(f\"    Content: {doc.page_content}\")\n",
    "            print(f\"    Metadata: {doc.metadata}\")\n",
    "    else:\n",
    "        print(\"  No source documents found or returned.\")\n",
    "    print(\"--- End of Output ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during RAG chain execution: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
