{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ca4a4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "#     os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce1e3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from src.graph_builder._llm import LLMGraphTransformer\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c350687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "text = \"\"\" Neuro Prune: An Adaptive Approach for Efficient\n",
    "Deep Neural Network Optimization on Edge\n",
    "Devices\n",
    "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
    "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
    "grk.cse@psgtech.ac.in\n",
    "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
    "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
    "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
    "Abstract—The optimization of deep neural networks for de-\n",
    "ployment on edge devices is a significant research area due to\n",
    "the demand for applications such as augmented reality, smart\n",
    "cameras, and autonomous navigation. However, deploying large\n",
    "deep learning models on edge devices poses challenges related to\n",
    "computational power, energy consumption, and latency. Pruning\n",
    "is a method to reduce the model size, accelerate inference, and\n",
    "save power. The objective of the paper is to propose the Neuro\n",
    "Prune algorithm and to apply it for the optimization of deep\n",
    "neural networks on edge devices. Efforts have been made to\n",
    "compare pruned and unpruned models. As a result, the pruned\n",
    "model has an accuracy increase of 0.22%.\n",
    "I. INTRODUCTION\n",
    "The rise of intelligent systems, ranging from autonomous\n",
    "drones to augmented reality (AR) devices and smart surveil-\n",
    "lance cameras, has intensified the need for efficient deep neural\n",
    "network deployment on edge devices with constrained com-\n",
    "putational resources. While advanced deep learning models\n",
    "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
    "their deployment on such devices is hindered by significant\n",
    "computational demands, large memory footprints, and high\n",
    "energy consumption. These challenges pose critical barriers to\n",
    "real-time applications and sustainable deployment.\n",
    "This paper builds on existing pruning techniques, includ-\n",
    "ing unstructured pruning, structured pruning, and approaches\n",
    "inspired by quantization, highlighting their strengths and lim-\n",
    "itations. In response to the challenges identified, we introduce\n",
    "Neuro Prune, a novel pruning approach that integrates princi-\n",
    "ples from reinforcement learning and the Lottery Ticket Hy-\n",
    "pothesis to achieve an optimal balance between accuracy, com-\n",
    "putational efficiency, and energy consumption. By integrating\n",
    "these principles, Neuro Prune provides a robust framework\n",
    "for optimizing deep neural networks for deployment on edge\n",
    "devices without sacrificing performance.\n",
    "Neuro Prune reduces model size and computational re-\n",
    "quirements while maintaining or even improving the model’s\n",
    "accuracy. These findings underscore the potential of Neuro\n",
    "Prune to enable real-time, energy-efficient deep learning on\n",
    "resource-constrained devices.\n",
    "Neuro Prune employs a systematic process to optimize\n",
    "neural networks for edge deployment:\n",
    "• Mask Initialization: Each layer’s weights are paired with\n",
    "a binary mask (original_mask), initialized to ones.\n",
    "• Activation Tracking: During forward passes over the\n",
    "dataset, the magnitudes of weight activations are mon-\n",
    "itored and recorded.\n",
    "• Normalize and Prune: The recorded activations are\n",
    "normalized, and a threshold based on the pruning fraction\n",
    "is computed. Using this threshold, the masks are dynam-\n",
    "ically updated to prune unimportant weights.\n",
    "• Evaluate and Log Metrics: After pruning, metrics such\n",
    "as model sparsity and accuracy are computed. A reward\n",
    "metric is derived to balance accuracy and sparsity using\n",
    "a tunable parameter, λweight.\n",
    "• Retrain the Model: The pruned model undergoes retrain-\n",
    "ing to improve the loss in accuracy while maintaining low\n",
    "computational requirements\n",
    "Neuro Prune’s unique combination of reinforcement learn-\n",
    "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
    "pothesis for identifying optimal subnetworks enables it to\n",
    "achieve remarkable efficiency. This approach ensures substan-\n",
    "tial reduction in model size and energy consumption while\n",
    "preserving accuracy and inference speed, making it an ideal\n",
    "solution for edge-based deep learning applications.\n",
    "Through extensive empirical evaluations and theoretical\n",
    "insights, this paper demonstrates how Neuro Pruning outper-\n",
    "forms traditional pruning methods, setting a new benchmark\n",
    "for sustainable, high-performance AI deployment on edge\n",
    "devices. By situating our work within the broader research\n",
    "context, we aim to offer a critical contribution to the ongoing\n",
    "evolution of neural network optimization techniques.\n",
    "II. BACKGROUND\n",
    "A. Need for Pruning Techniques\n",
    "Pruning is a crucial optimization technique for deploying\n",
    "deep neural networks in resource-constrained environments\n",
    "like healthcare. For example, wearable devices like smart-\n",
    "watches that monitor heart rates or portable EEG systems\n",
    "for brain activity analysis require lightweight models due to\n",
    "their limited computational power and memory. Pruning helps\n",
    "address these constraints by reducing the memory footprint\n",
    "and computational load, enabling efficient model deployment.\n",
    "Additionally, it minimizes energy consumption, which is vital\n",
    "for sustainable and real-time healthcare applications. Faster\n",
    "inference enabled by pruning is particularly beneficial for\n",
    "time-sensitive tasks like seizure detection or emergency di-\n",
    "agnostics. Moreover, pruning enhances model interpretability\n",
    "by retaining only the most critical components, which is\n",
    "essential for gaining trust and ensuring regulatory compliance\n",
    "in healthcare. By reducing the hardware and operational costs,\n",
    "pruning also makes deploying advanced AI solutions on edge\n",
    "devices more cost-effective and accessible.\n",
    "B. Types of Pruning Techniques\n",
    "Pruning methods are broadly classified based on granu-\n",
    "larity, timing, and approach. Granularity-based pruning in-\n",
    "cludes structured pruning, which removes entire components\n",
    "such as neurons or filters to streamline computations, and\n",
    "unstructured pruning, which eliminates individual weights\n",
    "for finer optimization, though it often requires specialized\n",
    "hardware. Timing-based pruning involves pre-training pruning,\n",
    "which optimizes models before training; post-training prun-\n",
    "ing, where redundant components are removed from trained\n",
    "models with possible retraining; and dynamic pruning during\n",
    "training, which adjusts models in real-time for improved\n",
    "adaptability. Metric-based pruning uses parameters like weight\n",
    "magnitude or gradient contribution to identify and remove\n",
    "less critical components, while application-specific pruning\n",
    "tailors strategies to specific tasks, such as optimizing for EEG\n",
    "signal analysis, or hardware constraints like GPUs. Together,\n",
    "these methods enable the development of efficient, scalable,\n",
    "and task-specific models, particularly suited for edge device\n",
    "deployment in healthcare and other domains.\n",
    "III. RELATED WORK\n",
    "Jielei Wang et al.[12] introduced an absorption pruning\n",
    "method for object detection in remote sensing imagery. This\n",
    "study achieves efficient compression with minimal accuracy\n",
    "loss but requires careful layer-wise pruning ratio tuning.\n",
    "Jan Muller et al., in their research[12], propose a neural\n",
    "network pruning method for multi-object tracking (MOT) that\n",
    "reduces model size by up to 70\n",
    "Liang Li et al., in their work[9], introduce a novel pruning\n",
    "method for DNNs that utilizes a self-adaptive mechanism\n",
    "based on weight sparsity ratios and a protective reconstruction\n",
    "mechanism. Their approach improves both model compres-\n",
    "sion and accuracy, outperforming state-of-the-art methods on\n",
    "CIFAR-10 and ImageNet datasets.\n",
    "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
    "learning with edge devices like Google Coral AI and Nvidia\n",
    "Jetson Nano. Their work revolutionizes computer vision and\n",
    "real-time tracking but raises energy concerns due to the\n",
    "computational demands of advanced algorithms.\n",
    "Satoru Koda et al.[7] explore how weight pruning in\n",
    "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
    "larly with Mahalanobis-based approaches, by improving global\n",
    "feature extraction and leveraging weights not critical for\n",
    "classification.\n",
    "Shvetha S Kumar et al. analyze and compare three pruning\n",
    "techniques—L1-norm filter pruning, channel pruning, and\n",
    "weight pruning—on CNNs for accuracy and inference time,\n",
    "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
    "pared to GTX1080 Ti in their study[8].\n",
    "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
    "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
    "on stochastic networks, extending it to related problems and\n",
    "demonstrating its efficiency on real-world networks in their\n",
    "study[13].\n",
    "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
    "Training outperforms Post-Training Quantization, and sparsity\n",
    "training with Optimal Reduction pruning improves YOLOv4’s\n",
    "efficiency and edge-device performance, despite potential\n",
    "high-pruning rate drawbacks.\n",
    "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
    "method for DNNs. This approach optimizes inference speed\n",
    "and resource efficiency on edge devices with minimal accuracy\n",
    "loss but requires careful tuning of layer-specific pruning ratios.\n",
    "Yongqi An et al.[1] introduce a retraining-free structured\n",
    "pruning framework for Large Language Models (LLMs). Their\n",
    "method incorporates a fluctuation-based pruning metric, adap-\n",
    "tive structure search, and a bias compensation mechanism to\n",
    "achieve efficient pruning. FLAP significantly enhances infer-\n",
    "ence speed and reduces model size without requiring retrain-\n",
    "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
    "and Wanda-sp on LLaMA models across various benchmarks.\n",
    "From referring to these research papers, we derived an\n",
    "idea for pruning on edge devices. Additionally, our proposed\n",
    "solution is inspired by the research of Jaron Maene et al.[10],\n",
    "which suggests that sparse subnetworks in dense networks can\n",
    "achieve similar accuracy when retrained. This study demon-\n",
    "strates stable training with linear mode connectivity, support-\n",
    "ing the idea that lottery tickets retrain to similar regions, but\n",
    "questions their independence from dense training and iterative\n",
    "pruning.\n",
    "Eran Malach et al.[11] further strengthen the lottery ticket\n",
    "hypothesis by proving that over-parameterized neural networks\n",
    "with random weights always contain a subnetwork matching\n",
    "the accuracy of a target network, without additional training.\n",
    "IV. NEUROLOGICAL PRUNING APPROACH\n",
    "The Neurological Pruning Algorithm is designed to opti-\n",
    "mize neural networks by iteratively pruning less important\n",
    "weights while maintaining high performance. Unlike tradi-\n",
    "tional methods, this approach incorporates adaptive, layer-\n",
    "specific pruning thresholds that dynamically adjust based\n",
    "on weight activations, ensuring efficient model optimization\n",
    "across various stages of training. Drawing inspiration from\n",
    "the Lottery Ticket Hypothesis, which suggests that sparse\n",
    "subnetworks can perform similarly to their dense counterparts\n",
    "when retrained, the algorithm selectively deactivates weights\n",
    "using binary masks to enhance computational efficiency with-\n",
    "out sacrificing accuracy.\n",
    "The algorithm operates in several epochs, during which\n",
    "weight activations are evaluated. Based on a quantile-based\n",
    "strategy, pruning thresholds are established for each layer, en-\n",
    "suring that only weights contributing minimally to the model’s\n",
    "output are removed. After pruning, the model undergoes fine-\n",
    "tuning to restore any lost accuracy and further optimize the\n",
    "network. This step enhances the adaptability and scalability\n",
    "of the pruning method, making it suitable for deployment in\n",
    "resource-constrained environments\n",
    "A. Key Characteristics and Advantages\n",
    "The Neurological Pruning Algorithm boasts several key\n",
    "characteristics that distinguish it from conventional pruning\n",
    "techniques. One notable feature is its layer-specific pruning,\n",
    "which evaluates the importance of weights individually for\n",
    "each layer, leading to a more nuanced and efficient sparsifi-\n",
    "cation process. This approach prevents the model from losing\n",
    "essential features that may otherwise be pruned by a global\n",
    "approach.\n",
    "The algorithm’s use of quantile-based thresholding en-\n",
    "sures that only weights with the least contribution to the\n",
    "model’s output are pruned. This fine-tuned method avoids\n",
    "performance degradation by carefully targeting weights that\n",
    "are less significant to the network’s overall functionality.\n",
    "Furthermore, the pruning process is iterative, allowing for\n",
    "continuous model evaluation and real-time adjustments based\n",
    "on performance feedback, ensuring that accuracy is retained\n",
    "even as sparsity increases.\n",
    "Additionally, the algorithm maintains a careful balance\n",
    "between sparsity and accuracy, enabling a reduction in the\n",
    "model’s computational complexity while preserving its predic-\n",
    "tive capabilities. This characteristic is particularly important\n",
    "for applications requiring rapid inference times, such as in\n",
    "real-time healthcare systems and edge computing scenarios,\n",
    "where both energy efficiency and computational power are\n",
    "crucial.\n",
    "B. Impact on Model Performance\n",
    "The Neurological Pruning Algorithm significantly improves\n",
    "the efficiency of neural networks, making them more suitable\n",
    "for deployment on edge devices. Key performance metrics\n",
    "such as compression ratio (CR) and accuracy retention\n",
    "(AR) are critical indicators of the algorithm’s effectiveness.\n",
    "For instance, in evaluating a ResNet-18 model, the algorithm\n",
    "achieved a CR of 1, indicating that while the number of\n",
    "parameters remained the same, the model’s efficiency im-\n",
    "proved through selective pruning, reducing computational load\n",
    "without sacrificing performance.\n",
    "Moreover, the algorithm demonstrates the ability to achieve\n",
    "high accuracy retention, with models maintaining or even\n",
    "slightly improving their predictive performance post-pruning.\n",
    "This outcome highlights the algorithm’s capability to effi-\n",
    "ciently remove redundant weights while preserving the core\n",
    "functionality of the network. The result is a model that is\n",
    "not only more computationally efficient but also adaptable\n",
    "across different architectures and datasets, making it suit-\n",
    "able for a wide range of applications, from image classification\n",
    "to natural language processing .\n",
    "In practical terms, the Neurological Pruning Algorithm en-\n",
    "ables faster inference times and lower energy consumption,\n",
    "two crucial aspects for deploying AI models in resource-\n",
    "constrained environments. As such, it holds significant poten-\n",
    "tial for improving the viability of real-time, energy-efficient\n",
    "AI applications, particularly in fields like healthcare, where\n",
    "both speed and sustainability are essential.\n",
    "V. NEUROLOGICAL PRUNING ALGORITHM\n",
    "The Neurological Pruning Algorithm is a structured and\n",
    "adaptive methodology designed to optimize neural networks\n",
    "for resource-constrained environments. This algorithm goes\n",
    "beyond traditional pruning approaches by incorporating layer-\n",
    "specific pruning thresholds that dynamically adjust to the\n",
    "importance of weight activations within each layer. Drawing\n",
    "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
    "inforcement learning, the algorithm strikes an optimal balance\n",
    "between model sparsity and accuracy .\n",
    "A. Algorithm Description\n",
    "The Neurological Pruning Algorithm iteratively refines the\n",
    "neural network through a series of training epochs. During\n",
    "each epoch, weight activations are tracked, normalized, and\n",
    "analyzed to identify low-importance weights. These weights\n",
    "are selectively deactivated using binary masks, ensuring com-\n",
    "putational efficiency without compromising the network’s pre-\n",
    "dictive performance. The algorithm further integrates a fine-\n",
    "tuning phase, enabling the pruned network to recover and\n",
    "enhance its accuracy. The detailed process is presented in\n",
    "Algorithm 1.\n",
    "By dynamically adjusting to layer-specific characteristics\n",
    "and maintaining performance metrics, the Neurological Prun-\n",
    "ing Algorithm ensures a significant reduction in computational\n",
    "and memory requirements, paving the way for efficient deploy-\n",
    "ment of deep learning models on edge devices .\n",
    "B. Input Parameters and Output\n",
    "Inputs:\n",
    "• Pretrained model (M)\n",
    "• Dataset (D)\n",
    "• Prune fraction (p)\n",
    "• Total epochs (E)\n",
    "Output:\n",
    "• Pruned and fine-tuned model (Mpruned)\n",
    "C. Key Features and Advantages\n",
    "The Neurological Pruning Algorithm is designed with sev-\n",
    "eral distinctive features that contribute to its effectiveness\n",
    "in optimizing neural network performance. These features\n",
    "include:\n",
    "Algorithm 1 Simplified Neurological Pruning Algorithm\n",
    "Require: Pretrained model M, Dataset D, Prune fraction p,\n",
    "Total epochs E\n",
    "Ensure: Pruned and fine-tuned model Mpruned\n",
    "1: Initialize binary masks M with all ones\n",
    "2: for each epoch e = 1 to E do\n",
    "3:\n",
    "Compute weight activations A during forward passes\n",
    "4:\n",
    "for each layer i in M do\n",
    "5:\n",
    "Compute pruning threshold Ti as p-quantile of Ai\n",
    "6:\n",
    "Update mask Mi: retain weights with Ai > Ti\n",
    "7:\n",
    "Apply mask to layer: θi ←θi · Mi\n",
    "8:\n",
    "end for\n",
    "9:\n",
    "Evaluate model performance and update metrics\n",
    "10: end for\n",
    "11: Fine-tune Mpruned on D\n",
    "12: return Mpruned\n",
    "• Layer-specific Pruning: The algorithm applies pruning\n",
    "on a per-layer basis, evaluating the weight activations\n",
    "within each layer individually. This approach ensures that\n",
    "the sparsity introduced by pruning is well-distributed and\n",
    "context-dependent, facilitating more nuanced control over\n",
    "model optimization .\n",
    "• Quantile-based Thresholding: A quantile-based strat-\n",
    "egy is employed to determine the pruning threshold for\n",
    "each layer. This method ensures that only weights with\n",
    "minimal contribution to the network’s output are pruned,\n",
    "thereby reducing the risk of adversely affecting model\n",
    "accuracy while enhancing sparsity .\n",
    "• Iterative Pruning and Evaluation: The pruning pro-\n",
    "cess follows an iterative approach, enabling continuous\n",
    "assessment of the model’s performance after each pruning\n",
    "step. This ensures dynamic adjustments to maintain a bal-\n",
    "ance between model sparsity and predictive performance\n",
    "throughout training.\n",
    "• Preservation of Model Accuracy: By adjusting pruning\n",
    "thresholds in response to ongoing performance metrics,\n",
    "the algorithm ensures that model accuracy is not com-\n",
    "promised. The careful tuning of these thresholds allows\n",
    "for significant sparsity while maintaining the robustness\n",
    "of the network’s predictive capabilities .\n",
    "D. Impact on Neural Network Performance\n",
    "The Neurological Pruning Algorithm has been extensively\n",
    "evaluated across various benchmark datasets and model archi-\n",
    "tectures, demonstrating its capacity to achieve the following\n",
    "outcomes:\n",
    "• High Sparsity with Minimal Accuracy Loss: The algo-\n",
    "rithm effectively prunes unnecessary weights, achieving\n",
    "high sparsity levels without causing significant degrada-\n",
    "tion in model accuracy. This outcome is critical for the\n",
    "efficient deployment of models in resource-constrained\n",
    "environments.\n",
    "• Reduced Computational Complexity: By sparsifying\n",
    "the weight matrices, the algorithm reduces the compu-\n",
    "tational load during inference, which translates to faster\n",
    "model execution and lower resource consumption. This\n",
    "characteristic is especially beneficial in edge computing\n",
    "and real-time applications .\n",
    "• Broad Applicability: The algorithm is highly flexible\n",
    "and can be adapted to various neural network architec-\n",
    "tures and datasets, making it suitable for a wide range\n",
    "of tasks, from image recognition to natural language\n",
    "processing .\n",
    "VI. EVALUATION METRICS\n",
    "A. Performance Comparison\n",
    "Fig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\n",
    "TABLE I\n",
    "COMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\n",
    "Metric\n",
    "Unpruned\n",
    "Model\n",
    "Pruned\n",
    "Model\n",
    "Accuracy\n",
    "0.9562\n",
    "0.9583\n",
    "Train Loss\n",
    "0.1414\n",
    "0.1625\n",
    "Inference\n",
    "Time\n",
    "(s)\n",
    "0.0318\n",
    "0.0314\n",
    "No of Parame-\n",
    "ters(million)\n",
    "11.7\n",
    "11.7\n",
    "B. Evaluation Metrics Calculation\n",
    "The key metrics used to evaluate the proposed pruning\n",
    "technique are as follows:\n",
    "Compression Ratio (CR): The Compression Ratio (CR) is\n",
    "calculated as:\n",
    "CR = Total Parameters After Pruning\n",
    "Total Parameters Before Pruning\n",
    "(1)\n",
    "CR = 11.7M\n",
    "11.7M = 1\n",
    "Accuracy Retention (AR): The Accuracy Retention (AR) is\n",
    "calculated as:\n",
    "AR =\n",
    "\u0012Accuracy After Pruning −Baseline Accuracy\n",
    "Baseline Accuracy\n",
    "\u0013\n",
    "×100\n",
    "(2)\n",
    "AR =\n",
    "\u00120.958284 −0.956206\n",
    "0.956206\n",
    "\u0013\n",
    "× 100 = 0.22%\n",
    "This result indicates that the pruned model achieves an ac-\n",
    "curacy improvement of 0.22% when compared to unpruned\n",
    "model.\n",
    "C. Illustration of Results\n",
    "Fig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\n",
    "18.\n",
    "D. Discussion\n",
    "The evaluation metrics highlight the effectiveness of the\n",
    "proposed Neurological Pruning method on the ResNet-18\n",
    "model. The Compression Ratio (CR) remains 1, indicating\n",
    "no reduction in the total number of parameters, which suggests\n",
    "that this approach optimizes the neural network by selectively\n",
    "pruning connections rather than reducing the overall parameter\n",
    "count. The Accuracy Retention (AR), which is approximately\n",
    "0.22%, demonstrates that the Neurological Pruning technique\n",
    "successfully improved the accuracy compared to the baseline\n",
    "model.\n",
    "These results validate the potential of Neurological Pruning\n",
    "as an innovative approach to optimizing deep learning models.\n",
    "By maintaining accuracy while likely enhancing sparsity and\n",
    "computational efficiency, this method presents a promising di-\n",
    "rection for achieving high-performance models under resource\n",
    "constraints.\n",
    "VII. CONCLUSION\n",
    "In summary, the Neurological Pruning Algorithm provides\n",
    "a systematic and principled approach to optimizing neural\n",
    "networks. By balancing the trade-off between sparsity and\n",
    "accuracy, it enables significant reductions in computational\n",
    "complexity without sacrificing the network’s predictive per-\n",
    "formance. Future work will focus on extending the algorithm\n",
    "to dynamic network architectures and incorporating advanced\n",
    "regularization techniques to further enhance its robustness and\n",
    "adaptability in diverse applications.\n",
    "REFERENCES\n",
    "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\n",
    "Fluctuation-based Adaptive Structured Pruning for Large Language\n",
    "Models.\n",
    "[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\n",
    "Simplification of Deep Neural Network-Based Object Detector for Real-\n",
    "Time Edge Computing.\n",
    "[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\n",
    "Devices Object Detection by Filter Pruning.\n",
    "[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\n",
    "and Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\n",
    "Tracking Algorithms in Edge Devices.\n",
    "[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\n",
    "Chen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\n",
    "Compression Method for Object Detection Network for Edge Devices.\n",
    "[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\n",
    "Cao. Absorption Pruning of Deep Neural Network for Object Detection\n",
    "in Remote Sensing Imagery.\n",
    "[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\n",
    "and Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\n",
    "Distribution Detection: An Empirical Survey.\n",
    "[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\n",
    "Sahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\n",
    "[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\n",
    "Compress DNNs.\n",
    "[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\n",
    "derstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\n",
    "[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\n",
    "Proving the Lottery Ticket Hypothesis: Pruning is All You Need.\n",
    "[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\n",
    "Devices via Reconstruction-Based Channel Pruning.\n",
    "[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\n",
    "Determine Reliable Paths on Networks with Random and Correlated\n",
    "Link Travel Times.\n",
    "[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\n",
    "and D. Suganthi. Real-Time Object Detection on Edge Devices Using\n",
    "Mobile Neural Networks.\n",
    "[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\n",
    "Tang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\n",
    "Models.\n",
    "[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\n",
    "Li, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\n",
    "Deng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\n",
    "Detection of Tea Leaves with One Bud and Two Leaves Based on\n",
    "ShuffleNetv2-YOLOv5-Lite-E.\n",
    "Neuro Prune: An Adaptive Approach for Efficient\n",
    "Deep Neural Network Optimization on Edge\n",
    "Devices\n",
    "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
    "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
    "grk.cse@psgtech.ac.in\n",
    "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
    "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
    "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
    "Abstract—The optimization of deep neural networks for de-\n",
    "ployment on edge devices is a significant research area due to\n",
    "the demand for applications such as augmented reality, smart\n",
    "cameras, and autonomous navigation. However, deploying large\n",
    "deep learning models on edge devices poses challenges related to\n",
    "computational power, energy consumption, and latency. Pruning\n",
    "is a method to reduce the model size, accelerate inference, and\n",
    "save power. The objective of the paper is to propose the Neuro\n",
    "Prune algorithm and to apply it for the optimization of deep\n",
    "neural networks on edge devices. Efforts have been made to\n",
    "compare pruned and unpruned models. As a result, the pruned\n",
    "model has an accuracy increase of 0.22%.\n",
    "I. INTRODUCTION\n",
    "The rise of intelligent systems, ranging from autonomous\n",
    "drones to augmented reality (AR) devices and smart surveil-\n",
    "lance cameras, has intensified the need for efficient deep neural\n",
    "network deployment on edge devices with constrained com-\n",
    "putational resources. While advanced deep learning models\n",
    "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
    "their deployment on such devices is hindered by significant\n",
    "computational demands, large memory footprints, and high\n",
    "energy consumption. These challenges pose critical barriers to\n",
    "real-time applications and sustainable deployment.\n",
    "This paper builds on existing pruning techniques, includ-\n",
    "ing unstructured pruning, structured pruning, and approaches\n",
    "inspired by quantization, highlighting their strengths and lim-\n",
    "itations. In response to the challenges identified, we introduce\n",
    "Neuro Prune, a novel pruning approach that integrates princi-\n",
    "ples from reinforcement learning and the Lottery Ticket Hy-\n",
    "pothesis to achieve an optimal balance between accuracy, com-\n",
    "putational efficiency, and energy consumption. By integrating\n",
    "these principles, Neuro Prune provides a robust framework\n",
    "for optimizing deep neural networks for deployment on edge\n",
    "devices without sacrificing performance.\n",
    "Neuro Prune reduces model size and computational re-\n",
    "quirements while maintaining or even improving the model’s\n",
    "accuracy. These findings underscore the potential of Neuro\n",
    "Prune to enable real-time, energy-efficient deep learning on\n",
    "resource-constrained devices.\n",
    "Neuro Prune employs a systematic process to optimize\n",
    "neural networks for edge deployment:\n",
    "• Mask Initialization: Each layer’s weights are paired with\n",
    "a binary mask (original_mask), initialized to ones.\n",
    "• Activation Tracking: During forward passes over the\n",
    "dataset, the magnitudes of weight activations are mon-\n",
    "itored and recorded.\n",
    "• Normalize and Prune: The recorded activations are\n",
    "normalized, and a threshold based on the pruning fraction\n",
    "is computed. Using this threshold, the masks are dynam-\n",
    "ically updated to prune unimportant weights.\n",
    "• Evaluate and Log Metrics: After pruning, metrics such\n",
    "as model sparsity and accuracy are computed. A reward\n",
    "metric is derived to balance accuracy and sparsity using\n",
    "a tunable parameter, λweight.\n",
    "• Retrain the Model: The pruned model undergoes retrain-\n",
    "ing to improve the loss in accuracy while maintaining low\n",
    "computational requirements\n",
    "Neuro Prune’s unique combination of reinforcement learn-\n",
    "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
    "pothesis for identifying optimal subnetworks enables it to\n",
    "achieve remarkable efficiency. This approach ensures substan-\n",
    "tial reduction in model size and energy consumption while\n",
    "preserving accuracy and inference speed, making it an ideal\n",
    "solution for edge-based deep learning applications.\n",
    "Through extensive empirical evaluations and theoretical\n",
    "insights, this paper demonstrates how Neuro Pruning outper-\n",
    "forms traditional pruning methods, setting a new benchmark\n",
    "for sustainable, high-performance AI deployment on edge\n",
    "devices. By situating our work within the broader research\n",
    "context, we aim to offer a critical contribution to the ongoing\n",
    "evolution of neural network optimization techniques.\n",
    "II. BACKGROUND\n",
    "A. Need for Pruning Techniques\n",
    "Pruning is a crucial optimization technique for deploying\n",
    "deep neural networks in resource-constrained environments\n",
    "like healthcare. For example, wearable devices like smart-\n",
    "watches that monitor heart rates or portable EEG systems\n",
    "for brain activity analysis require lightweight models due to\n",
    "their limited computational power and memory. Pruning helps\n",
    "address these constraints by reducing the memory footprint\n",
    "and computational load, enabling efficient model deployment.\n",
    "Additionally, it minimizes energy consumption, which is vital\n",
    "for sustainable and real-time healthcare applications. Faster\n",
    "inference enabled by pruning is particularly beneficial for\n",
    "time-sensitive tasks like seizure detection or emergency di-\n",
    "agnostics. Moreover, pruning enhances model interpretability\n",
    "by retaining only the most critical components, which is\n",
    "essential for gaining trust and ensuring regulatory compliance\n",
    "in healthcare. By reducing the hardware and operational costs,\n",
    "pruning also makes deploying advanced AI solutions on edge\n",
    "devices more cost-effective and accessible.\n",
    "B. Types of Pruning Techniques\n",
    "Pruning methods are broadly classified based on granu-\n",
    "larity, timing, and approach. Granularity-based pruning in-\n",
    "cludes structured pruning, which removes entire components\n",
    "such as neurons or filters to streamline computations, and\n",
    "unstructured pruning, which eliminates individual weights\n",
    "for finer optimization, though it often requires specialized\n",
    "hardware. Timing-based pruning involves pre-training pruning,\n",
    "which optimizes models before training; post-training prun-\n",
    "ing, where redundant components are removed from trained\n",
    "models with possible retraining; and dynamic pruning during\n",
    "training, which adjusts models in real-time for improved\n",
    "adaptability. Metric-based pruning uses parameters like weight\n",
    "magnitude or gradient contribution to identify and remove\n",
    "less critical components, while application-specific pruning\n",
    "tailors strategies to specific tasks, such as optimizing for EEG\n",
    "signal analysis, or hardware constraints like GPUs. Together,\n",
    "these methods enable the development of efficient, scalable,\n",
    "and task-specific models, particularly suited for edge device\n",
    "deployment in healthcare and other domains.\n",
    "III. RELATED WORK\n",
    "Jielei Wang et al.[12] introduced an absorption pruning\n",
    "method for object detection in remote sensing imagery. This\n",
    "study achieves efficient compression with minimal accuracy\n",
    "loss but requires careful layer-wise pruning ratio tuning.\n",
    "Jan Muller et al., in their research[12], propose a neural\n",
    "network pruning method for multi-object tracking (MOT) that\n",
    "reduces model size by up to 70\n",
    "Liang Li et al., in their work[9], introduce a novel pruning\n",
    "method for DNNs that utilizes a self-adaptive mechanism\n",
    "based on weight sparsity ratios and a protective reconstruction\n",
    "mechanism. Their approach improves both model compres-\n",
    "sion and accuracy, outperforming state-of-the-art methods on\n",
    "CIFAR-10 and ImageNet datasets.\n",
    "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
    "learning with edge devices like Google Coral AI and Nvidia\n",
    "Jetson Nano. Their work revolutionizes computer vision and\n",
    "real-time tracking but raises energy concerns due to the\n",
    "computational demands of advanced algorithms.\n",
    "Satoru Koda et al.[7] explore how weight pruning in\n",
    "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
    "larly with Mahalanobis-based approaches, by improving global\n",
    "feature extraction and leveraging weights not critical for\n",
    "classification.\n",
    "Shvetha S Kumar et al. analyze and compare three pruning\n",
    "techniques—L1-norm filter pruning, channel pruning, and\n",
    "weight pruning—on CNNs for accuracy and inference time,\n",
    "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
    "pared to GTX1080 Ti in their study[8].\n",
    "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
    "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
    "on stochastic networks, extending it to related problems and\n",
    "demonstrating its efficiency on real-world networks in their\n",
    "study[13].\n",
    "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
    "Training outperforms Post-Training Quantization, and sparsity\n",
    "training with Optimal Reduction pruning improves YOLOv4’s\n",
    "efficiency and edge-device performance, despite potential\n",
    "high-pruning rate drawbacks.\n",
    "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
    "method for DNNs. This approach optimizes inference speed\n",
    "and resource efficiency on edge devices with minimal accuracy\n",
    "loss but requires careful tuning of layer-specific pruning ratios.\n",
    "Yongqi An et al.[1] introduce a retraining-free structured\n",
    "pruning framework for Large Language Models (LLMs). Their\n",
    "method incorporates a fluctuation-based pruning metric, adap-\n",
    "tive structure search, and a bias compensation mechanism to\n",
    "achieve efficient pruning. FLAP significantly enhances infer-\n",
    "ence speed and reduces model size without requiring retrain-\n",
    "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
    "and Wanda-sp on LLaMA models across various benchmarks.\n",
    "From referring to these research papers, we derived an\n",
    "idea for pruning on edge devices. Additionally, our proposed\n",
    "solution is inspired by the research of Jaron Maene et al.[10],\n",
    "which suggests that sparse subnetworks in dense networks can\n",
    "achieve similar accuracy when retrained. This study demon-\n",
    "strates stable training with linear mode connectivity, support-\n",
    "ing the idea that lottery tickets retrain to similar regions, but\n",
    "questions their independence from dense training and iterative\n",
    "pruning.\n",
    "Eran Malach et al.[11] further strengthen the lottery ticket\n",
    "hypothesis by proving that over-parameterized neural networks\n",
    "with random weights always contain a subnetwork matching\n",
    "the accuracy of a target network, without additional training.\n",
    "IV. NEUROLOGICAL PRUNING APPROACH\n",
    "The Neurological Pruning Algorithm is designed to opti-\n",
    "mize neural networks by iteratively pruning less important\n",
    "weights while maintaining high performance. Unlike tradi-\n",
    "tional methods, this approach incorporates adaptive, layer-\n",
    "specific pruning thresholds that dynamically adjust based\n",
    "on weight activations, ensuring efficient model optimization\n",
    "across various stages of training. Drawing inspiration from\n",
    "the Lottery Ticket Hypothesis, which suggests that sparse\n",
    "subnetworks can perform similarly to their dense counterparts\n",
    "when retrained, the algorithm selectively deactivates weights\n",
    "using binary masks to enhance computational efficiency with-\n",
    "out sacrificing accuracy.\n",
    "The algorithm operates in several epochs, during which\n",
    "weight activations are evaluated. Based on a quantile-based\n",
    "strategy, pruning thresholds are established for each layer, en-\n",
    "suring that only weights contributing minimally to the model’s\n",
    "output are removed. After pruning, the model undergoes fine-\n",
    "tuning to restore any lost accuracy and further optimize the\n",
    "network. This step enhances the adaptability and scalability\n",
    "of the pruning method, making it suitable for deployment in\n",
    "resource-constrained environments\n",
    "A. Key Characteristics and Advantages\n",
    "The Neurological Pruning Algorithm boasts several key\n",
    "characteristics that distinguish it from conventional pruning\n",
    "techniques. One notable feature is its layer-specific pruning,\n",
    "which evaluates the importance of weights individually for\n",
    "each layer, leading to a more nuanced and efficient sparsifi-\n",
    "cation process. This approach prevents the model from losing\n",
    "essential features that may otherwise be pruned by a global\n",
    "approach.\n",
    "The algorithm’s use of quantile-based thresholding en-\n",
    "sures that only weights with the least contribution to the\n",
    "model’s output are pruned. This fine-tuned method avoids\n",
    "performance degradation by carefully targeting weights that\n",
    "are less significant to the network’s overall functionality.\n",
    "Furthermore, the pruning process is iterative, allowing for\n",
    "continuous model evaluation and real-time adjustments based\n",
    "on performance feedback, ensuring that accuracy is retained\n",
    "even as sparsity increases.\n",
    "Additionally, the algorithm maintains a careful balance\n",
    "between sparsity and accuracy, enabling a reduction in the\n",
    "model’s computational complexity while preserving its predic-\n",
    "tive capabilities. This characteristic is particularly important\n",
    "for applications requiring rapid inference times, such as in\n",
    "real-time healthcare systems and edge computing scenarios,\n",
    "where both energy efficiency and computational power are\n",
    "crucial.\n",
    "B. Impact on Model Performance\n",
    "The Neurological Pruning Algorithm significantly improves\n",
    "the efficiency of neural networks, making them more suitable\n",
    "for deployment on edge devices. Key performance metrics\n",
    "such as compression ratio (CR) and accuracy retention\n",
    "(AR) are critical indicators of the algorithm’s effectiveness.\n",
    "For instance, in evaluating a ResNet-18 model, the algorithm\n",
    "achieved a CR of 1, indicating that while the number of\n",
    "parameters remained the same, the model’s efficiency im-\n",
    "proved through selective pruning, reducing computational load\n",
    "without sacrificing performance.\n",
    "Moreover, the algorithm demonstrates the ability to achieve\n",
    "high accuracy retention, with models maintaining or even\n",
    "slightly improving their predictive performance post-pruning.\n",
    "This outcome highlights the algorithm’s capability to effi-\n",
    "ciently remove redundant weights while preserving the core\n",
    "functionality of the network. The result is a model that is\n",
    "not only more computationally efficient but also adaptable\n",
    "across different architectures and datasets, making it suit-\n",
    "able for a wide range of applications, from image classification\n",
    "to natural language processing .\n",
    "In practical terms, the Neurological Pruning Algorithm en-\n",
    "ables faster inference times and lower energy consumption,\n",
    "two crucial aspects for deploying AI models in resource-\n",
    "constrained environments. As such, it holds significant poten-\n",
    "tial for improving the viability of real-time, energy-efficient\n",
    "AI applications, particularly in fields like healthcare, where\n",
    "both speed and sustainability are essential.\n",
    "V. NEUROLOGICAL PRUNING ALGORITHM\n",
    "The Neurological Pruning Algorithm is a structured and\n",
    "adaptive methodology designed to optimize neural networks\n",
    "for resource-constrained environments. This algorithm goes\n",
    "beyond traditional pruning approaches by incorporating layer-\n",
    "specific pruning thresholds that dynamically adjust to the\n",
    "importance of weight activations within each layer. Drawing\n",
    "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
    "inforcement learning, the algorithm strikes an optimal balance\n",
    "between model sparsity and accuracy .\n",
    "A. Algorithm Description\n",
    "The Neurological Pruning Algorithm iteratively refines the\n",
    "neural network through a series of training epochs. During\n",
    "each epoch, weight activations are tracked, normalized, and\n",
    "analyzed to identify low-importance weights. These weights\n",
    "are selectively deactivated using binary masks, ensuring com-\n",
    "putational efficiency without compromising the network’s pre-\n",
    "dictive performance. The algorithm further integrates a fine-\n",
    "tuning phase, enabling the pruned network to recover and\n",
    "enhance its accuracy. The detailed process is presented in\n",
    "Algorithm 1.\n",
    "By dynamically adjusting to layer-specific characteristics\n",
    "and maintaining performance metrics, the Neurological Prun-\n",
    "ing Algorithm ensures a significant reduction in computational\n",
    "and memory requirements, paving the way for efficient deploy-\n",
    "ment of deep learning models on edge devices .\n",
    "B. Input Parameters and Output\n",
    "Inputs:\n",
    "• Pretrained model (M)\n",
    "• Dataset (D)\n",
    "• Prune fraction (p)\n",
    "• Total epochs (E)\n",
    "Output:\n",
    "• Pruned and fine-tuned model (Mpruned)\n",
    "C. Key Features and Advantages\n",
    "The Neurological Pruning Algorithm is designed with sev-\n",
    "eral distinctive features that contribute to its effectiveness\n",
    "in optimizing neural network performance. These features\n",
    "include:\n",
    "Algorithm 1 Simplified Neurological Pruning Algorithm\n",
    "Require: Pretrained model M, Dataset D, Prune fraction p,\n",
    "Total epochs E\n",
    "Ensure: Pruned and fine-tuned model Mpruned\n",
    "1: Initialize binary masks M with all ones\n",
    "2: for each epoch e = 1 to E do\n",
    "3:\n",
    "Compute weight activations A during forward passes\n",
    "4:\n",
    "for each layer i in M do\n",
    "5:\n",
    "Compute pruning threshold Ti as p-quantile of Ai\n",
    "6:\n",
    "Update mask Mi: retain weights with Ai > Ti\n",
    "7:\n",
    "Apply mask to layer: θi ←θi · Mi\n",
    "8:\n",
    "end for\n",
    "9:\n",
    "Evaluate model performance and update metrics\n",
    "10: end for\n",
    "11: Fine-tune Mpruned on D\n",
    "12: return Mpruned\n",
    "• Layer-specific Pruning: The algorithm applies pruning\n",
    "on a per-layer basis, evaluating the weight activations\n",
    "within each layer individually. This approach ensures that\n",
    "the sparsity introduced by pruning is well-distributed and\n",
    "context-dependent, facilitating more nuanced control over\n",
    "model optimization .\n",
    "• Quantile-based Thresholding: A quantile-based strat-\n",
    "egy is employed to determine the pruning threshold for\n",
    "each layer. This method ensures that only weights with\n",
    "minimal contribution to the network’s output are pruned,\n",
    "thereby reducing the risk of adversely affecting model\n",
    "accuracy while enhancing sparsity .\n",
    "• Iterative Pruning and Evaluation: The pruning pro-\n",
    "cess follows an iterative approach, enabling continuous\n",
    "assessment of the model’s performance after each pruning\n",
    "step. This ensures dynamic adjustments to maintain a bal-\n",
    "ance between model sparsity and predictive performance\n",
    "throughout training.\n",
    "• Preservation of Model Accuracy: By adjusting pruning\n",
    "thresholds in response to ongoing performance metrics,\n",
    "the algorithm ensures that model accuracy is not com-\n",
    "promised. The careful tuning of these thresholds allows\n",
    "for significant sparsity while maintaining the robustness\n",
    "of the network’s predictive capabilities .\n",
    "D. Impact on Neural Network Performance\n",
    "The Neurological Pruning Algorithm has been extensively\n",
    "evaluated across various benchmark datasets and model archi-\n",
    "tectures, demonstrating its capacity to achieve the following\n",
    "outcomes:\n",
    "• High Sparsity with Minimal Accuracy Loss: The algo-\n",
    "rithm effectively prunes unnecessary weights, achieving\n",
    "high sparsity levels without causing significant degrada-\n",
    "tion in model accuracy. This outcome is critical for the\n",
    "efficient deployment of models in resource-constrained\n",
    "environments.\n",
    "• Reduced Computational Complexity: By sparsifying\n",
    "the weight matrices, the algorithm reduces the compu-\n",
    "tational load during inference, which translates to faster\n",
    "model execution and lower resource consumption. This\n",
    "characteristic is especially beneficial in edge computing\n",
    "and real-time applications .\n",
    "• Broad Applicability: The algorithm is highly flexible\n",
    "and can be adapted to various neural network architec-\n",
    "tures and datasets, making it suitable for a wide range\n",
    "of tasks, from image recognition to natural language\n",
    "processing .\n",
    "VI. EVALUATION METRICS\n",
    "A. Performance Comparison\n",
    "Fig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\n",
    "TABLE I\n",
    "COMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\n",
    "Metric\n",
    "Unpruned\n",
    "Model\n",
    "Pruned\n",
    "Model\n",
    "Accuracy\n",
    "0.9562\n",
    "0.9583\n",
    "Train Loss\n",
    "0.1414\n",
    "0.1625\n",
    "Inference\n",
    "Time\n",
    "(s)\n",
    "0.0318\n",
    "0.0314\n",
    "No of Parame-\n",
    "ters(million)\n",
    "11.7\n",
    "11.7\n",
    "B. Evaluation Metrics Calculation\n",
    "The key metrics used to evaluate the proposed pruning\n",
    "technique are as follows:\n",
    "Compression Ratio (CR): The Compression Ratio (CR) is\n",
    "calculated as:\n",
    "CR = Total Parameters After Pruning\n",
    "Total Parameters Before Pruning\n",
    "(1)\n",
    "CR = 11.7M\n",
    "11.7M = 1\n",
    "Accuracy Retention (AR): The Accuracy Retention (AR) is\n",
    "calculated as:\n",
    "AR =\n",
    "\u0012Accuracy After Pruning −Baseline Accuracy\n",
    "Baseline Accuracy\n",
    "\u0013\n",
    "×100\n",
    "(2)\n",
    "AR =\n",
    "\u00120.958284 −0.956206\n",
    "0.956206\n",
    "\u0013\n",
    "× 100 = 0.22%\n",
    "This result indicates that the pruned model achieves an ac-\n",
    "curacy improvement of 0.22% when compared to unpruned\n",
    "model.\n",
    "C. Illustration of Results\n",
    "Fig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\n",
    "18.\n",
    "D. Discussion\n",
    "The evaluation metrics highlight the effectiveness of the\n",
    "proposed Neurological Pruning method on the ResNet-18\n",
    "model. The Compression Ratio (CR) remains 1, indicating\n",
    "no reduction in the total number of parameters, which suggests\n",
    "that this approach optimizes the neural network by selectively\n",
    "pruning connections rather than reducing the overall parameter\n",
    "count. The Accuracy Retention (AR), which is approximately\n",
    "0.22%, demonstrates that the Neurological Pruning technique\n",
    "successfully improved the accuracy compared to the baseline\n",
    "model.\n",
    "These results validate the potential of Neurological Pruning\n",
    "as an innovative approach to optimizing deep learning models.\n",
    "By maintaining accuracy while likely enhancing sparsity and\n",
    "computational efficiency, this method presents a promising di-\n",
    "rection for achieving high-performance models under resource\n",
    "constraints.\n",
    "VII. CONCLUSION\n",
    "In summary, the Neurological Pruning Algorithm provides\n",
    "a systematic and principled approach to optimizing neural\n",
    "networks. By balancing the trade-off between sparsity and\n",
    "accuracy, it enables significant reductions in computational\n",
    "complexity without sacrificing the network’s predictive per-\n",
    "formance. Future work will focus on extending the algorithm\n",
    "to dynamic network architectures and incorporating advanced\n",
    "regularization techniques to further enhance its robustness and\n",
    "adaptability in diverse applications.\n",
    "REFERENCES\n",
    "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\n",
    "Fluctuation-based Adaptive Structured Pruning for Large Language\n",
    "Models.\n",
    "[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\n",
    "Simplification of Deep Neural Network-Based Object Detector for Real-\n",
    "Time Edge Computing.\n",
    "[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\n",
    "Devices Object Detection by Filter Pruning.\n",
    "[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\n",
    "and Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\n",
    "Tracking Algorithms in Edge Devices.\n",
    "[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\n",
    "Chen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\n",
    "Compression Method for Object Detection Network for Edge Devices.\n",
    "[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\n",
    "Cao. Absorption Pruning of Deep Neural Network for Object Detection\n",
    "in Remote Sensing Imagery.\n",
    "[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\n",
    "and Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\n",
    "Distribution Detection: An Empirical Survey.\n",
    "[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\n",
    "Sahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\n",
    "[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\n",
    "Compress DNNs.\n",
    "[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\n",
    "derstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\n",
    "[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\n",
    "Proving the Lottery Ticket Hypothesis: Pruning is All You Need.\n",
    "[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\n",
    "Devices via Reconstruction-Based Channel Pruning.\n",
    "[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\n",
    "Determine Reliable Paths on Networks with Random and Correlated\n",
    "Link Travel Times.\n",
    "[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\n",
    "and D. Suganthi. Real-Time Object Detection on Edge Devices Using\n",
    "Mobile Neural Networks.\n",
    "[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\n",
    "Tang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\n",
    "Models.\n",
    "[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\n",
    "Li, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\n",
    "Deng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\n",
    "Detection of Tea Leaves with One Bud and Two Leaves Based on\n",
    "ShuffleNetv2-YOLOv5-Lite-E.\n",
    "VitaLink - Blood Thinner Tracking App for\n",
    "Hospitals\n",
    "User Manual\n",
    "Welcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\n",
    "Technology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\n",
    "efficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \n",
    "values. Follow the steps below to navigate the app and utilize its features effectively.\n",
    "Doctor Portal\n",
    "Welcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\n",
    "Technology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\n",
    "efficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \n",
    "values. Follow the steps below to navigate the app and utilize its features effectively.\n",
    "1. Logging In \n",
    "To access VitaLink, you need to log in with your credentials.\n",
    "Steps:\n",
    "Open the App: Launch the VitaLink app on your device.\n",
    "1.\n",
    "Enter Credentials:On the login screen, enter your username and password in the respective fields.\n",
    "2.\n",
    "Username: Enter your assigned username.\n",
    "Password: Enter your secure password.\n",
    "Sign In: Tap the red Sign In button to log in.\n",
    "3.\n",
    "2. Viewing Patient Lists\n",
    "Once logged in, you can view a list of patients under your care in either Card View or Table View.\n",
    "Card View:\n",
    " Navigate: After logging in, you’ll see the View Patient Page. \n",
    "1.\n",
    "Patient Cards: Each patient is displayed as a card with their name, age, and gender. \n",
    "2.\n",
    "Show Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\n",
    "3.\n",
    "Switch Views: Tap the Table View button at the top to switch to a tabular format.\n",
    "4.\n",
    "Table View:\n",
    "Table Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\n",
    "1.\n",
    "Filter and Search: Use the Entries per page dropdown to adjust the number of patients\n",
    "shown, and the Search bar to find a specific patient.\n",
    "2.\n",
    "Pagination: Use the navigation arrows at the bottom to browse through pages.\n",
    "3.\n",
    "Switch Views: Tap the Cards View button to return to the card layout.\n",
    "4.\n",
    "3. Viewing Patient Details\n",
    "Access detailed information about a specific patient, including their INR, therapy, and missed\n",
    "doses.\n",
    "Steps:\n",
    "Select a Patient: From the patient list (Card or Table View), tap Show Options on a patient\n",
    "card, or click the patient’s name in Table View.\n",
    "1.\n",
    "Patient Details: The patient’s page displays:\n",
    "2.\n",
    "Basic Info: Name, age, gender, target INR, and latest INR.\n",
    "Assigned Roles: Doctor and caregiver names.\n",
    "Therapy Details: Therapy type and start date.\n",
    "Missed Doses: Dates when doses were missed.\n",
    "Prescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\n",
    "Navigation: Tap Back to return to the patient list.\n",
    "3.\n",
    "4. Viewing Reports\n",
    "Monitor patient INR reports and critical values through the View Report Page.\n",
    "Steps:\n",
    "access Reports: From the main menu, tap View Report Page.\n",
    "1.\n",
    "Tabs:\n",
    "2.\n",
    "Today’s Reports: Shows reports for the current day.\n",
    "All Reports: Displays all historical reports.\n",
    "Report Details: Each report includes:\n",
    "3.\n",
    "Patient name and location.\n",
    "a.\n",
    "Test type (e.g., INR Report).\n",
    "b.\n",
    "INR value (highlighted as Critical if outside safe range).\n",
    "c.\n",
    "Date of the report.\n",
    "d.\n",
    "Document format (e.g., APP.pdf).\n",
    "e.\n",
    "View Full Report: Tap View Full Report to access the complete document.\n",
    "4.\n",
    "5. Adding a New Patient\n",
    "Add new patients to the system to begin tracking their blood thinner therapy.\n",
    "Steps:\n",
    "Navigate: From the main menu, tap Add Patient Page.\n",
    "1.\n",
    "Enter Details: Fill in the following fields:\n",
    "2.\n",
    "Name: Enter the patient’s full name.\n",
    "Age: Enter the patient’s age.\n",
    "Gender: Select the patient’s gender from the dropdown (e.g., M, F).\n",
    "Target INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\n",
    "Caregiver: Assign a caregiver (default: Not Assigned).\n",
    "Therapy: Select the therapy type (e.g., Acitrom).\n",
    "Therapy Start Date: Enter the start date (format: dd-mm-yyyy).\n",
    "Medical History:\n",
    "Diagnosis: Enter the patient’s diagnosis.\n",
    "Duration and Unit: Specify the duration of the condition (e.g., 5 Days).\n",
    "Tap + Add Medical History to include additional entries.\n",
    "Prescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\n",
    "Contact Information:\n",
    "Contact: Enter the patient’s contact number.\n",
    "Kin Name: Enter the name of the patient’s next of kin.\n",
    "Kin Contact: Enter the kin’s contact number.\n",
    "Save: Tap the Add Patient button to save the record.\n",
    "3.\n",
    "Patient Portal\n",
    "1.LOGIN\n",
    "Getting Started\n",
    "Steps to Access Your Account:\n",
    "1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\n",
    "2. Password: Input your secure password.\n",
    "3. Sign In: Click the Sign In button.\n",
    "Notes:\n",
    "1. Phone Number is Password (contact admin for reset).\n",
    "2.Ensure your device has internet connectivity.\n",
    "2.PROFILE OVERVIEW\n",
    "Your profile is the dashboard for critical health data:\n",
    "Personal Details:\n",
    "1.\n",
    "Name, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\n",
    "INR Targets:\n",
    "2.\n",
    "Your ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\n",
    "Latest INR Value:\n",
    "3.\n",
    "Displays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\n",
    "Medical Team:\n",
    "4.\n",
    "Lists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\n",
    "24/01/2025).\n",
    "   5.Medical History:\n",
    "Chronic conditions and durations (e.g., RHD-Post MVR, 27 years).\n",
    "   6.Missed Doses:\n",
    "Dates of skipped doses (e.g., 31-03-2025). Address these promptly.\n",
    "   Tip: Review this page weekly to stay aligned with your care plan.\n",
    "3.Updating INR Values\n",
    "INR (International Normalized Ratio) measures blood clotting time. Regular updates\n",
    "ensure your dosage is safe.\n",
    "1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\n",
    "2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\n",
    "3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\n",
    "4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\n",
    "5. Submit: Click Submit INR Report to save.\n",
    "Important:\n",
    "- Inconsistent INR values trigger alerts to your care team.\n",
    "- Update within 24 hours of receiving results.\n",
    "4.Take Dosage Page\n",
    "1. Missed Doses (Last 7 Days):\n",
    "- View recent missed doses (e.g., 23-04-2025).\n",
    "- Mark as Taken: Click a date to update its status.\n",
    "2. Older Missed Doses:\n",
    "- Scroll through historical entries (e.g., 01-02-2025).\n",
    "- Use Previous and Next buttons to navigate pages.\n",
    "Why This Matters:\n",
    "- Missing doses increases clotting risks.\n",
    "- Accurate logs help doctors adjust prescriptions.\n",
    "Tip: Sync this page with your calendar for reminders.\n",
    "5.Lifestyle Changes\n",
    "Report Changes Affecting Therapy:\n",
    "1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\n",
    "2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\n",
    "supplements).\n",
    "3. Submit: Click to notify your care team.\n",
    "Note: Alcohol consumption or new supplements can alter INR—always report\n",
    "these.\n",
    "Other Medications\n",
    "6.Other Medications Page\n",
    "Track Non-Blood-Thinners:\n",
    "1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\n",
    "daily).\n",
    "2. Submit: Save to your profile.\n",
    "Tip: Update this page after every pharmacy visit\n",
    "7.Prolonged Illness \n",
    "Reporting Health Changes\n",
    "steps:\n",
    "1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\n",
    "2. Medications: List prescriptions for the illness (e.g., Antibiotics).\n",
    "3. Submit: Alert your doctor for follow-up.\n",
    "Note: Illnesses like infections can impact INR stability.\n",
    "8.Side Effects\n",
    "Report Adverse Reactions:\n",
    "steps:\n",
    "Report Adverse Reactions:\n",
    "1. Check Symptoms: Select all that apply (e.g., Bruising without\n",
    "Injury).\n",
    "2. Describe Other Effects: Use the text box for details (e.g., Rash on\n",
    "arms).\n",
    "3. Submit: Click to send an urgent alert.\n",
    "Emergency Symptoms:\n",
    "- Seek immediate care for:\n",
    "- Vomiting Blood\n",
    "- Black Stool\n",
    "- Vision Changes\n",
    "Design and build  \n",
    "accessible PDF tables  \n",
    "Sample tables \n",
    "Table 1 \n",
    "Column header (TH) \n",
    "Column header (TH) \n",
    "Column header (TH) \n",
    "Row header (TH) \n",
    "Data cell (TD) \n",
    "Data cell (TD) \n",
    "Row header(TH) \n",
    "Data cell (TD) \n",
    "Data cell (TD) \n",
    "Table 2: example of footnotes referenced from within a table \n",
    "Expenditure by function £ million \n",
    "2009/10 \n",
    "2010/11 1 \n",
    "Policy functions \n",
    "Financial \n",
    "22.5 \n",
    "30.57 \n",
    "Information 2 \n",
    "10.2 \n",
    "14.8 \n",
    "Contingency \n",
    "2.6 \n",
    "1.2 \n",
    "Remunerated functions \n",
    "Agency services 3 \n",
    "44.7 \n",
    "35.91 \n",
    "Payments \n",
    "22.41 \n",
    "19.88 \n",
    "Banking \n",
    "22.90 \n",
    "44.23 \n",
    "Other \n",
    "12.69 \n",
    "10.32 \n",
    " \n",
    "(1) Provisional total as of publication date. \n",
    "(2) Costs associated with on-going information programmes. \n",
    "(3) From the management accounts, net of recoveries, including interest charges. \n",
    "Table 3: \"film credits\" style layout \n",
    "Main character Daniel Radcliffe \n",
    "Sidekick 1 Rupert Grint \n",
    "Sidekick 2 Emma Watson \n",
    "Lovable ogre Robbie Coltrane \n",
    "Professor Maggie Smith \n",
    "Headmaster Richard Harris \n",
    " \n",
    "Table 4: table 3 with column headers added \n",
    " RoleActor\n",
    "Main character Daniel Radcliffe \n",
    "Sidekick 1 Rupert Grint \n",
    "Sidekick 2 Emma Watson \n",
    "Lovable ogre Robbie Coltrane \n",
    "Professor Maggie Smith \n",
    "Headmaster Richard Harris \n",
    " \n",
    "Table 5: year-end financial statement (£, thousands) \n",
    " \n",
    "2010 \n",
    "2009 \n",
    "2008 \n",
    "Non-current assets \n",
    "Property \n",
    "345 \n",
    "445 \n",
    "222 \n",
    "Investment \n",
    "567 \n",
    "654 \n",
    "423 \n",
    "Intangibles \n",
    "423 \n",
    "123 \n",
    "453 \n",
    "Current assets \n",
    "Trade and other receivables \n",
    "435 \n",
    "634 \n",
    "231 \n",
    "Cash and cash equivalents \n",
    "524 \n",
    "123 \n",
    "482 \n",
    "Other \n",
    "223 \n",
    "211 \n",
    "254 \n",
    " \n",
    "Table 6: a table with a more serious headings problem \n",
    "Rainfall \n",
    "(inches) \n",
    "Americas \n",
    "Asia \n",
    "Europe \n",
    "Africa \n",
    "2010 \n",
    "Average \n",
    "104 \n",
    "201 \n",
    "193 \n",
    "144 \n",
    "24 hour high \n",
    "15 \n",
    "26 \n",
    "27 \n",
    "18 \n",
    "12 hour high \n",
    "9 \n",
    "10 \n",
    "11 \n",
    "12 \n",
    "2009 \n",
    "Average \n",
    "133 \n",
    "244 \n",
    "155 \n",
    "166 \n",
    "24 hour high \n",
    "27 \n",
    "28 \n",
    "29 \n",
    "20 \n",
    "12 hour high \n",
    "11 \n",
    "12 \n",
    "13 \n",
    "16 \n",
    "Table 7: year-end statement, non-current assets (£, thousands) \n",
    "Non-current assets \n",
    "2010 \n",
    "2009 \n",
    "2008 \n",
    "Property \n",
    "345 \n",
    "445 \n",
    "222 \n",
    "Investment \n",
    "567 \n",
    "654 \n",
    "423 \n",
    "Intangibles \n",
    "423 \n",
    "123 \n",
    "453 \n",
    "Table 8: year-end statement, current assets (£, thousands) \n",
    "Current assets \n",
    "2010 \n",
    "2009 \n",
    "2008 \n",
    "Trade and other receivables \n",
    "435 \n",
    "634 \n",
    "231 \n",
    "Cash and cash equivalents \n",
    "524 \n",
    "123 \n",
    "482 \n",
    "Other \n",
    "223 \n",
    "211 \n",
    "254 \n",
    "Table 9: rainfall by continent, 2009 \n",
    "Rainfall (inches) \n",
    "Americas \n",
    "Asia \n",
    "Europe \n",
    "Africa \n",
    "Average \n",
    "133 \n",
    "244 \n",
    "155 \n",
    "166 \n",
    "24 hour high \n",
    "27 \n",
    "28 \n",
    "29 \n",
    "20 \n",
    "12 hour high \n",
    "11 \n",
    "12 \n",
    "13 \n",
    "16 \n",
    " \n",
    "Table 10: self-contained year-end statement (£, thousands) (multiple \n",
    "layout problems) \n",
    " \n",
    "2011 \n",
    "2010 restated \n",
    "General income \n",
    " \n",
    "250,000 \n",
    " \n",
    "200,000 \n",
    "Increase in value, WIP \n",
    " \n",
    "15,000 \n",
    " \n",
    "30,000 \n",
    " \n",
    " \n",
    "265,000 \n",
    " \n",
    "230,000 \n",
    "Administrative costs \n",
    "Staff costs \n",
    "(200,000) \n",
    " \n",
    "(150,000) \n",
    " \n",
    "Early departures \n",
    "(10,000) \n",
    " \n",
    "(20,000) \n",
    " \n",
    "Other \n",
    "(25,000) \n",
    " \n",
    "(10,000) \n",
    " \n",
    "Depreciation \n",
    "(10,000) \n",
    " \n",
    "(10,000) \n",
    " \n",
    "Programme costs \n",
    "Impairment loss \n",
    "(10,000) \n",
    " \n",
    "(5,000) \n",
    " \n",
    "Other \n",
    "(5,000) \n",
    " \n",
    "(5,000) \n",
    " \n",
    " \n",
    "(260,000) \n",
    " \n",
    "(200,000) \n",
    " \n",
    "Surplus \n",
    " \n",
    " \n",
    "5,000 \n",
    " \n",
    "30,000 \n",
    "Table 11: self-contained year-end statement (£, thousands) (multiple \n",
    "problems resolved) \n",
    " \n",
    "2011 \n",
    "2010 restated \n",
    "Income \n",
    "General income \n",
    "250,000 \n",
    "200,000 \n",
    "Increase in value \n",
    "15,000 \n",
    "30,000 \n",
    "TotalTotalTotalincome \n",
    "265,000 \n",
    "230,000 \n",
    "Administrative costs \n",
    "Staff costs \n",
    "(200,000) \n",
    "(150,000) \n",
    "Early departures \n",
    "(10,000) \n",
    "(20,000) \n",
    "Other operating costs \n",
    "(25,000) \n",
    "(10,000) \n",
    "Depreciation \n",
    "(10,000) \n",
    "(10,000) \n",
    "Programme costs \n",
    "Impairment loss \n",
    "(10,000) \n",
    "(5,000) \n",
    "Other \n",
    "(5,000) \n",
    "(5,000) \n",
    "Total  costsTotal  costsTotal  costsTotal  costsTotal  costs\n",
    "(260,000) \n",
    "(200,000) \n",
    "Surplus \n",
    "5,000 \n",
    "30,000 \n",
    " \n",
    "Table 12: merged data cells are not recommended \n",
    " \n",
    "2008 \n",
    "2009 \n",
    "Name \n",
    "Yes \n",
    "No \n",
    "Yes \n",
    "No \n",
    "Bob \n",
    "2 \n",
    "5 \n",
    "6 \n",
    "7 \n",
    "Sue \n",
    "3 \n",
    "8 \n",
    "4 \n",
    "7 \n",
    "Sam \n",
    "[data relating to both columns in \n",
    "a single cell spanning both] \n",
    "[data relating to both columns in \n",
    "a single cell spanning both] \n",
    "Table 13: use of graphic symbols \n",
    "Question \n",
    "Respondent A Respondent B \n",
    "Respondent C \n",
    "Are you a UK citizen? \n",
    " \n",
    " \n",
    " \n",
    "Are you currently employed? \n",
    " \n",
    " \n",
    " \n",
    "Do you have a driving licence? \n",
    " \n",
    " \n",
    " \n",
    "Table 14: symbols replaced by real text \n",
    "Question \n",
    "Respondent A Respondent B \n",
    "Respondent C \n",
    "Are you a UK citizen? \n",
    "No \n",
    "Yes \n",
    "No \n",
    "Are you currently employed? \n",
    "Yes \n",
    "No \n",
    "Yes \n",
    "Do you have a driving licence? \n",
    "No \n",
    "No \n",
    "Yes \n",
    "Table 15: courses offered by Institution X. A = Bachelor of Science,  \n",
    "B = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \n",
    " \n",
    "2006 \n",
    "2007 \n",
    "2008 \n",
    "2009 \n",
    "Economics \n",
    "A, B \n",
    "A, C \n",
    "A, B \n",
    "A, C \n",
    "International relations \n",
    "A, E \n",
    "A, E \n",
    "A, B \n",
    "A, E \n",
    "Philosophy \n",
    "A \n",
    "A \n",
    "A \n",
    "A, D \n",
    "Politics \n",
    "A, D \n",
    "A, D \n",
    "A, D \n",
    "A \n",
    "Mathematics \n",
    "B, C \n",
    "B \n",
    "A, E \n",
    "A, B \n",
    "English \n",
    "A, C \n",
    "A, B \n",
    "A,B \n",
    "C \n",
    " \n",
    "Table 16:  Masters courses offered by Institution X \n",
    " \n",
    "2006 \n",
    "2007 \n",
    "2008 \n",
    "2009 \n",
    "Economics \n",
    "No \n",
    "Yes \n",
    "Yes \n",
    "Yes \n",
    "International relations \n",
    "No \n",
    "No \n",
    "No \n",
    "No \n",
    "Philosophy \n",
    "No \n",
    "No \n",
    "No \n",
    "No \n",
    "Politics \n",
    "No \n",
    "No \n",
    "No \n",
    "No \n",
    "Mathematics \n",
    "Yes \n",
    "No \n",
    "No \n",
    "No \n",
    "English \n",
    "Yes \n",
    "Yes \n",
    "Yes \n",
    "Yes \n",
    "Table 17: accounts, 2011 (£, thousands)  \n",
    "Accounting item \n",
    "2011 \n",
    "Income General income \n",
    "200,000 \n",
    "Increase in value, WIP \n",
    "30,000 \n",
    "Income subtotal \n",
    "230,000 \n",
    "Administrative costs Staff  \n",
    "150,000 \n",
    "Early departures \n",
    "20,000 \n",
    "Other operating costs \n",
    "10,000 \n",
    "Depreciation \n",
    "10,000 \n",
    "Programme costs Impairment loss \n",
    "10,000 \n",
    "Costs subtotal \n",
    "200,000 \n",
    "Balance \n",
    "30,000 \n",
    "Table 18: accounts, 2011 (£, thousands) \n",
    "Accounting item \n",
    "2011 \n",
    "Income General income \n",
    "200,000 \n",
    "Increase in value, WIP \n",
    "30,000 \n",
    "Income subtotal \n",
    "230,000 \n",
    "Administrative costs Staff  \n",
    "(150,000) \n",
    "Early departures \n",
    "(20,000) \n",
    "Other operating costs \n",
    "(10,000) \n",
    "Depreciation \n",
    "(10,000) \n",
    "Programme costs Impairment loss \n",
    "(10,000) \n",
    "Costs subtotal \n",
    "(200,000) \n",
    "Balance \n",
    "30,000 \n",
    "Table 19: Human Development Index (HDI)  \n",
    "trends, 1980 to 2010. Source: Barro-Lee March, 2010  \n",
    "Country \n",
    "1980 \n",
    "1990 \n",
    "2000 \n",
    "2010 \n",
    "Afghanistan \n",
    "0.78 \n",
    "1.48 \n",
    "2.16 \n",
    "3.33 \n",
    "Albania \n",
    "8.89 \n",
    "9.67 \n",
    "9.89 \n",
    "10.38 \n",
    "Algeria \n",
    "4.74 \n",
    "3.33 \n",
    "5.50 \n",
    "7.24 \n",
    "Andorra \n",
    "4.98 \n",
    "5.63 \n",
    "9.09 \n",
    "10.35 \n",
    "Angola \n",
    "- \n",
    "- \n",
    "4.42 \n",
    "4.42 \n",
    "Table 20: footnotes referenced from within a table \n",
    "Expenditure by function £million \n",
    "2009/10 \n",
    "2010/11 1 \n",
    "Policy functions \n",
    "Financial \n",
    "22.5 \n",
    "30.57 \n",
    "Information 2 \n",
    "10.2 \n",
    "14.8 \n",
    "Contingency \n",
    "2.6 \n",
    "1.2 \n",
    "Remunerated functions \n",
    "Agency services 3 \n",
    "44.7 \n",
    "35.91 \n",
    "Payments \n",
    "22.41 \n",
    "19.88 \n",
    "Banking \n",
    "22.90 \n",
    "44.23 \n",
    "Other \n",
    "12.69 \n",
    "10.32 \n",
    " \n",
    "(1) Provisional total as of publication date. \n",
    "(2) Costs associated with on-going information programmes. \n",
    "(3) From the management accounts, net of recoveries and including interest charges \n",
    "Table 21: footnotes replaced by additional table summary text \n",
    "Expenditure by function £million \n",
    "2009/10 \n",
    "2010/11 \n",
    "Policy functions \n",
    "Financial \n",
    "22.5 \n",
    "30.57 \n",
    "Information \n",
    "10.2 \n",
    "14.8 \n",
    "Contingency \n",
    "2.6 \n",
    "1.2 \n",
    "Remunerated functions \n",
    "Agency services  \n",
    "44.7 \n",
    "35.91 \n",
    "Payments \n",
    "22.41 \n",
    "19.88 \n",
    "Banking \n",
    "22.90 \n",
    "44.23 \n",
    "Other \n",
    "12.69 \n",
    "10.32 \n",
    "Table 22: referencing multiple endnotes from within a table \n",
    "Expenditure £m \n",
    "Notes  \n",
    "(Notes located on  page [n]) \n",
    "2010 \n",
    "2011 \n",
    "Information \n",
    "1 \n",
    "10.2 \n",
    "14.8 \n",
    "Contingency \n",
    " \n",
    "2.6 \n",
    "1.2 \n",
    "Payments \n",
    "3 \n",
    "22.41 \n",
    "19.88 \n",
    "Banking services \n",
    "4 \n",
    "22.90 \n",
    "44.23 \n",
    "Interest \n",
    " \n",
    "0.23 \n",
    "0.10 \n",
    "Dividends \n",
    "23 \n",
    "2.5 \n",
    "3.68 \n",
    "Other \n",
    "9 \n",
    "12.69 \n",
    "10.32 \n",
    "Table 23: simulated table created using tabs and containing no \n",
    "structure \n",
    " \n",
    "2008 \n",
    "2009 \n",
    " \n",
    "Name  \n",
    "Entered \n",
    "Completed \n",
    "Entered \n",
    "Completed \n",
    "Bob \n",
    "22 \n",
    "21 \n",
    "20 \n",
    "19 \n",
    "Sue \n",
    "44 \n",
    "12 \n",
    "12 \n",
    "10 \n",
    " \n",
    "Table 24: year-end financial statement (£, thousands) \n",
    " \n",
    "2010 \n",
    "2009 \n",
    "2008 \n",
    "Non-current assets \n",
    "Buildings \n",
    "345 \n",
    "445 \n",
    "222 \n",
    "Investment \n",
    "567 \n",
    "654 \n",
    "423 \n",
    "Intangibles \n",
    "423 \n",
    "123 \n",
    "453 \n",
    "Current assets \n",
    "Trade \n",
    "435 \n",
    "634 \n",
    "231 \n",
    "Cash \n",
    "524 \n",
    "123 \n",
    "482 \n",
    "Other \n",
    "223 \n",
    "211 \n",
    "254 \n",
    "Current liabilities \n",
    "Trade liabilities \n",
    "154 \n",
    "125 \n",
    "421 \n",
    "Financial debt \n",
    "231 \n",
    "474 \n",
    "572 \n",
    "Provisions \n",
    "111 \n",
    "312 \n",
    "347 \n",
    "Table 25: setting column and row scope via the tags panel  \n",
    " \n",
    "2008 \n",
    "2009 \n",
    "Name \n",
    "Entered \n",
    "Won \n",
    "Entered \n",
    "Won \n",
    "Bob \n",
    "22 \n",
    "21 \n",
    "20 \n",
    "19 \n",
    "Sue \n",
    "44 \n",
    "12 \n",
    "12 \n",
    "10 \n",
    "Sam \n",
    "16 \n",
    "4 \n",
    "45 \n",
    "30 \n",
    "Table 26: courses offered by Institution X. A = Bachelor of Science,  \n",
    "B = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \n",
    " \n",
    "2006 \n",
    "2007 \n",
    "2008 \n",
    "2009 \n",
    "Economics \n",
    "A, B \n",
    "A, C \n",
    "A, C \n",
    "A, C \n",
    "International relations \n",
    "A, E \n",
    "A, E \n",
    "A, B \n",
    "A, B \n",
    "Philosophy \n",
    "A \n",
    "A \n",
    "A \n",
    "A \n",
    "Politics \n",
    "A, D \n",
    "A, D \n",
    "A, B \n",
    "A \n",
    "Mathematics \n",
    "B, C \n",
    "B \n",
    "A, B \n",
    "A, B \n",
    "English \n",
    "A, C \n",
    "A, B \n",
    "A,B \n",
    "A, C \n",
    " \n",
    "Table 27: “table” with columns simulated by using tab stops \n",
    "Name \n",
    "Apples \n",
    "Pears \n",
    "Bob Scott \n",
    "20 \n",
    "25 \n",
    "Susan. P. Arnold-Jones, BA, FRSA, MD \n",
    "24 \n",
    "15 \n",
    "Sam Holder-Dickinson \n",
    "14 \n",
    "10 \n",
    "Table 28: year-end financial table (£, thousands) – headings problem \n",
    "revisited \n",
    " \n",
    "2010 \n",
    "2009 \n",
    "2008 \n",
    "Non-current assets \n",
    "Buildings \n",
    "345 \n",
    "445 \n",
    "222 \n",
    "Investment \n",
    "567 \n",
    "654 \n",
    "423 \n",
    "Intangibles \n",
    "423 \n",
    "123 \n",
    "453 \n",
    "Current assets \n",
    "Trade \n",
    "435 \n",
    "634 \n",
    "231 \n",
    "Cash \n",
    "524 \n",
    "123 \n",
    "482 \n",
    "Other \n",
    "223 \n",
    "211 \n",
    "254 \n",
    "Current liabilities \n",
    "Trade liabilities \n",
    "154 \n",
    "125 \n",
    "421 \n",
    "Financial debt \n",
    "231 \n",
    "474 \n",
    "572 \n",
    "Provisions \n",
    "111 \n",
    "312 \n",
    "347 \n",
    "  \n",
    "Table 29: multiple headers attributes for each data cell \n",
    " \n",
    "South America \n",
    "Asia \n",
    "Africa \n",
    "Australia \n",
    "2010 \n",
    "Highest average \n",
    "523.6 \n",
    "467.4 \n",
    "405.0 \n",
    "340.5 \n",
    "Highest in 24 hours \n",
    "73.1 \n",
    "54.1 \n",
    "27.2 \n",
    "66.3 \n",
    "Highest in 12 hours \n",
    "42.4 \n",
    "30.1 \n",
    "15.9 \n",
    "40.3 \n",
    "2009 \n",
    "Highest average \n",
    "487.7 \n",
    "453.6 \n",
    "398.7 \n",
    "356 \n",
    "Highest in 24 hours \n",
    "67.2 \n",
    "53.2 \n",
    "44.3 \n",
    "53.8 \n",
    "Highest in 12 hours \n",
    "34.7 \n",
    "34.1 \n",
    "29.8 \n",
    "31.0 \n",
    "2008 \n",
    "Highest average \n",
    "496.7 \n",
    "444.3 \n",
    "502.1 \n",
    "399.6 \n",
    "Highest in 24 hours \n",
    "44.2 \n",
    "56.7 \n",
    "32.1 \n",
    "63.2 \n",
    "Highest in 12 hours \n",
    "30.1 \n",
    "32.7 \n",
    "21.9 \n",
    "40.2 \n",
    " \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f113e82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:[Node(id='Vitalink', type='App', properties={}), Node(id='Psg College Of Technology', type='Organization', properties={}), Node(id='Psg Institute Of Medical Sciences & Research', type='Organization', properties={}), Node(id='Inr', type='Medical term', properties={}), Node(id='Doctor Portal', type='Portal', properties={}), Node(id='Patient Portal', type='Portal', properties={}), Node(id='Adithya A', type='Person', properties={}), Node(id='City Hospital Lab', type='Location', properties={}), Node(id='Acitrom', type='Drug', properties={}), Node(id='Bob Scott', type='Person', properties={}), Node(id='Susan. P. Arnold-Jones', type='Person', properties={}), Node(id='Sam Holder-Dickinson', type='Person', properties={})]\n",
      "Relationships:[Relationship(source=Node(id='Vitalink', type='App', properties={}), target=Node(id='Psg College Of Technology', type='Organization', properties={}), type='DEVELOPED_BY', properties={}), Relationship(source=Node(id='Vitalink', type='App', properties={}), target=Node(id='Psg Institute Of Medical Sciences & Research', type='Organization', properties={}), type='DEVELOPED_BY', properties={}), Relationship(source=Node(id='Vitalink', type='App', properties={}), target=Node(id='Inr', type='Medical term', properties={}), type='TRACKS', properties={}), Relationship(source=Node(id='Adithya A', type='Person', properties={}), target=Node(id='Inr', type='Medical term', properties={}), type='HAS', properties={}), Relationship(source=Node(id='Adithya A', type='Person', properties={}), target=Node(id='Acitrom', type='Drug', properties={}), type='PRESCRIBED_THERAPY', properties={})]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "documents = [Document(page_content=text)]\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "print(f\"Nodes:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a1ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GraphDocument(nodes=[Node(id='Vitalink - Blood Thinner Tracking App', type='Document', properties={}), Node(id='Psg College Of Technology', type='Organization', properties={}), Node(id='Psg Institute Of Medical Sciences & Research', type='Organization', properties={}), Node(id='Inr', type='Property', properties={}), Node(id='Doctor Portal', type='Concept', properties={}), Node(id='Patient Portal', type='Concept', properties={}), Node(id='Adithya A', type='Person', properties={}), Node(id='City Hospital Lab', type='Location', properties={}), Node(id='Acitrom', type='Other', properties={}), Node(id='Aspirin', type='Other', properties={}), Node(id='Omega-3 Supplements', type='Other', properties={})], relationships=[Relationship(source=Node(id='Vitalink - Blood Thinner Tracking App', type='Document', properties={}), target=Node(id='Psg College Of Technology', type='Organization', properties={}), type='CREATED', properties={}), Relationship(source=Node(id='Vitalink - Blood Thinner Tracking App', type='Document', properties={}), target=Node(id='Psg Institute Of Medical Sciences & Research', type='Organization', properties={}), type='CREATED', properties={}), Relationship(source=Node(id='Vitalink - Blood Thinner Tracking App', type='Document', properties={}), target=Node(id='Inr', type='Property', properties={}), type='MENTIONS', properties={}), Relationship(source=Node(id='Adithya A', type='Person', properties={}), target=Node(id='Acitrom', type='Other', properties={}), type='HAS_PROPERTY', properties={})], source=Document(metadata={}, page_content=' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "956bc1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "# os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "# os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "# os.environ[\"NEO4J_PASSWORD\"] = \"iambatman\"\n",
    "load_dotenv()\n",
    "graph = Neo4jGraph(refresh_schema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea232a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_graph_documents(graph_documents, include_source=True, baseEntityLabel=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a554b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing query tool...\n",
      "{'n': {'id': '717eed0f966b74bef08a2d3c2cd7fe48', 'text': ' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'}, 'r': ({'id': '717eed0f966b74bef08a2d3c2cd7fe48', 'text': ' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'}, 'MENTIONS', {'id': 'Vitalink - Blood Thinner Tracking App'}), 'm': {'id': 'Vitalink - Blood Thinner Tracking App'}}\n",
      "{'n': {'id': '717eed0f966b74bef08a2d3c2cd7fe48', 'text': ' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'}, 'r': ({'id': '717eed0f966b74bef08a2d3c2cd7fe48', 'text': ' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'}, 'MENTIONS', {'id': 'Psg College Of Technology'}), 'm': {'id': 'Psg College Of Technology'}}\n",
      "{'n': {'id': '717eed0f966b74bef08a2d3c2cd7fe48', 'text': ' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'}, 'r': ({'id': '717eed0f966b74bef08a2d3c2cd7fe48', 'text': ' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'}, 'MENTIONS', {'id': 'Psg Institute Of Medical Sciences & Research'}), 'm': {'id': 'Psg Institute Of Medical Sciences & Research'}}\n",
      "{'n': {'id': '717eed0f966b74bef08a2d3c2cd7fe48', 'text': ' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'}, 'r': ({'id': '717eed0f966b74bef08a2d3c2cd7fe48', 'text': ' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'}, 'MENTIONS', {'id': 'Inr'}), 'm': {'id': 'Inr'}}\n",
      "{'n': {'id': '717eed0f966b74bef08a2d3c2cd7fe48', 'text': ' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'}, 'r': ({'id': '717eed0f966b74bef08a2d3c2cd7fe48', 'text': ' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'}, 'MENTIONS', {'id': 'Doctor Portal'}), 'm': {'id': 'Doctor Portal'}}\n",
      "{'n': {'id': '717eed0f966b74bef08a2d3c2cd7fe48', 'text': ' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'}, 'r': ({'id': '717eed0f966b74bef08a2d3c2cd7fe48', 'text': ' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'}, 'MENTIONS', {'id': 'Patient Portal'}), 'm': {'id': 'Patient Portal'}}\n",
      "{'n': {'id': '717eed0f966b74bef08a2d3c2cd7fe48', 'text': ' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'}, 'r': ({'id': '717eed0f966b74bef08a2d3c2cd7fe48', 'text': ' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'}, 'MENTIONS', {'id': 'Adithya A'}), 'm': {'id': 'Adithya A'}}\n",
      "{'n': {'id': '717eed0f966b74bef08a2d3c2cd7fe48', 'text': ' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'}, 'r': ({'id': '717eed0f966b74bef08a2d3c2cd7fe48', 'text': ' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'}, 'MENTIONS', {'id': 'City Hospital Lab'}), 'm': {'id': 'City Hospital Lab'}}\n",
      "{'n': {'id': '717eed0f966b74bef08a2d3c2cd7fe48', 'text': ' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'}, 'r': ({'id': '717eed0f966b74bef08a2d3c2cd7fe48', 'text': ' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'}, 'MENTIONS', {'id': 'Acitrom'}), 'm': {'id': 'Acitrom'}}\n",
      "{'n': {'id': '717eed0f966b74bef08a2d3c2cd7fe48', 'text': ' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'}, 'r': ({'id': '717eed0f966b74bef08a2d3c2cd7fe48', 'text': ' Neuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nNeuro Prune: An Adaptive Approach for Efficient\\nDeep Neural Network Optimization on Edge\\nDevices\\nDr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\\n1Professor, Department of Computer Science and Engineering, PSG College of Technology\\ngrk.cse@psgtech.ac.in\\n2,3,4,5BE-CSE (AI&ML), PSG College of Technology\\nadhishthesak@gmail.com, mohanakumarp2828@gmail.com\\nsanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\\nAbstract—The optimization of deep neural networks for de-\\nployment on edge devices is a significant research area due to\\nthe demand for applications such as augmented reality, smart\\ncameras, and autonomous navigation. However, deploying large\\ndeep learning models on edge devices poses challenges related to\\ncomputational power, energy consumption, and latency. Pruning\\nis a method to reduce the model size, accelerate inference, and\\nsave power. The objective of the paper is to propose the Neuro\\nPrune algorithm and to apply it for the optimization of deep\\nneural networks on edge devices. Efforts have been made to\\ncompare pruned and unpruned models. As a result, the pruned\\nmodel has an accuracy increase of 0.22%.\\nI. INTRODUCTION\\nThe rise of intelligent systems, ranging from autonomous\\ndrones to augmented reality (AR) devices and smart surveil-\\nlance cameras, has intensified the need for efficient deep neural\\nnetwork deployment on edge devices with constrained com-\\nputational resources. While advanced deep learning models\\nlike YOLO, SSD, ResNet, and MobileNet excel in accuracy,\\ntheir deployment on such devices is hindered by significant\\ncomputational demands, large memory footprints, and high\\nenergy consumption. These challenges pose critical barriers to\\nreal-time applications and sustainable deployment.\\nThis paper builds on existing pruning techniques, includ-\\ning unstructured pruning, structured pruning, and approaches\\ninspired by quantization, highlighting their strengths and lim-\\nitations. In response to the challenges identified, we introduce\\nNeuro Prune, a novel pruning approach that integrates princi-\\nples from reinforcement learning and the Lottery Ticket Hy-\\npothesis to achieve an optimal balance between accuracy, com-\\nputational efficiency, and energy consumption. By integrating\\nthese principles, Neuro Prune provides a robust framework\\nfor optimizing deep neural networks for deployment on edge\\ndevices without sacrificing performance.\\nNeuro Prune reduces model size and computational re-\\nquirements while maintaining or even improving the model’s\\naccuracy. These findings underscore the potential of Neuro\\nPrune to enable real-time, energy-efficient deep learning on\\nresource-constrained devices.\\nNeuro Prune employs a systematic process to optimize\\nneural networks for edge deployment:\\n• Mask Initialization: Each layer’s weights are paired with\\na binary mask (original_mask), initialized to ones.\\n• Activation Tracking: During forward passes over the\\ndataset, the magnitudes of weight activations are mon-\\nitored and recorded.\\n• Normalize and Prune: The recorded activations are\\nnormalized, and a threshold based on the pruning fraction\\nis computed. Using this threshold, the masks are dynam-\\nically updated to prune unimportant weights.\\n• Evaluate and Log Metrics: After pruning, metrics such\\nas model sparsity and accuracy are computed. A reward\\nmetric is derived to balance accuracy and sparsity using\\na tunable parameter, λweight.\\n• Retrain the Model: The pruned model undergoes retrain-\\ning to improve the loss in accuracy while maintaining low\\ncomputational requirements\\nNeuro Prune’s unique combination of reinforcement learn-\\ning for adaptive decision-making and the Lottery Ticket Hy-\\npothesis for identifying optimal subnetworks enables it to\\nachieve remarkable efficiency. This approach ensures substan-\\ntial reduction in model size and energy consumption while\\npreserving accuracy and inference speed, making it an ideal\\nsolution for edge-based deep learning applications.\\nThrough extensive empirical evaluations and theoretical\\ninsights, this paper demonstrates how Neuro Pruning outper-\\nforms traditional pruning methods, setting a new benchmark\\nfor sustainable, high-performance AI deployment on edge\\ndevices. By situating our work within the broader research\\ncontext, we aim to offer a critical contribution to the ongoing\\nevolution of neural network optimization techniques.\\nII. BACKGROUND\\nA. Need for Pruning Techniques\\nPruning is a crucial optimization technique for deploying\\ndeep neural networks in resource-constrained environments\\nlike healthcare. For example, wearable devices like smart-\\nwatches that monitor heart rates or portable EEG systems\\nfor brain activity analysis require lightweight models due to\\ntheir limited computational power and memory. Pruning helps\\naddress these constraints by reducing the memory footprint\\nand computational load, enabling efficient model deployment.\\nAdditionally, it minimizes energy consumption, which is vital\\nfor sustainable and real-time healthcare applications. Faster\\ninference enabled by pruning is particularly beneficial for\\ntime-sensitive tasks like seizure detection or emergency di-\\nagnostics. Moreover, pruning enhances model interpretability\\nby retaining only the most critical components, which is\\nessential for gaining trust and ensuring regulatory compliance\\nin healthcare. By reducing the hardware and operational costs,\\npruning also makes deploying advanced AI solutions on edge\\ndevices more cost-effective and accessible.\\nB. Types of Pruning Techniques\\nPruning methods are broadly classified based on granu-\\nlarity, timing, and approach. Granularity-based pruning in-\\ncludes structured pruning, which removes entire components\\nsuch as neurons or filters to streamline computations, and\\nunstructured pruning, which eliminates individual weights\\nfor finer optimization, though it often requires specialized\\nhardware. Timing-based pruning involves pre-training pruning,\\nwhich optimizes models before training; post-training prun-\\ning, where redundant components are removed from trained\\nmodels with possible retraining; and dynamic pruning during\\ntraining, which adjusts models in real-time for improved\\nadaptability. Metric-based pruning uses parameters like weight\\nmagnitude or gradient contribution to identify and remove\\nless critical components, while application-specific pruning\\ntailors strategies to specific tasks, such as optimizing for EEG\\nsignal analysis, or hardware constraints like GPUs. Together,\\nthese methods enable the development of efficient, scalable,\\nand task-specific models, particularly suited for edge device\\ndeployment in healthcare and other domains.\\nIII. RELATED WORK\\nJielei Wang et al.[12] introduced an absorption pruning\\nmethod for object detection in remote sensing imagery. This\\nstudy achieves efficient compression with minimal accuracy\\nloss but requires careful layer-wise pruning ratio tuning.\\nJan Muller et al., in their research[12], propose a neural\\nnetwork pruning method for multi-object tracking (MOT) that\\nreduces model size by up to 70\\nLiang Li et al., in their work[9], introduce a novel pruning\\nmethod for DNNs that utilizes a self-adaptive mechanism\\nbased on weight sparsity ratios and a protective reconstruction\\nmechanism. Their approach improves both model compres-\\nsion and accuracy, outperforming state-of-the-art methods on\\nCIFAR-10 and ImageNet datasets.\\nGiacomo Di Fabrizio et al.[4] discuss integrating machine\\nlearning with edge devices like Google Coral AI and Nvidia\\nJetson Nano. Their work revolutionizes computer vision and\\nreal-time tracking but raises energy concerns due to the\\ncomputational demands of advanced algorithms.\\nSatoru Koda et al.[7] explore how weight pruning in\\nDNNs enhances out-of-distribution (OOD) detection, particu-\\nlarly with Mahalanobis-based approaches, by improving global\\nfeature extraction and leveraging weights not critical for\\nclassification.\\nShvetha S Kumar et al. analyze and compare three pruning\\ntechniques—L1-norm filter pruning, channel pruning, and\\nweight pruning—on CNNs for accuracy and inference time,\\nhighlighting greater speedup on NVIDIA V100 GPUs com-\\npared to GTX1080 Ti in their study[8].\\nA Arun Prakash et al. propose a sub-path pruning algorithm\\nfor solving the Minimum Robust-Cost Path (MRCP) problem\\non stochastic networks, extending it to related problems and\\ndemonstrating its efficiency on real-world networks in their\\nstudy[13].\\nKyoungtaek Choi et al.[2] show that Quantization-Aware\\nTraining outperforms Post-Training Quantization, and sparsity\\ntraining with Optimal Reduction pruning improves YOLOv4’s\\nefficiency and edge-device performance, despite potential\\nhigh-pruning rate drawbacks.\\nViviana Crescitelli et al.[3] propose a filter-level pruning\\nmethod for DNNs. This approach optimizes inference speed\\nand resource efficiency on edge devices with minimal accuracy\\nloss but requires careful tuning of layer-specific pruning ratios.\\nYongqi An et al.[1] introduce a retraining-free structured\\npruning framework for Large Language Models (LLMs). Their\\nmethod incorporates a fluctuation-based pruning metric, adap-\\ntive structure search, and a bias compensation mechanism to\\nachieve efficient pruning. FLAP significantly enhances infer-\\nence speed and reduces model size without requiring retrain-\\ning, outperforming state-of-the-art methods like LLM-Pruner\\nand Wanda-sp on LLaMA models across various benchmarks.\\nFrom referring to these research papers, we derived an\\nidea for pruning on edge devices. Additionally, our proposed\\nsolution is inspired by the research of Jaron Maene et al.[10],\\nwhich suggests that sparse subnetworks in dense networks can\\nachieve similar accuracy when retrained. This study demon-\\nstrates stable training with linear mode connectivity, support-\\ning the idea that lottery tickets retrain to similar regions, but\\nquestions their independence from dense training and iterative\\npruning.\\nEran Malach et al.[11] further strengthen the lottery ticket\\nhypothesis by proving that over-parameterized neural networks\\nwith random weights always contain a subnetwork matching\\nthe accuracy of a target network, without additional training.\\nIV. NEUROLOGICAL PRUNING APPROACH\\nThe Neurological Pruning Algorithm is designed to opti-\\nmize neural networks by iteratively pruning less important\\nweights while maintaining high performance. Unlike tradi-\\ntional methods, this approach incorporates adaptive, layer-\\nspecific pruning thresholds that dynamically adjust based\\non weight activations, ensuring efficient model optimization\\nacross various stages of training. Drawing inspiration from\\nthe Lottery Ticket Hypothesis, which suggests that sparse\\nsubnetworks can perform similarly to their dense counterparts\\nwhen retrained, the algorithm selectively deactivates weights\\nusing binary masks to enhance computational efficiency with-\\nout sacrificing accuracy.\\nThe algorithm operates in several epochs, during which\\nweight activations are evaluated. Based on a quantile-based\\nstrategy, pruning thresholds are established for each layer, en-\\nsuring that only weights contributing minimally to the model’s\\noutput are removed. After pruning, the model undergoes fine-\\ntuning to restore any lost accuracy and further optimize the\\nnetwork. This step enhances the adaptability and scalability\\nof the pruning method, making it suitable for deployment in\\nresource-constrained environments\\nA. Key Characteristics and Advantages\\nThe Neurological Pruning Algorithm boasts several key\\ncharacteristics that distinguish it from conventional pruning\\ntechniques. One notable feature is its layer-specific pruning,\\nwhich evaluates the importance of weights individually for\\neach layer, leading to a more nuanced and efficient sparsifi-\\ncation process. This approach prevents the model from losing\\nessential features that may otherwise be pruned by a global\\napproach.\\nThe algorithm’s use of quantile-based thresholding en-\\nsures that only weights with the least contribution to the\\nmodel’s output are pruned. This fine-tuned method avoids\\nperformance degradation by carefully targeting weights that\\nare less significant to the network’s overall functionality.\\nFurthermore, the pruning process is iterative, allowing for\\ncontinuous model evaluation and real-time adjustments based\\non performance feedback, ensuring that accuracy is retained\\neven as sparsity increases.\\nAdditionally, the algorithm maintains a careful balance\\nbetween sparsity and accuracy, enabling a reduction in the\\nmodel’s computational complexity while preserving its predic-\\ntive capabilities. This characteristic is particularly important\\nfor applications requiring rapid inference times, such as in\\nreal-time healthcare systems and edge computing scenarios,\\nwhere both energy efficiency and computational power are\\ncrucial.\\nB. Impact on Model Performance\\nThe Neurological Pruning Algorithm significantly improves\\nthe efficiency of neural networks, making them more suitable\\nfor deployment on edge devices. Key performance metrics\\nsuch as compression ratio (CR) and accuracy retention\\n(AR) are critical indicators of the algorithm’s effectiveness.\\nFor instance, in evaluating a ResNet-18 model, the algorithm\\nachieved a CR of 1, indicating that while the number of\\nparameters remained the same, the model’s efficiency im-\\nproved through selective pruning, reducing computational load\\nwithout sacrificing performance.\\nMoreover, the algorithm demonstrates the ability to achieve\\nhigh accuracy retention, with models maintaining or even\\nslightly improving their predictive performance post-pruning.\\nThis outcome highlights the algorithm’s capability to effi-\\nciently remove redundant weights while preserving the core\\nfunctionality of the network. The result is a model that is\\nnot only more computationally efficient but also adaptable\\nacross different architectures and datasets, making it suit-\\nable for a wide range of applications, from image classification\\nto natural language processing .\\nIn practical terms, the Neurological Pruning Algorithm en-\\nables faster inference times and lower energy consumption,\\ntwo crucial aspects for deploying AI models in resource-\\nconstrained environments. As such, it holds significant poten-\\ntial for improving the viability of real-time, energy-efficient\\nAI applications, particularly in fields like healthcare, where\\nboth speed and sustainability are essential.\\nV. NEUROLOGICAL PRUNING ALGORITHM\\nThe Neurological Pruning Algorithm is a structured and\\nadaptive methodology designed to optimize neural networks\\nfor resource-constrained environments. This algorithm goes\\nbeyond traditional pruning approaches by incorporating layer-\\nspecific pruning thresholds that dynamically adjust to the\\nimportance of weight activations within each layer. Drawing\\non principles inspired by the Lottery Ticket Hypothesis and re-\\ninforcement learning, the algorithm strikes an optimal balance\\nbetween model sparsity and accuracy .\\nA. Algorithm Description\\nThe Neurological Pruning Algorithm iteratively refines the\\nneural network through a series of training epochs. During\\neach epoch, weight activations are tracked, normalized, and\\nanalyzed to identify low-importance weights. These weights\\nare selectively deactivated using binary masks, ensuring com-\\nputational efficiency without compromising the network’s pre-\\ndictive performance. The algorithm further integrates a fine-\\ntuning phase, enabling the pruned network to recover and\\nenhance its accuracy. The detailed process is presented in\\nAlgorithm 1.\\nBy dynamically adjusting to layer-specific characteristics\\nand maintaining performance metrics, the Neurological Prun-\\ning Algorithm ensures a significant reduction in computational\\nand memory requirements, paving the way for efficient deploy-\\nment of deep learning models on edge devices .\\nB. Input Parameters and Output\\nInputs:\\n• Pretrained model (M)\\n• Dataset (D)\\n• Prune fraction (p)\\n• Total epochs (E)\\nOutput:\\n• Pruned and fine-tuned model (Mpruned)\\nC. Key Features and Advantages\\nThe Neurological Pruning Algorithm is designed with sev-\\neral distinctive features that contribute to its effectiveness\\nin optimizing neural network performance. These features\\ninclude:\\nAlgorithm 1 Simplified Neurological Pruning Algorithm\\nRequire: Pretrained model M, Dataset D, Prune fraction p,\\nTotal epochs E\\nEnsure: Pruned and fine-tuned model Mpruned\\n1: Initialize binary masks M with all ones\\n2: for each epoch e = 1 to E do\\n3:\\nCompute weight activations A during forward passes\\n4:\\nfor each layer i in M do\\n5:\\nCompute pruning threshold Ti as p-quantile of Ai\\n6:\\nUpdate mask Mi: retain weights with Ai > Ti\\n7:\\nApply mask to layer: θi ←θi · Mi\\n8:\\nend for\\n9:\\nEvaluate model performance and update metrics\\n10: end for\\n11: Fine-tune Mpruned on D\\n12: return Mpruned\\n• Layer-specific Pruning: The algorithm applies pruning\\non a per-layer basis, evaluating the weight activations\\nwithin each layer individually. This approach ensures that\\nthe sparsity introduced by pruning is well-distributed and\\ncontext-dependent, facilitating more nuanced control over\\nmodel optimization .\\n• Quantile-based Thresholding: A quantile-based strat-\\negy is employed to determine the pruning threshold for\\neach layer. This method ensures that only weights with\\nminimal contribution to the network’s output are pruned,\\nthereby reducing the risk of adversely affecting model\\naccuracy while enhancing sparsity .\\n• Iterative Pruning and Evaluation: The pruning pro-\\ncess follows an iterative approach, enabling continuous\\nassessment of the model’s performance after each pruning\\nstep. This ensures dynamic adjustments to maintain a bal-\\nance between model sparsity and predictive performance\\nthroughout training.\\n• Preservation of Model Accuracy: By adjusting pruning\\nthresholds in response to ongoing performance metrics,\\nthe algorithm ensures that model accuracy is not com-\\npromised. The careful tuning of these thresholds allows\\nfor significant sparsity while maintaining the robustness\\nof the network’s predictive capabilities .\\nD. Impact on Neural Network Performance\\nThe Neurological Pruning Algorithm has been extensively\\nevaluated across various benchmark datasets and model archi-\\ntectures, demonstrating its capacity to achieve the following\\noutcomes:\\n• High Sparsity with Minimal Accuracy Loss: The algo-\\nrithm effectively prunes unnecessary weights, achieving\\nhigh sparsity levels without causing significant degrada-\\ntion in model accuracy. This outcome is critical for the\\nefficient deployment of models in resource-constrained\\nenvironments.\\n• Reduced Computational Complexity: By sparsifying\\nthe weight matrices, the algorithm reduces the compu-\\ntational load during inference, which translates to faster\\nmodel execution and lower resource consumption. This\\ncharacteristic is especially beneficial in edge computing\\nand real-time applications .\\n• Broad Applicability: The algorithm is highly flexible\\nand can be adapted to various neural network architec-\\ntures and datasets, making it suitable for a wide range\\nof tasks, from image recognition to natural language\\nprocessing .\\nVI. EVALUATION METRICS\\nA. Performance Comparison\\nFig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\\nTABLE I\\nCOMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\\nMetric\\nUnpruned\\nModel\\nPruned\\nModel\\nAccuracy\\n0.9562\\n0.9583\\nTrain Loss\\n0.1414\\n0.1625\\nInference\\nTime\\n(s)\\n0.0318\\n0.0314\\nNo of Parame-\\nters(million)\\n11.7\\n11.7\\nB. Evaluation Metrics Calculation\\nThe key metrics used to evaluate the proposed pruning\\ntechnique are as follows:\\nCompression Ratio (CR): The Compression Ratio (CR) is\\ncalculated as:\\nCR = Total Parameters After Pruning\\nTotal Parameters Before Pruning\\n(1)\\nCR = 11.7M\\n11.7M = 1\\nAccuracy Retention (AR): The Accuracy Retention (AR) is\\ncalculated as:\\nAR =\\n\\x12Accuracy After Pruning −Baseline Accuracy\\nBaseline Accuracy\\n\\x13\\n×100\\n(2)\\nAR =\\n\\x120.958284 −0.956206\\n0.956206\\n\\x13\\n× 100 = 0.22%\\nThis result indicates that the pruned model achieves an ac-\\ncuracy improvement of 0.22% when compared to unpruned\\nmodel.\\nC. Illustration of Results\\nFig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\\n18.\\nD. Discussion\\nThe evaluation metrics highlight the effectiveness of the\\nproposed Neurological Pruning method on the ResNet-18\\nmodel. The Compression Ratio (CR) remains 1, indicating\\nno reduction in the total number of parameters, which suggests\\nthat this approach optimizes the neural network by selectively\\npruning connections rather than reducing the overall parameter\\ncount. The Accuracy Retention (AR), which is approximately\\n0.22%, demonstrates that the Neurological Pruning technique\\nsuccessfully improved the accuracy compared to the baseline\\nmodel.\\nThese results validate the potential of Neurological Pruning\\nas an innovative approach to optimizing deep learning models.\\nBy maintaining accuracy while likely enhancing sparsity and\\ncomputational efficiency, this method presents a promising di-\\nrection for achieving high-performance models under resource\\nconstraints.\\nVII. CONCLUSION\\nIn summary, the Neurological Pruning Algorithm provides\\na systematic and principled approach to optimizing neural\\nnetworks. By balancing the trade-off between sparsity and\\naccuracy, it enables significant reductions in computational\\ncomplexity without sacrificing the network’s predictive per-\\nformance. Future work will focus on extending the algorithm\\nto dynamic network architectures and incorporating advanced\\nregularization techniques to further enhance its robustness and\\nadaptability in diverse applications.\\nREFERENCES\\n[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\\nFluctuation-based Adaptive Structured Pruning for Large Language\\nModels.\\n[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\\nSimplification of Deep Neural Network-Based Object Detector for Real-\\nTime Edge Computing.\\n[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\\nDevices Object Detection by Filter Pruning.\\n[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\\nand Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\\nTracking Algorithms in Edge Devices.\\n[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\\nChen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\\nCompression Method for Object Detection Network for Edge Devices.\\n[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\\nCao. Absorption Pruning of Deep Neural Network for Object Detection\\nin Remote Sensing Imagery.\\n[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\\nand Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\\nDistribution Detection: An Empirical Survey.\\n[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\\nSahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\\n[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\\nCompress DNNs.\\n[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\\nderstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\\n[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\\nProving the Lottery Ticket Hypothesis: Pruning is All You Need.\\n[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\\nDevices via Reconstruction-Based Channel Pruning.\\n[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\\nDetermine Reliable Paths on Networks with Random and Correlated\\nLink Travel Times.\\n[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\\nand D. Suganthi. Real-Time Object Detection on Edge Devices Using\\nMobile Neural Networks.\\n[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\\nTang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\\nModels.\\n[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\\nLi, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\\nDeng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\\nDetection of Tea Leaves with One Bud and Two Leaves Based on\\nShuffleNetv2-YOLOv5-Lite-E.\\nVitaLink - Blood Thinner Tracking App for\\nHospitals\\nUser Manual\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\nDoctor Portal\\nWelcome to the VitaLink - Blood Thinner Tracking App for Hospitals, developed by students of PSG College of\\nTechnology and PSG Institute of Medical Sciences & Research. This manual is designed to help healthcare professionals\\nefficiently manage patient records, track blood thinner therapy, and monitor INR (International Normalized Ratio) \\nvalues. Follow the steps below to navigate the app and utilize its features effectively.\\n1. Logging In \\nTo access VitaLink, you need to log in with your credentials.\\nSteps:\\nOpen the App: Launch the VitaLink app on your device.\\n1.\\nEnter Credentials:On the login screen, enter your username and password in the respective fields.\\n2.\\nUsername: Enter your assigned username.\\nPassword: Enter your secure password.\\nSign In: Tap the red Sign In button to log in.\\n3.\\n2. Viewing Patient Lists\\nOnce logged in, you can view a list of patients under your care in either Card View or Table View.\\nCard View:\\n Navigate: After logging in, you’ll see the View Patient Page. \\n1.\\nPatient Cards: Each patient is displayed as a card with their name, age, and gender. \\n2.\\nShow Options: Tap Show Options on a patient card to view more details (e.g., INR, therapy).\\n3.\\nSwitch Views: Tap the Table View button at the top to switch to a tabular format.\\n4.\\nTable View:\\nTable Format: Displays patients in a table with columns for Name, Age, Gender, and Doctor.\\n1.\\nFilter and Search: Use the Entries per page dropdown to adjust the number of patients\\nshown, and the Search bar to find a specific patient.\\n2.\\nPagination: Use the navigation arrows at the bottom to browse through pages.\\n3.\\nSwitch Views: Tap the Cards View button to return to the card layout.\\n4.\\n3. Viewing Patient Details\\nAccess detailed information about a specific patient, including their INR, therapy, and missed\\ndoses.\\nSteps:\\nSelect a Patient: From the patient list (Card or Table View), tap Show Options on a patient\\ncard, or click the patient’s name in Table View.\\n1.\\nPatient Details: The patient’s page displays:\\n2.\\nBasic Info: Name, age, gender, target INR, and latest INR.\\nAssigned Roles: Doctor and caregiver names.\\nTherapy Details: Therapy type and start date.\\nMissed Doses: Dates when doses were missed.\\nPrescription: Weekly dosage schedule (e.g., Mon: 5.6 mg).\\nNavigation: Tap Back to return to the patient list.\\n3.\\n4. Viewing Reports\\nMonitor patient INR reports and critical values through the View Report Page.\\nSteps:\\naccess Reports: From the main menu, tap View Report Page.\\n1.\\nTabs:\\n2.\\nToday’s Reports: Shows reports for the current day.\\nAll Reports: Displays all historical reports.\\nReport Details: Each report includes:\\n3.\\nPatient name and location.\\na.\\nTest type (e.g., INR Report).\\nb.\\nINR value (highlighted as Critical if outside safe range).\\nc.\\nDate of the report.\\nd.\\nDocument format (e.g., APP.pdf).\\ne.\\nView Full Report: Tap View Full Report to access the complete document.\\n4.\\n5. Adding a New Patient\\nAdd new patients to the system to begin tracking their blood thinner therapy.\\nSteps:\\nNavigate: From the main menu, tap Add Patient Page.\\n1.\\nEnter Details: Fill in the following fields:\\n2.\\nName: Enter the patient’s full name.\\nAge: Enter the patient’s age.\\nGender: Select the patient’s gender from the dropdown (e.g., M, F).\\nTarget INR: Set the minimum and maximum INR range (e.g., 2.5–3.5).\\nCaregiver: Assign a caregiver (default: Not Assigned).\\nTherapy: Select the therapy type (e.g., Acitrom).\\nTherapy Start Date: Enter the start date (format: dd-mm-yyyy).\\nMedical History:\\nDiagnosis: Enter the patient’s diagnosis.\\nDuration and Unit: Specify the duration of the condition (e.g., 5 Days).\\nTap + Add Medical History to include additional entries.\\nPrescription: Set the dosage for each day of the week (e.g., Mon: 5 mg).\\nContact Information:\\nContact: Enter the patient’s contact number.\\nKin Name: Enter the name of the patient’s next of kin.\\nKin Contact: Enter the kin’s contact number.\\nSave: Tap the Add Patient button to save the record.\\n3.\\nPatient Portal\\n1.LOGIN\\nGetting Started\\nSteps to Access Your Account:\\n1. Username: Enter the username provided by your hospital (e.g.,john.doe@hospital.org).\\n2. Password: Input your secure password.\\n3. Sign In: Click the Sign In button.\\nNotes:\\n1. Phone Number is Password (contact admin for reset).\\n2.Ensure your device has internet connectivity.\\n2.PROFILE OVERVIEW\\nYour profile is the dashboard for critical health data:\\nPersonal Details:\\n1.\\nName, age, and gender (e.g., Adithya A, Age: 25, Gender: M).\\nINR Targets:\\n2.\\nYour ideal INR range (e.g., 8.5 - 10.5). Deviations may require dosage adjustments.\\nLatest INR Value:\\n3.\\nDisplays the most recent INR reading with timestamp (e.g., 20.0 as of October 28, 2024).\\nMedical Team:\\n4.\\nLists assigned doctors, caregivers, and prescribed therapy (e.g., Acitrom started on\\n24/01/2025).\\n   5.Medical History:\\nChronic conditions and durations (e.g., RHD-Post MVR, 27 years).\\n   6.Missed Doses:\\nDates of skipped doses (e.g., 31-03-2025). Address these promptly.\\n   Tip: Review this page weekly to stay aligned with your care plan.\\n3.Updating INR Values\\nINR (International Normalized Ratio) measures blood clotting time. Regular updates\\nensure your dosage is safe.\\n1. INR Value: Enter the numeric result from your lab test (e.g., 2.8).\\n2. Test Location: Specify where the test was done (e.g., City Hospital Lab).\\n3. Test Date: Use dd-mm-yyyy format (e.g., 28-10-2024).\\n4. Upload Document: Attach a scanned copy of your lab report (PDF/JPEG/PNG).\\n5. Submit: Click Submit INR Report to save.\\nImportant:\\n- Inconsistent INR values trigger alerts to your care team.\\n- Update within 24 hours of receiving results.\\n4.Take Dosage Page\\n1. Missed Doses (Last 7 Days):\\n- View recent missed doses (e.g., 23-04-2025).\\n- Mark as Taken: Click a date to update its status.\\n2. Older Missed Doses:\\n- Scroll through historical entries (e.g., 01-02-2025).\\n- Use Previous and Next buttons to navigate pages.\\nWhy This Matters:\\n- Missing doses increases clotting risks.\\n- Accurate logs help doctors adjust prescriptions.\\nTip: Sync this page with your calendar for reminders.\\n5.Lifestyle Changes\\nReport Changes Affecting Therapy:\\n1. Details: Describe diet, exercise, or habit changes (e.g., Started keto diet).\\n2. Other Medications: List non-blood-thinner drugs (e.g., Omega-3\\nsupplements).\\n3. Submit: Click to notify your care team.\\nNote: Alcohol consumption or new supplements can alter INR—always report\\nthese.\\nOther Medications\\n6.Other Medications Page\\nTrack Non-Blood-Thinners:\\n1. List Medications: Enter names and dosages (e.g., Aspirin 80mg\\ndaily).\\n2. Submit: Save to your profile.\\nTip: Update this page after every pharmacy visit\\n7.Prolonged Illness \\nReporting Health Changes\\nsteps:\\n1. Details: Describe symptoms (e.g., Persistent cough for 2 weeks).\\n2. Medications: List prescriptions for the illness (e.g., Antibiotics).\\n3. Submit: Alert your doctor for follow-up.\\nNote: Illnesses like infections can impact INR stability.\\n8.Side Effects\\nReport Adverse Reactions:\\nsteps:\\nReport Adverse Reactions:\\n1. Check Symptoms: Select all that apply (e.g., Bruising without\\nInjury).\\n2. Describe Other Effects: Use the text box for details (e.g., Rash on\\narms).\\n3. Submit: Click to send an urgent alert.\\nEmergency Symptoms:\\n- Seek immediate care for:\\n- Vomiting Blood\\n- Black Stool\\n- Vision Changes\\nDesign and build  \\naccessible PDF tables  \\nSample tables \\nTable 1 \\nColumn header (TH) \\nColumn header (TH) \\nColumn header (TH) \\nRow header (TH) \\nData cell (TD) \\nData cell (TD) \\nRow header(TH) \\nData cell (TD) \\nData cell (TD) \\nTable 2: example of footnotes referenced from within a table \\nExpenditure by function £ million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation 2 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries, including interest charges. \\nTable 3: \"film credits\" style layout \\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 4: table 3 with column headers added \\n RoleActor\\nMain character Daniel Radcliffe \\nSidekick 1 Rupert Grint \\nSidekick 2 Emma Watson \\nLovable ogre Robbie Coltrane \\nProfessor Maggie Smith \\nHeadmaster Richard Harris \\n\\nTable 5: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\n\\nTable 6: a table with a more serious headings problem \\nRainfall \\n(inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\n2010 \\nAverage \\n104 \\n201 \\n193 \\n144 \\n24 hour high \\n15 \\n26 \\n27 \\n18 \\n12 hour high \\n9 \\n10 \\n11 \\n12 \\n2009 \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\nTable 7: year-end statement, non-current assets (£, thousands) \\nNon-current assets \\n2010 \\n2009 \\n2008 \\nProperty \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nTable 8: year-end statement, current assets (£, thousands) \\nCurrent assets \\n2010 \\n2009 \\n2008 \\nTrade and other receivables \\n435 \\n634 \\n231 \\nCash and cash equivalents \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nTable 9: rainfall by continent, 2009 \\nRainfall (inches) \\nAmericas \\nAsia \\nEurope \\nAfrica \\nAverage \\n133 \\n244 \\n155 \\n166 \\n24 hour high \\n27 \\n28 \\n29 \\n20 \\n12 hour high \\n11 \\n12 \\n13 \\n16 \\n\\nTable 10: self-contained year-end statement (£, thousands) (multiple \\nlayout problems) \\n\\n2011 \\n2010 restated \\nGeneral income \\n\\n250,000 \\n\\n200,000 \\nIncrease in value, WIP \\n\\n15,000 \\n\\n30,000 \\n\\n\\n265,000 \\n\\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n\\n(150,000) \\n\\nEarly departures \\n(10,000) \\n\\n(20,000) \\n\\nOther \\n(25,000) \\n\\n(10,000) \\n\\nDepreciation \\n(10,000) \\n\\n(10,000) \\n\\nProgramme costs \\nImpairment loss \\n(10,000) \\n\\n(5,000) \\n\\nOther \\n(5,000) \\n\\n(5,000) \\n\\n\\n(260,000) \\n\\n(200,000) \\n\\nSurplus \\n\\n\\n5,000 \\n\\n30,000 \\nTable 11: self-contained year-end statement (£, thousands) (multiple \\nproblems resolved) \\n\\n2011 \\n2010 restated \\nIncome \\nGeneral income \\n250,000 \\n200,000 \\nIncrease in value \\n15,000 \\n30,000 \\nTotalTotalTotalincome \\n265,000 \\n230,000 \\nAdministrative costs \\nStaff costs \\n(200,000) \\n(150,000) \\nEarly departures \\n(10,000) \\n(20,000) \\nOther operating costs \\n(25,000) \\n(10,000) \\nDepreciation \\n(10,000) \\n(10,000) \\nProgramme costs \\nImpairment loss \\n(10,000) \\n(5,000) \\nOther \\n(5,000) \\n(5,000) \\nTotal  costsTotal  costsTotal  costsTotal  costsTotal  costs\\n(260,000) \\n(200,000) \\nSurplus \\n5,000 \\n30,000 \\n\\nTable 12: merged data cells are not recommended \\n\\n2008 \\n2009 \\nName \\nYes \\nNo \\nYes \\nNo \\nBob \\n2 \\n5 \\n6 \\n7 \\nSue \\n3 \\n8 \\n4 \\n7 \\nSam \\n[data relating to both columns in \\na single cell spanning both] \\n[data relating to both columns in \\na single cell spanning both] \\nTable 13: use of graphic symbols \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\n\\uf0fd \\n\\uf0fc \\n\\uf0fd \\nAre you currently employed? \\n\\uf0fc \\n\\uf0fd \\n\\uf0fc \\nDo you have a driving licence? \\n\\uf0fd \\n\\uf0fd \\n\\uf0fc \\nTable 14: symbols replaced by real text \\nQuestion \\nRespondent A Respondent B \\nRespondent C \\nAre you a UK citizen? \\nNo \\nYes \\nNo \\nAre you currently employed? \\nYes \\nNo \\nYes \\nDo you have a driving licence? \\nNo \\nNo \\nYes \\nTable 15: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, B \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, E \\nPhilosophy \\nA \\nA \\nA \\nA, D \\nPolitics \\nA, D \\nA, D \\nA, D \\nA \\nMathematics \\nB, C \\nB \\nA, E \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nC \\n\\nTable 16:  Masters courses offered by Institution X \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nNo \\nYes \\nYes \\nYes \\nInternational relations \\nNo \\nNo \\nNo \\nNo \\nPhilosophy \\nNo \\nNo \\nNo \\nNo \\nPolitics \\nNo \\nNo \\nNo \\nNo \\nMathematics \\nYes \\nNo \\nNo \\nNo \\nEnglish \\nYes \\nYes \\nYes \\nYes \\nTable 17: accounts, 2011 (£, thousands)  \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n150,000 \\nEarly departures \\n20,000 \\nOther operating costs \\n10,000 \\nDepreciation \\n10,000 \\nProgramme costs Impairment loss \\n10,000 \\nCosts subtotal \\n200,000 \\nBalance \\n30,000 \\nTable 18: accounts, 2011 (£, thousands) \\nAccounting item \\n2011 \\nIncome General income \\n200,000 \\nIncrease in value, WIP \\n30,000 \\nIncome subtotal \\n230,000 \\nAdministrative costs Staff  \\n(150,000) \\nEarly departures \\n(20,000) \\nOther operating costs \\n(10,000) \\nDepreciation \\n(10,000) \\nProgramme costs Impairment loss \\n(10,000) \\nCosts subtotal \\n(200,000) \\nBalance \\n30,000 \\nTable 19: Human Development Index (HDI)  \\ntrends, 1980 to 2010. Source: Barro-Lee March, 2010  \\nCountry \\n1980 \\n1990 \\n2000 \\n2010 \\nAfghanistan \\n0.78 \\n1.48 \\n2.16 \\n3.33 \\nAlbania \\n8.89 \\n9.67 \\n9.89 \\n10.38 \\nAlgeria \\n4.74 \\n3.33 \\n5.50 \\n7.24 \\nAndorra \\n4.98 \\n5.63 \\n9.09 \\n10.35 \\nAngola \\n- \\n- \\n4.42 \\n4.42 \\nTable 20: footnotes referenced from within a table \\nExpenditure by function £million \\n2009/10 \\n2010/11\\xa01 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation\\xa02 \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services 3 \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\n\\n(1) Provisional total as of publication date. \\n(2) Costs associated with on-going information programmes. \\n(3) From the management accounts, net of recoveries and including interest charges \\nTable 21: footnotes replaced by additional table summary text \\nExpenditure by function £million \\n2009/10 \\n2010/11 \\nPolicy functions \\nFinancial \\n22.5 \\n30.57 \\nInformation \\n10.2 \\n14.8 \\nContingency \\n2.6 \\n1.2 \\nRemunerated functions \\nAgency services  \\n44.7 \\n35.91 \\nPayments \\n22.41 \\n19.88 \\nBanking \\n22.90 \\n44.23 \\nOther \\n12.69 \\n10.32 \\nTable 22: referencing multiple endnotes from within a table \\nExpenditure £m \\nNotes  \\n(Notes located on  page [n]) \\n2010 \\n2011 \\nInformation \\n1 \\n10.2 \\n14.8 \\nContingency \\n\\n2.6 \\n1.2 \\nPayments \\n3 \\n22.41 \\n19.88 \\nBanking services \\n4 \\n22.90 \\n44.23 \\nInterest \\n\\n0.23 \\n0.10 \\nDividends \\n23 \\n2.5 \\n3.68 \\nOther \\n9 \\n12.69 \\n10.32 \\nTable 23: simulated table created using tabs and containing no \\nstructure \\n\\n2008 \\n2009 \\n\\nName  \\nEntered \\nCompleted \\nEntered \\nCompleted \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\n\\nTable 24: year-end financial statement (£, thousands) \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\nTable 25: setting column and row scope via the tags panel  \\n\\n2008 \\n2009 \\nName \\nEntered \\nWon \\nEntered \\nWon \\nBob \\n22 \\n21 \\n20 \\n19 \\nSue \\n44 \\n12 \\n12 \\n10 \\nSam \\n16 \\n4 \\n45 \\n30 \\nTable 26: courses offered by Institution X. A = Bachelor of Science,  \\nB = Bachelor of Arts, C = Masters, D = Doctorate, E = Diploma \\n\\n2006 \\n2007 \\n2008 \\n2009 \\nEconomics \\nA, B \\nA, C \\nA, C \\nA, C \\nInternational relations \\nA, E \\nA, E \\nA, B \\nA, B \\nPhilosophy \\nA \\nA \\nA \\nA \\nPolitics \\nA, D \\nA, D \\nA, B \\nA \\nMathematics \\nB, C \\nB \\nA, B \\nA, B \\nEnglish \\nA, C \\nA, B \\nA,B \\nA, C \\n\\nTable 27: “table” with columns simulated by using tab stops \\nName \\nApples \\nPears \\nBob Scott \\n20 \\n25 \\nSusan. P. Arnold-Jones, BA, FRSA, MD \\n24 \\n15 \\nSam Holder-Dickinson \\n14 \\n10 \\nTable 28: year-end financial table (£, thousands) – headings problem \\nrevisited \\n\\n2010 \\n2009 \\n2008 \\nNon-current assets \\nBuildings \\n345 \\n445 \\n222 \\nInvestment \\n567 \\n654 \\n423 \\nIntangibles \\n423 \\n123 \\n453 \\nCurrent assets \\nTrade \\n435 \\n634 \\n231 \\nCash \\n524 \\n123 \\n482 \\nOther \\n223 \\n211 \\n254 \\nCurrent liabilities \\nTrade liabilities \\n154 \\n125 \\n421 \\nFinancial debt \\n231 \\n474 \\n572 \\nProvisions \\n111 \\n312 \\n347 \\n\\nTable 29: multiple headers attributes for each data cell \\n\\nSouth America \\nAsia \\nAfrica \\nAustralia \\n2010 \\nHighest average \\n523.6 \\n467.4 \\n405.0 \\n340.5 \\nHighest in 24 hours \\n73.1 \\n54.1 \\n27.2 \\n66.3 \\nHighest in 12 hours \\n42.4 \\n30.1 \\n15.9 \\n40.3 \\n2009 \\nHighest average \\n487.7 \\n453.6 \\n398.7 \\n356 \\nHighest in 24 hours \\n67.2 \\n53.2 \\n44.3 \\n53.8 \\nHighest in 12 hours \\n34.7 \\n34.1 \\n29.8 \\n31.0 \\n2008 \\nHighest average \\n496.7 \\n444.3 \\n502.1 \\n399.6 \\nHighest in 24 hours \\n44.2 \\n56.7 \\n32.1 \\n63.2 \\nHighest in 12 hours \\n30.1 \\n32.7 \\n21.9 \\n40.2 \\n\\n\\n'}, 'MENTIONS', {'id': 'Aspirin'}), 'm': {'id': 'Aspirin'}}\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Neo4j credentials\n",
    "NEO4J_URI = \"neo4j+s://ffa05957.databases.neo4j.io\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"KYDXnv3miiiEFXu9p1119eo77ugFg_gcBqFxLlVD1h4\"\n",
    "# Create a class to interact with Neo4j\n",
    "class Neo4jQueryTool:\n",
    "    def __init__(self, uri, username, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def run_query(self, query):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(query)\n",
    "            return [record.data() for record in result]\n",
    "\n",
    "# Create an instance of the query tool\n",
    "tool = Neo4jQueryTool(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "\n",
    "# Test the tool with a sample query\n",
    "try:\n",
    "    print(\"Testing query tool...\")\n",
    "    \n",
    "    # Replace with your test query\n",
    "    sample_query = \"\"\"\n",
    "        MATCH (n)-[r]->(m)\n",
    "        RETURN n, r, m\n",
    "        LIMIT 10\n",
    "    \"\"\"\n",
    "    \n",
    "    results = tool.run_query(sample_query)\n",
    "    for record in results:\n",
    "        print(record)\n",
    "finally:\n",
    "    tool.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2522ed43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (d:Document)-[:MENTIONS]->(p:Person)\n",
      "RETURN p.id AS PersonId, count(d) AS MentionCount\n",
      "ORDER BY MentionCount DESC\n",
      "LIMIT 1\n",
      "\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'PersonId': 'Adithya A', 'MentionCount': 1}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Adithya A is the person mentioned most.\n"
     ]
    }
   ],
   "source": [
    "from langchain_neo4j import GraphCypherQAChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Initialize the Gemini model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "\n",
    "# Assuming 'graph' is your Neo4jGraph instance\n",
    "qa_chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True  # Explicitly allow dangerous requests\n",
    ")\n",
    "\n",
    "# Now you can run your query\n",
    "\n",
    "response = qa_chain.run(\"who person most\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f079e608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding model...\n",
      "Embedding model initialized.\n",
      "Preparing documents and creating FAISS vector store...\n",
      "FAISS vector store created successfully.\n",
      "Creating RAG chain...\n",
      "RAG chain created.\n"
     ]
    }
   ],
   "source": [
    "# rag_faiss_system.py\n",
    "\n",
    "# --- Dependencies ---\n",
    "# Make sure you have installed the necessary packages:\n",
    "# pip install langchain faiss-cpu sentence-transformers langchain-google-genai python-dotenv\n",
    "# (or faiss-gpu if you have CUDA installed and want GPU acceleration)\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS # Use langchain_community\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings # Use langchain_community\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # Import Gemini\n",
    "from langchain.docstore.document import Document # For structured documents\n",
    "\n",
    "# --- Configuration ---\n",
    "# Load environment variables (especially GOOGLE_API_KEY)\n",
    "load_dotenv() \n",
    "\n",
    "# Check if the API key is available\n",
    "if os.getenv(\"GOOGLE_API_KEY\") is None:\n",
    "    print(\"Error: GOOGLE_API_KEY environment variable not set.\")\n",
    "    print(\"Please get an API key from Google AI Studio (https://aistudio.google.com/app/apikey)\")\n",
    "    print(\"and set it as an environment variable (e.g., in a .env file).\")\n",
    "    exit() # Exit if the key is missing\n",
    "\n",
    "# --- Main Function ---\n",
    "# Step 1: Initialize the embedding model\n",
    "print(\"Initializing embedding model...\")\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'} # Specify device (cpu or cuda)\n",
    "    # Set encode_kwargs={'normalize_embeddings': True} if using cosine similarity,\n",
    "    # but FAISS IndexFlatL2 uses L2 distance (Euclidean), so normalization is optional \n",
    "    # but often helpful. Let's keep it simple for L2.\n",
    ")\n",
    "print(\"Embedding model initialized.\")\n",
    "\n",
    "# Step 2 & 3: Prepare documents and create FAISS index\n",
    "print(\"Preparing documents and creating FAISS vector store...\")\n",
    "documents_data = [\n",
    "    {\"content\": \"\"\"\n",
    "        Neuro Prune: An Adaptive Approach for Efficient\n",
    "Deep Neural Network Optimization on Edge\n",
    "Devices\n",
    "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
    "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
    "grk.cse@psgtech.ac.in\n",
    "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
    "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
    "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
    "Abstract—The optimization of deep neural networks for de-\n",
    "ployment on edge devices is a significant research area due to\n",
    "the demand for applications such as augmented reality, smart\n",
    "cameras, and autonomous navigation. However, deploying large\n",
    "deep learning models on edge devices poses challenges related to\n",
    "computational power, energy consumption, and latency. Pruning\n",
    "is a method to reduce the model size, accelerate inference, and\n",
    "save power. The objective of the paper is to propose the Neuro\n",
    "Prune algorithm and to apply it for the optimization of deep\n",
    "neural networks on edge devices. Efforts have been made to\n",
    "compare pruned and unpruned models. As a result, the pruned\n",
    "model has an accuracy increase of 0.22%.\n",
    "I. INTRODUCTION\n",
    "The rise of intelligent systems, ranging from autonomous\n",
    "drones to augmented reality (AR) devices and smart surveil-\n",
    "lance cameras, has intensified the need for efficient deep neural\n",
    "network deployment on edge devices with constrained com-\n",
    "putational resources. While advanced deep learning models\n",
    "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
    "their deployment on such devices is hindered by significant\n",
    "computational demands, large memory footprints, and high\n",
    "energy consumption. These challenges pose critical barriers to\n",
    "real-time applications and sustainable deployment.\n",
    "This paper builds on existing pruning techniques, includ-\n",
    "ing unstructured pruning, structured pruning, and approaches\n",
    "inspired by quantization, highlighting their strengths and lim-\n",
    "itations. In response to the challenges identified, we introduce\n",
    "Neuro Prune, a novel pruning approach that integrates princi-\n",
    "ples from reinforcement learning and the Lottery Ticket Hy-\n",
    "pothesis to achieve an optimal balance between accuracy, com-\n",
    "putational efficiency, and energy consumption. By integrating\n",
    "these principles, Neuro Prune provides a robust framework\n",
    "for optimizing deep neural networks for deployment on edge\n",
    "devices without sacrificing performance.\n",
    "Neuro Prune reduces model size and computational re-\n",
    "quirements while maintaining or even improving the model’s\n",
    "accuracy. These findings underscore the potential of Neuro\n",
    "Prune to enable real-time, energy-efficient deep learning on\n",
    "resource-constrained devices.\n",
    "Neuro Prune employs a systematic process to optimize\n",
    "neural networks for edge deployment:\n",
    "• Mask Initialization: Each layer’s weights are paired with\n",
    "a binary mask (original_mask), initialized to ones.\n",
    "• Activation Tracking: During forward passes over the\n",
    "dataset, the magnitudes of weight activations are mon-\n",
    "itored and recorded.\n",
    "• Normalize and Prune: The recorded activations are\n",
    "normalized, and a threshold based on the pruning fraction\n",
    "is computed. Using this threshold, the masks are dynam-\n",
    "ically updated to prune unimportant weights.\n",
    "• Evaluate and Log Metrics: After pruning, metrics such\n",
    "as model sparsity and accuracy are computed. A reward\n",
    "metric is derived to balance accuracy and sparsity using\n",
    "a tunable parameter, λweight.\n",
    "• Retrain the Model: The pruned model undergoes retrain-\n",
    "ing to improve the loss in accuracy while maintaining low\n",
    "computational requirements\n",
    "Neuro Prune’s unique combination of reinforcement learn-\n",
    "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
    "pothesis for identifying optimal subnetworks enables it to\n",
    "achieve remarkable efficiency. This approach ensures substan-\n",
    "tial reduction in model size and energy consumption while\n",
    "preserving accuracy and inference speed, making it an ideal\n",
    "solution for edge-based deep learning applications.\n",
    "Through extensive empirical evaluations and theoretical\n",
    "insights, this paper demonstrates how Neuro Pruning outper-\n",
    "forms traditional pruning methods, setting a new benchmark\n",
    "for sustainable, high-performance AI deployment on edge\n",
    "devices. By situating our work within the broader research\n",
    "context, we aim to offer a critical contribution to the ongoing\n",
    "evolution of neural network optimization techniques.\n",
    "II. BACKGROUND\n",
    "A. Need for Pruning Techniques\n",
    "Pruning is a crucial optimization technique for deploying\n",
    "deep neural networks in resource-constrained environments\n",
    "like healthcare. For example, wearable devices like smart-\n",
    "watches that monitor heart rates or portable EEG systems\n",
    "for brain activity analysis require lightweight models due to\n",
    "their limited computational power and memory. Pruning helps\n",
    "address these constraints by reducing the memory footprint\n",
    "and computational load, enabling efficient model deployment.\n",
    "Additionally, it minimizes energy consumption, which is vital\n",
    "for sustainable and real-time healthcare applications. Faster\n",
    "inference enabled by pruning is particularly beneficial for\n",
    "time-sensitive tasks like seizure detection or emergency di-\n",
    "agnostics. Moreover, pruning enhances model interpretability\n",
    "by retaining only the most critical components, which is\n",
    "essential for gaining trust and ensuring regulatory compliance\n",
    "in healthcare. By reducing the hardware and operational costs,\n",
    "pruning also makes deploying advanced AI solutions on edge\n",
    "devices more cost-effective and accessible.\n",
    "B. Types of Pruning Techniques\n",
    "Pruning methods are broadly classified based on granu-\n",
    "larity, timing, and approach. Granularity-based pruning in-\n",
    "cludes structured pruning, which removes entire components\n",
    "such as neurons or filters to streamline computations, and\n",
    "unstructured pruning, which eliminates individual weights\n",
    "for finer optimization, though it often requires specialized\n",
    "hardware. Timing-based pruning involves pre-training pruning,\n",
    "which optimizes models before training; post-training prun-\n",
    "ing, where redundant components are removed from trained\n",
    "models with possible retraining; and dynamic pruning during\n",
    "training, which adjusts models in real-time for improved\n",
    "adaptability. Metric-based pruning uses parameters like weight\n",
    "magnitude or gradient contribution to identify and remove\n",
    "less critical components, while application-specific pruning\n",
    "tailors strategies to specific tasks, such as optimizing for EEG\n",
    "signal analysis, or hardware constraints like GPUs. Together,\n",
    "these methods enable the development of efficient, scalable,\n",
    "and task-specific models, particularly suited for edge device\n",
    "deployment in healthcare and other domains.\n",
    "III. RELATED WORK\n",
    "Jielei Wang et al.[12] introduced an absorption pruning\n",
    "method for object detection in remote sensing imagery. This\n",
    "study achieves efficient compression with minimal accuracy\n",
    "loss but requires careful layer-wise pruning ratio tuning.\n",
    "Jan Muller et al., in their research[12], propose a neural\n",
    "network pruning method for multi-object tracking (MOT) that\n",
    "reduces model size by up to 70\n",
    "Liang Li et al., in their work[9], introduce a novel pruning\n",
    "method for DNNs that utilizes a self-adaptive mechanism\n",
    "based on weight sparsity ratios and a protective reconstruction\n",
    "mechanism. Their approach improves both model compres-\n",
    "sion and accuracy, outperforming state-of-the-art methods on\n",
    "CIFAR-10 and ImageNet datasets.\n",
    "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
    "learning with edge devices like Google Coral AI and Nvidia\n",
    "Jetson Nano. Their work revolutionizes computer vision and\n",
    "real-time tracking but raises energy concerns due to the\n",
    "computational demands of advanced algorithms.\n",
    "Satoru Koda et al.[7] explore how weight pruning in\n",
    "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
    "larly with Mahalanobis-based approaches, by improving global\n",
    "feature extraction and leveraging weights not critical for\n",
    "classification.\n",
    "Shvetha S Kumar et al. analyze and compare three pruning\n",
    "techniques—L1-norm filter pruning, channel pruning, and\n",
    "weight pruning—on CNNs for accuracy and inference time,\n",
    "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
    "pared to GTX1080 Ti in their study[8].\n",
    "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
    "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
    "on stochastic networks, extending it to related problems and\n",
    "demonstrating its efficiency on real-world networks in their\n",
    "study[13].\n",
    "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
    "Training outperforms Post-Training Quantization, and sparsity\n",
    "training with Optimal Reduction pruning improves YOLOv4’s\n",
    "efficiency and edge-device performance, despite potential\n",
    "high-pruning rate drawbacks.\n",
    "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
    "method for DNNs. This approach optimizes inference speed\n",
    "and resource efficiency on edge devices with minimal accuracy\n",
    "loss but requires careful tuning of layer-specific pruning ratios.\n",
    "Yongqi An et al.[1] introduce a retraining-free structured\n",
    "pruning framework for Large Language Models (LLMs). Their\n",
    "method incorporates a fluctuation-based pruning metric, adap-\n",
    "tive structure search, and a bias compensation mechanism to\n",
    "achieve efficient pruning. FLAP significantly enhances infer-\n",
    "ence speed and reduces model size without requiring retrain-\n",
    "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
    "and Wanda-sp on LLaMA models across various benchmarks.\n",
    "From referring to these research papers, we derived an\n",
    "idea for pruning on edge devices. Additionally, our proposed\n",
    "solution is inspired by the research of Jaron Maene et al.[10],\n",
    "which suggests that sparse subnetworks in dense networks can\n",
    "achieve similar accuracy when retrained. This study demon-\n",
    "strates stable training with linear mode connectivity, support-\n",
    "ing the idea that lottery tickets retrain to similar regions, but\n",
    "questions their independence from dense training and iterative\n",
    "pruning.\n",
    "Eran Malach et al.[11] further strengthen the lottery ticket\n",
    "hypothesis by proving that over-parameterized neural networks\n",
    "with random weights always contain a subnetwork matching\n",
    "the accuracy of a target network, without additional training.\n",
    "IV. NEUROLOGICAL PRUNING APPROACH\n",
    "The Neurological Pruning Algorithm is designed to opti-\n",
    "mize neural networks by iteratively pruning less important\n",
    "weights while maintaining high performance. Unlike tradi-\n",
    "tional methods, this approach incorporates adaptive, layer-\n",
    "specific pruning thresholds that dynamically adjust based\n",
    "on weight activations, ensuring efficient model optimization\n",
    "across various stages of training. Drawing inspiration from\n",
    "the Lottery Ticket Hypothesis, which suggests that sparse\n",
    "subnetworks can perform similarly to their dense counterparts\n",
    "when retrained, the algorithm selectively deactivates weights\n",
    "using binary masks to enhance computational efficiency with-\n",
    "out sacrificing accuracy.\n",
    "The algorithm operates in several epochs, during which\n",
    "weight activations are evaluated. Based on a quantile-based\n",
    "strategy, pruning thresholds are established for each layer, en-\n",
    "suring that only weights contributing minimally to the model’s\n",
    "output are removed. After pruning, the model undergoes fine-\n",
    "tuning to restore any lost accuracy and further optimize the\n",
    "network. This step enhances the adaptability and scalability\n",
    "of the pruning method, making it suitable for deployment in\n",
    "resource-constrained environments\n",
    "A. Key Characteristics and Advantages\n",
    "The Neurological Pruning Algorithm boasts several key\n",
    "characteristics that distinguish it from conventional pruning\n",
    "techniques. One notable feature is its layer-specific pruning,\n",
    "which evaluates the importance of weights individually for\n",
    "each layer, leading to a more nuanced and efficient sparsifi-\n",
    "cation process. This approach prevents the model from losing\n",
    "essential features that may otherwise be pruned by a global\n",
    "approach.\n",
    "The algorithm’s use of quantile-based thresholding en-\n",
    "sures that only weights with the least contribution to the\n",
    "model’s output are pruned. This fine-tuned method avoids\n",
    "performance degradation by carefully targeting weights that\n",
    "are less significant to the network’s overall functionality.\n",
    "Furthermore, the pruning process is iterative, allowing for\n",
    "continuous model evaluation and real-time adjustments based\n",
    "on performance feedback, ensuring that accuracy is retained\n",
    "even as sparsity increases.\n",
    "Additionally, the algorithm maintains a careful balance\n",
    "between sparsity and accuracy, enabling a reduction in the\n",
    "model’s computational complexity while preserving its predic-\n",
    "tive capabilities. This characteristic is particularly important\n",
    "for applications requiring rapid inference times, such as in\n",
    "real-time healthcare systems and edge computing scenarios,\n",
    "where both energy efficiency and computational power are\n",
    "crucial.\n",
    "B. Impact on Model Performance\n",
    "The Neurological Pruning Algorithm significantly improves\n",
    "the efficiency of neural networks, making them more suitable\n",
    "for deployment on edge devices. Key performance metrics\n",
    "such as compression ratio (CR) and accuracy retention\n",
    "(AR) are critical indicators of the algorithm’s effectiveness.\n",
    "For instance, in evaluating a ResNet-18 model, the algorithm\n",
    "achieved a CR of 1, indicating that while the number of\n",
    "parameters remained the same, the model’s efficiency im-\n",
    "proved through selective pruning, reducing computational load\n",
    "without sacrificing performance.\n",
    "Moreover, the algorithm demonstrates the ability to achieve\n",
    "high accuracy retention, with models maintaining or even\n",
    "slightly improving their predictive performance post-pruning.\n",
    "This outcome highlights the algorithm’s capability to effi-\n",
    "ciently remove redundant weights while preserving the core\n",
    "functionality of the network. The result is a model that is\n",
    "not only more computationally efficient but also adaptable\n",
    "across different architectures and datasets, making it suit-\n",
    "able for a wide range of applications, from image classification\n",
    "to natural language processing .\n",
    "In practical terms, the Neurological Pruning Algorithm en-\n",
    "ables faster inference times and lower energy consumption,\n",
    "two crucial aspects for deploying AI models in resource-\n",
    "constrained environments. As such, it holds significant poten-\n",
    "tial for improving the viability of real-time, energy-efficient\n",
    "AI applications, particularly in fields like healthcare, where\n",
    "both speed and sustainability are essential.\n",
    "V. NEUROLOGICAL PRUNING ALGORITHM\n",
    "The Neurological Pruning Algorithm is a structured and\n",
    "adaptive methodology designed to optimize neural networks\n",
    "for resource-constrained environments. This algorithm goes\n",
    "beyond traditional pruning approaches by incorporating layer-\n",
    "specific pruning thresholds that dynamically adjust to the\n",
    "importance of weight activations within each layer. Drawing\n",
    "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
    "inforcement learning, the algorithm strikes an optimal balance\n",
    "between model sparsity and accuracy .\n",
    "A. Algorithm Description\n",
    "The Neurological Pruning Algorithm iteratively refines the\n",
    "neural network through a series of training epochs. During\n",
    "each epoch, weight activations are tracked, normalized, and\n",
    "analyzed to identify low-importance weights. These weights\n",
    "are selectively deactivated using binary masks, ensuring com-\n",
    "putational efficiency without compromising the network’s pre-\n",
    "dictive performance. The algorithm further integrates a fine-\n",
    "tuning phase, enabling the pruned network to recover and\n",
    "enhance its accuracy. The detailed process is presented in\n",
    "Algorithm 1.\n",
    "By dynamically adjusting to layer-specific characteristics\n",
    "and maintaining performance metrics, the Neurological Prun-\n",
    "ing Algorithm ensures a significant reduction in computational\n",
    "and memory requirements, paving the way for efficient deploy-\n",
    "ment of deep learning models on edge devices .\n",
    "B. Input Parameters and Output\n",
    "Inputs:\n",
    "• Pretrained model (M)\n",
    "• Dataset (D)\n",
    "• Prune fraction (p)\n",
    "• Total epochs (E)\n",
    "Output:\n",
    "• Pruned and fine-tuned model (Mpruned)\n",
    "C. Key Features and Advantages\n",
    "The Neurological Pruning Algorithm is designed with sev-\n",
    "eral distinctive features that contribute to its effectiveness\n",
    "in optimizing neural network performance. These features\n",
    "include:\n",
    "Algorithm 1 Simplified Neurological Pruning Algorithm\n",
    "Require: Pretrained model M, Dataset D, Prune fraction p,\n",
    "Total epochs E\n",
    "Ensure: Pruned and fine-tuned model Mpruned\n",
    "1: Initialize binary masks M with all ones\n",
    "2: for each epoch e = 1 to E do\n",
    "3:\n",
    "Compute weight activations A during forward passes\n",
    "4:\n",
    "for each layer i in M do\n",
    "5:\n",
    "Compute pruning threshold Ti as p-quantile of Ai\n",
    "6:\n",
    "Update mask Mi: retain weights with Ai > Ti\n",
    "7:\n",
    "Apply mask to layer: θi ←θi · Mi\n",
    "8:\n",
    "end for\n",
    "9:\n",
    "Evaluate model performance and update metrics\n",
    "10: end for\n",
    "11: Fine-tune Mpruned on D\n",
    "12: return Mpruned\n",
    "• Layer-specific Pruning: The algorithm applies pruning\n",
    "on a per-layer basis, evaluating the weight activations\n",
    "within each layer individually. This approach ensures that\n",
    "the sparsity introduced by pruning is well-distributed and\n",
    "context-dependent, facilitating more nuanced control over\n",
    "model optimization .\n",
    "• Quantile-based Thresholding: A quantile-based strat-\n",
    "egy is employed to determine the pruning threshold for\n",
    "each layer. This method ensures that only weights with\n",
    "minimal contribution to the network’s output are pruned,\n",
    "thereby reducing the risk of adversely affecting model\n",
    "accuracy while enhancing sparsity .\n",
    "• Iterative Pruning and Evaluation: The pruning pro-\n",
    "cess follows an iterative approach, enabling continuous\n",
    "assessment of the model’s performance after each pruning\n",
    "step. This ensures dynamic adjustments to maintain a bal-\n",
    "ance between model sparsity and predictive performance\n",
    "throughout training.\n",
    "• Preservation of Model Accuracy: By adjusting pruning\n",
    "thresholds in response to ongoing performance metrics,\n",
    "the algorithm ensures that model accuracy is not com-\n",
    "promised. The careful tuning of these thresholds allows\n",
    "for significant sparsity while maintaining the robustness\n",
    "of the network’s predictive capabilities .\n",
    "D. Impact on Neural Network Performance\n",
    "The Neurological Pruning Algorithm has been extensively\n",
    "evaluated across various benchmark datasets and model archi-\n",
    "tectures, demonstrating its capacity to achieve the following\n",
    "outcomes:\n",
    "• High Sparsity with Minimal Accuracy Loss: The algo-\n",
    "rithm effectively prunes unnecessary weights, achieving\n",
    "high sparsity levels without causing significant degrada-\n",
    "tion in model accuracy. This outcome is critical for the\n",
    "efficient deployment of models in resource-constrained\n",
    "environments.\n",
    "• Reduced Computational Complexity: By sparsifying\n",
    "the weight matrices, the algorithm reduces the compu-\n",
    "tational load during inference, which translates to faster\n",
    "model execution and lower resource consumption. This\n",
    "characteristic is especially beneficial in edge computing\n",
    "and real-time applications .\n",
    "• Broad Applicability: The algorithm is highly flexible\n",
    "and can be adapted to various neural network architec-\n",
    "tures and datasets, making it suitable for a wide range\n",
    "of tasks, from image recognition to natural language\n",
    "processing .\n",
    "VI. EVALUATION METRICS\n",
    "A. Performance Comparison\n",
    "Fig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\n",
    "TABLE I\n",
    "COMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\n",
    "Metric\n",
    "Unpruned\n",
    "Model\n",
    "Pruned\n",
    "Model\n",
    "Accuracy\n",
    "0.9562\n",
    "0.9583\n",
    "Train Loss\n",
    "0.1414\n",
    "0.1625\n",
    "Inference\n",
    "Time\n",
    "(s)\n",
    "0.0318\n",
    "0.0314\n",
    "No of Parame-\n",
    "ters(million)\n",
    "11.7\n",
    "11.7\n",
    "B. Evaluation Metrics Calculation\n",
    "The key metrics used to evaluate the proposed pruning\n",
    "technique are as follows:\n",
    "Compression Ratio (CR): The Compression Ratio (CR) is\n",
    "calculated as:\n",
    "CR = Total Parameters After Pruning\n",
    "Total Parameters Before Pruning\n",
    "(1)\n",
    "CR = 11.7M\n",
    "11.7M = 1\n",
    "Accuracy Retention (AR): The Accuracy Retention (AR) is\n",
    "calculated as:\n",
    "AR =\n",
    "\u0012Accuracy After Pruning −Baseline Accuracy\n",
    "Baseline Accuracy\n",
    "\u0013\n",
    "×100\n",
    "(2)\n",
    "AR =\n",
    "\u00120.958284 −0.956206\n",
    "0.956206\n",
    "\u0013\n",
    "× 100 = 0.22%\n",
    "This result indicates that the pruned model achieves an ac-\n",
    "curacy improvement of 0.22% when compared to unpruned\n",
    "model.\n",
    "C. Illustration of Results\n",
    "Fig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\n",
    "18.\n",
    "D. Discussion\n",
    "The evaluation metrics highlight the effectiveness of the\n",
    "proposed Neurological Pruning method on the ResNet-18\n",
    "model. The Compression Ratio (CR) remains 1, indicating\n",
    "no reduction in the total number of parameters, which suggests\n",
    "that this approach optimizes the neural network by selectively\n",
    "pruning connections rather than reducing the overall parameter\n",
    "count. The Accuracy Retention (AR), which is approximately\n",
    "0.22%, demonstrates that the Neurological Pruning technique\n",
    "successfully improved the accuracy compared to the baseline\n",
    "model.\n",
    "These results validate the potential of Neurological Pruning\n",
    "as an innovative approach to optimizing deep learning models.\n",
    "By maintaining accuracy while likely enhancing sparsity and\n",
    "computational efficiency, this method presents a promising di-\n",
    "rection for achieving high-performance models under resource\n",
    "constraints.\n",
    "VII. CONCLUSION\n",
    "In summary, the Neurological Pruning Algorithm provides\n",
    "a systematic and principled approach to optimizing neural\n",
    "networks. By balancing the trade-off between sparsity and\n",
    "accuracy, it enables significant reductions in computational\n",
    "complexity without sacrificing the network’s predictive per-\n",
    "formance. Future work will focus on extending the algorithm\n",
    "to dynamic network architectures and incorporating advanced\n",
    "regularization techniques to further enhance its robustness and\n",
    "adaptability in diverse applications.\n",
    "REFERENCES\n",
    "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\n",
    "Fluctuation-based Adaptive Structured Pruning for Large Language\n",
    "Models.\n",
    "[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\n",
    "Simplification of Deep Neural Network-Based Object Detector for Real-\n",
    "Time Edge Computing.\n",
    "[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\n",
    "Devices Object Detection by Filter Pruning.\n",
    "[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\n",
    "and Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\n",
    "Tracking Algorithms in Edge Devices.\n",
    "[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\n",
    "Chen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\n",
    "Compression Method for Object Detection Network for Edge Devices.\n",
    "[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\n",
    "Cao. Absorption Pruning of Deep Neural Network for Object Detection\n",
    "in Remote Sensing Imagery.\n",
    "[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\n",
    "and Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\n",
    "Distribution Detection: An Empirical Survey.\n",
    "[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\n",
    "Sahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\n",
    "[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\n",
    "Compress DNNs.\n",
    "[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\n",
    "derstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\n",
    "[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\n",
    "Proving the Lottery Ticket Hypothesis: Pruning is All You Need.\n",
    "[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\n",
    "Devices via Reconstruction-Based Channel Pruning.\n",
    "[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\n",
    "Determine Reliable Paths on Networks with Random and Correlated\n",
    "Link Travel Times.\n",
    "[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\n",
    "and D. Suganthi. Real-Time Object Detection on Edge Devices Using\n",
    "Mobile Neural Networks.\n",
    "[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\n",
    "Tang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\n",
    "Models.\n",
    "[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\n",
    "Li, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\n",
    "Deng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\n",
    "Detection of Tea Leaves with One Bud and Two Leaves Based on\n",
    "ShuffleNetv2-YOLOv5-Lite-E.\n",
    "Neuro Prune: An Adaptive Approach for Efficient\n",
    "Deep Neural Network Optimization on Edge\n",
    "Devices\n",
    "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
    "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
    "grk.cse@psgtech.ac.in\n",
    "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
    "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
    "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
    "Abstract—The optimization of deep neural networks for de-\n",
    "ployment on edge devices is a significant research area due to\n",
    "the demand for applications such as augmented reality, smart\n",
    "cameras, and autonomous navigation. However, deploying large\n",
    "deep learning models on edge devices poses challenges related to\n",
    "computational power, energy consumption, and latency. Pruning\n",
    "is a method to reduce the model size, accelerate inference, and\n",
    "save power. The objective of the paper is to propose the Neuro\n",
    "Prune algorithm and to apply it for the optimization of deep\n",
    "neural networks on edge devices. Efforts have been made to\n",
    "compare pruned and unpruned models. As a result, the pruned\n",
    "model has an accuracy increase of 0.22%.\n",
    "I. INTRODUCTION\n",
    "The rise of intelligent systems, ranging from autonomous\n",
    "drones to augmented reality (AR) devices and smart surveil-\n",
    "lance cameras, has intensified the need for efficient deep neural\n",
    "network deployment on edge devices with constrained com-\n",
    "putational resources. While advanced deep learning models\n",
    "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
    "their deployment on such devices is hindered by significant\n",
    "computational demands, large memory footprints, and high\n",
    "energy consumption. These challenges pose critical barriers to\n",
    "real-time applications and sustainable deployment.\n",
    "This paper builds on existing pruning techniques, includ-\n",
    "ing unstructured pruning, structured pruning, and approaches\n",
    "inspired by quantization, highlighting their strengths and lim-\n",
    "itations. In response to the challenges identified, we introduce\n",
    "Neuro Prune, a novel pruning approach that integrates princi-\n",
    "ples from reinforcement learning and the Lottery Ticket Hy-\n",
    "pothesis to achieve an optimal balance between accuracy, com-\n",
    "putational efficiency, and energy consumption. By integrating\n",
    "these principles, Neuro Prune provides a robust framework\n",
    "for optimizing deep neural networks for deployment on edge\n",
    "devices without sacrificing performance.\n",
    "Neuro Prune reduces model size and computational re-\n",
    "quirements while maintaining or even improving the model’s\n",
    "accuracy. These findings underscore the potential of Neuro\n",
    "Prune to enable real-time, energy-efficient deep learning on\n",
    "resource-constrained devices.\n",
    "Neuro Prune employs a systematic process to optimize\n",
    "neural networks for edge deployment:\n",
    "• Mask Initialization: Each layer’s weights are paired with\n",
    "a binary mask (original_mask), initialized to ones.\n",
    "• Activation Tracking: During forward passes over the\n",
    "dataset, the magnitudes of weight activations are mon-\n",
    "itored and recorded.\n",
    "• Normalize and Prune: The recorded activations are\n",
    "normalized, and a threshold based on the pruning fraction\n",
    "is computed. Using this threshold, the masks are dynam-\n",
    "ically updated to prune unimportant weights.\n",
    "• Evaluate and Log Metrics: After pruning, metrics such\n",
    "as model sparsity and accuracy are computed. A reward\n",
    "metric is derived to balance accuracy and sparsity using\n",
    "a tunable parameter, λweight.\n",
    "• Retrain the Model: The pruned model undergoes retrain-\n",
    "ing to improve the loss in accuracy while maintaining low\n",
    "computational requirements\n",
    "Neuro Prune’s unique combination of reinforcement learn-\n",
    "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
    "pothesis for identifying optimal subnetworks enables it to\n",
    "achieve remarkable efficiency. This approach ensures substan-\n",
    "tial reduction in model size and energy consumption while\n",
    "preserving accuracy and inference speed, making it an ideal\n",
    "solution for edge-based deep learning applications.\n",
    "Through extensive empirical evaluations and theoretical\n",
    "insights, this paper demonstrates how Neuro Pruning outper-\n",
    "forms traditional pruning methods, setting a new benchmark\n",
    "for sustainable, high-performance AI deployment on edge\n",
    "devices. By situating our work within the broader research\n",
    "context, we aim to offer a critical contribution to the ongoing\n",
    "evolution of neural network optimization techniques.\n",
    "II. BACKGROUND\n",
    "A. Need for Pruning Techniques\n",
    "Pruning is a crucial optimization technique for deploying\n",
    "deep neural networks in resource-constrained environments\n",
    "like healthcare. For example, wearable devices like smart-\n",
    "watches that monitor heart rates or portable EEG systems\n",
    "for brain activity analysis require lightweight models due to\n",
    "their limited computational power and memory. Pruning helps\n",
    "address these constraints by reducing the memory footprint\n",
    "and computational load, enabling efficient model deployment.\n",
    "Additionally, it minimizes energy consumption, which is vital\n",
    "for sustainable and real-time healthcare applications. Faster\n",
    "inference enabled by pruning is particularly beneficial for\n",
    "time-sensitive tasks like seizure detection or emergency di-\n",
    "agnostics. Moreover, pruning enhances model interpretability\n",
    "by retaining only the most critical components, which is\n",
    "essential for gaining trust and ensuring regulatory compliance\n",
    "in healthcare. By reducing the hardware and operational costs,\n",
    "pruning also makes deploying advanced AI solutions on edge\n",
    "devices more cost-effective and accessible.\n",
    "B. Types of Pruning Techniques\n",
    "Pruning methods are broadly classified based on granu-\n",
    "larity, timing, and approach. Granularity-based pruning in-\n",
    "cludes structured pruning, which removes entire components\n",
    "such as neurons or filters to streamline computations, and\n",
    "unstructured pruning, which eliminates individual weights\n",
    "for finer optimization, though it often requires specialized\n",
    "hardware. Timing-based pruning involves pre-training pruning,\n",
    "which optimizes models before training; post-training prun-\n",
    "ing, where redundant components are removed from trained\n",
    "models with possible retraining; and dynamic pruning during\n",
    "training, which adjusts models in real-time for improved\n",
    "adaptability. Metric-based pruning uses parameters like weight\n",
    "magnitude or gradient contribution to identify and remove\n",
    "less critical components, while application-specific pruning\n",
    "tailors strategies to specific tasks, such as optimizing for EEG\n",
    "signal analysis, or hardware constraints like GPUs. Together,\n",
    "these methods enable the development of efficient, scalable,\n",
    "and task-specific models, particularly suited for edge device\n",
    "deployment in healthcare and other domains.\n",
    "III. RELATED WORK\n",
    "Jielei Wang et al.[12] introduced an absorption pruning\n",
    "method for object detection in remote sensing imagery. This\n",
    "study achieves efficient compression with minimal accuracy\n",
    "loss but requires careful layer-wise pruning ratio tuning.\n",
    "Jan Muller et al., in their research[12], propose a neural\n",
    "network pruning method for multi-object tracking (MOT) that\n",
    "reduces model size by up to 70\n",
    "Liang Li et al., in their work[9], introduce a novel pruning\n",
    "method for DNNs that utilizes a self-adaptive mechanism\n",
    "based on weight sparsity ratios and a protective reconstruction\n",
    "mechanism. Their approach improves both model compres-\n",
    "sion and accuracy, outperforming state-of-the-art methods on\n",
    "CIFAR-10 and ImageNet datasets.\n",
    "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
    "learning with edge devices like Google Coral AI and Nvidia\n",
    "Jetson Nano. Their work revolutionizes computer vision and\n",
    "real-time tracking but raises energy concerns due to the\n",
    "computational demands of advanced algorithms.\n",
    "Satoru Koda et al.[7] explore how weight pruning in\n",
    "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
    "larly with Mahalanobis-based approaches, by improving global\n",
    "feature extraction and leveraging weights not critical for\n",
    "classification.\n",
    "Shvetha S Kumar et al. analyze and compare three pruning\n",
    "techniques—L1-norm filter pruning, channel pruning, and\n",
    "weight pruning—on CNNs for accuracy and inference time,\n",
    "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
    "pared to GTX1080 Ti in their study[8].\n",
    "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
    "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
    "on stochastic networks, extending it to related problems and\n",
    "demonstrating its efficiency on real-world networks in their\n",
    "study[13].\n",
    "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
    "Training outperforms Post-Training Quantization, and sparsity\n",
    "training with Optimal Reduction pruning improves YOLOv4’s\n",
    "efficiency and edge-device performance, despite potential\n",
    "high-pruning rate drawbacks.\n",
    "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
    "method for DNNs. This approach optimizes inference speed\n",
    "and resource efficiency on edge devices with minimal accuracy\n",
    "loss but requires careful tuning of layer-specific pruning ratios.\n",
    "Yongqi An et al.[1] introduce a retraining-free structured\n",
    "pruning framework for Large Language Models (LLMs). Their\n",
    "method incorporates a fluctuation-based pruning metric, adap-\n",
    "tive structure search, and a bias compensation mechanism to\n",
    "achieve efficient pruning. FLAP significantly enhances infer-\n",
    "ence speed and reduces model size without requiring retrain-\n",
    "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
    "and Wanda-sp on LLaMA models across various benchmarks.\n",
    "From referring to these research papers, we derived an\n",
    "idea for pruning on edge devices. Additionally, our proposed\n",
    "solution is inspired by the research of Jaron Maene et al.[10],\n",
    "which suggests that sparse subnetworks in dense networks can\n",
    "achieve similar accuracy when retrained. This study demon-\n",
    "strates stable training with linear mode connectivity, support-\n",
    "ing the idea that lottery tickets retrain to similar regions, but\n",
    "questions their independence from dense training and iterative\n",
    "pruning.\n",
    "Eran Malach et al.[11] further strengthen the lottery ticket\n",
    "hypothesis by proving that over-parameterized neural networks\n",
    "with random weights always contain a subnetwork matching\n",
    "the accuracy of a target network, without additional training.\n",
    "IV. NEUROLOGICAL PRUNING APPROACH\n",
    "The Neurological Pruning Algorithm is designed to opti-\n",
    "mize neural networks by iteratively pruning less important\n",
    "weights while maintaining high performance. Unlike tradi-\n",
    "tional methods, this approach incorporates adaptive, layer-\n",
    "specific pruning thresholds that dynamically adjust based\n",
    "on weight activations, ensuring efficient model optimization\n",
    "across various stages of training. Drawing inspiration from\n",
    "the Lottery Ticket Hypothesis, which suggests that sparse\n",
    "subnetworks can perform similarly to their dense counterparts\n",
    "when retrained, the algorithm selectively deactivates weights\n",
    "using binary masks to enhance computational efficiency with-\n",
    "out sacrificing accuracy.\n",
    "The algorithm operates in several epochs, during which\n",
    "weight activations are evaluated. Based on a quantile-based\n",
    "strategy, pruning thresholds are established for each layer, en-\n",
    "suring that only weights contributing minimally to the model’s\n",
    "output are removed. After pruning, the model undergoes fine-\n",
    "tuning to restore any lost accuracy and further optimize the\n",
    "network. This step enhances the adaptability and scalability\n",
    "of the pruning method, making it suitable for deployment in\n",
    "resource-constrained environments\n",
    "A. Key Characteristics and Advantages\n",
    "The Neurological Pruning Algorithm boasts several key\n",
    "characteristics that distinguish it from conventional pruning\n",
    "techniques. One notable feature is its layer-specific pruning,\n",
    "which evaluates the importance of weights individually for\n",
    "each layer, leading to a more nuanced and efficient sparsifi-\n",
    "cation process. This approach prevents the model from losing\n",
    "essential features that may otherwise be pruned by a global\n",
    "approach.\n",
    "The algorithm’s use of quantile-based thresholding en-\n",
    "sures that only weights with the least contribution to the\n",
    "model’s output are pruned. This fine-tuned method avoids\n",
    "performance degradation by carefully targeting weights that\n",
    "are less significant to the network’s overall functionality.\n",
    "Furthermore, the pruning process is iterative, allowing for\n",
    "continuous model evaluation and real-time adjustments based\n",
    "on performance feedback, ensuring that accuracy is retained\n",
    "even as sparsity increases.\n",
    "Additionally, the algorithm maintains a careful balance\n",
    "between sparsity and accuracy, enabling a reduction in the\n",
    "model’s computational complexity while preserving its predic-\n",
    "tive capabilities. This characteristic is particularly important\n",
    "for applications requiring rapid inference times, such as in\n",
    "real-time healthcare systems and edge computing scenarios,\n",
    "where both energy efficiency and computational power are\n",
    "crucial.\n",
    "B. Impact on Model Performance\n",
    "The Neurological Pruning Algorithm significantly improves\n",
    "the efficiency of neural networks, making them more suitable\n",
    "for deployment on edge devices. Key performance metrics\n",
    "such as compression ratio (CR) and accuracy retention\n",
    "(AR) are critical indicators of the algorithm’s effectiveness.\n",
    "For instance, in evaluating a ResNet-18 model, the algorithm\n",
    "achieved a CR of 1, indicating that while the number of\n",
    "parameters remained the same, the model’s efficiency im-\n",
    "proved through selective pruning, reducing computational load\n",
    "without sacrificing performance.\n",
    "Moreover, the algorithm demonstrates the ability to achieve\n",
    "high accuracy retention, with models maintaining or even\n",
    "slightly improving their predictive performance post-pruning.\n",
    "This outcome highlights the algorithm’s capability to effi-\n",
    "ciently remove redundant weights while preserving the core\n",
    "functionality of the network. The result is a model that is\n",
    "not only more computationally efficient but also adaptable\n",
    "across different architectures and datasets, making it suit-\n",
    "able for a wide range of applications, from image classification\n",
    "to natural language processing .\n",
    "In practical terms, the Neurological Pruning Algorithm en-\n",
    "ables faster inference times and lower energy consumption,\n",
    "two crucial aspects for deploying AI models in resource-\n",
    "constrained environments. As such, it holds significant poten-\n",
    "tial for improving the viability of real-time, energy-efficient\n",
    "AI applications, particularly in fields like healthcare, where\n",
    "both speed and sustainability are essential.\n",
    "V. NEUROLOGICAL PRUNING ALGORITHM\n",
    "The Neurological Pruning Algorithm is a structured and\n",
    "adaptive methodology designed to optimize neural networks\n",
    "for resource-constrained environments. This algorithm goes\n",
    "beyond traditional pruning approaches by incorporating layer-\n",
    "specific pruning thresholds that dynamically adjust to the\n",
    "importance of weight activations within each layer. Drawing\n",
    "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
    "inforcement learning, the algorithm strikes an optimal balance\n",
    "between model sparsity and accuracy .\n",
    "A. Algorithm Description\n",
    "The Neurological Pruning Algorithm iteratively refines the\n",
    "neural network through a series of training epochs. During\n",
    "each epoch, weight activations are tracked, normalized, and\n",
    "analyzed to identify low-importance weights. These weights\n",
    "are selectively deactivated using binary masks, ensuring com-\n",
    "putational efficiency without compromising the network’s pre-\n",
    "dictive performance. The algorithm further integrates a fine-\n",
    "tuning phase, enabling the pruned network to recover and\n",
    "enhance its accuracy. The detailed process is presented in\n",
    "Algorithm 1.\n",
    "By dynamically adjusting to layer-specific characteristics\n",
    "and maintaining performance metrics, the Neurological Prun-\n",
    "ing Algorithm ensures a significant reduction in computational\n",
    "and memory requirements, paving the way for efficient deploy-\n",
    "ment of deep learning models on edge devices .\n",
    "B. Input Parameters and Output\n",
    "Inputs:\n",
    "• Pretrained model (M)\n",
    "• Dataset (D)\n",
    "• Prune fraction (p)\n",
    "• Total epochs (E)\n",
    "Output:\n",
    "• Pruned and fine-tuned model (Mpruned)\n",
    "C. Key Features and Advantages\n",
    "The Neurological Pruning Algorithm is designed with sev-\n",
    "eral distinctive features that contribute to its effectiveness\n",
    "in optimizing neural network performance. These features\n",
    "include:\n",
    "Algorithm 1 Simplified Neurological Pruning Algorithm\n",
    "Require: Pretrained model M, Dataset D, Prune fraction p,\n",
    "Total epochs E\n",
    "Ensure: Pruned and fine-tuned model Mpruned\n",
    "1: Initialize binary masks M with all ones\n",
    "2: for each epoch e = 1 to E do\n",
    "3:\n",
    "Compute weight activations A during forward passes\n",
    "4:\n",
    "for each layer i in M do\n",
    "5:\n",
    "Compute pruning threshold Ti as p-quantile of Ai\n",
    "6:\n",
    "Update mask Mi: retain weights with Ai > Ti\n",
    "7:\n",
    "Apply mask to layer: θi ←θi · Mi\n",
    "8:\n",
    "end for\n",
    "9:\n",
    "Evaluate model performance and update metrics\n",
    "10: end for\n",
    "11: Fine-tune Mpruned on D\n",
    "12: return Mpruned\n",
    "• Layer-specific Pruning: The algorithm applies pruning\n",
    "on a per-layer basis, evaluating the weight activations\n",
    "within each layer individually. This approach ensures that\n",
    "the sparsity introduced by pruning is well-distributed and\n",
    "context-dependent, facilitating more nuanced control over\n",
    "model optimization .\n",
    "• Quantile-based Thresholding: A quantile-based strat-\n",
    "egy is employed to determine the pruning threshold for\n",
    "each layer. This method ensures that only weights with\n",
    "minimal contribution to the network’s output are pruned,\n",
    "thereby reducing the risk of adversely affecting model\n",
    "accuracy while enhancing sparsity .\n",
    "• Iterative Pruning and Evaluation: The pruning pro-\n",
    "cess follows an iterative approach, enabling continuous\n",
    "assessment of the model’s performance after each pruning\n",
    "step. This ensures dynamic adjustments to maintain a bal-\n",
    "ance between model sparsity and predictive performance\n",
    "throughout training.\n",
    "• Preservation of Model Accuracy: By adjusting pruning\n",
    "thresholds in response to ongoing performance metrics,\n",
    "the algorithm ensures that model accuracy is not com-\n",
    "promised. The careful tuning of these thresholds allows\n",
    "for significant sparsity while maintaining the robustness\n",
    "of the network’s predictive capabilities .\n",
    "D. Impact on Neural Network Performance\n",
    "The Neurological Pruning Algorithm has been extensively\n",
    "evaluated across various benchmark datasets and model archi-\n",
    "tectures, demonstrating its capacity to achieve the following\n",
    "outcomes:\n",
    "• High Sparsity with Minimal Accuracy Loss: The algo-\n",
    "rithm effectively prunes unnecessary weights, achieving\n",
    "high sparsity levels without causing significant degrada-\n",
    "tion in model accuracy. This outcome is critical for the\n",
    "efficient deployment of models in resource-constrained\n",
    "environments.\n",
    "• Reduced Computational Complexity: By sparsifying\n",
    "the weight matrices, the algorithm reduces the compu-\n",
    "tational load during inference, which translates to faster\n",
    "model execution and lower resource consumption. This\n",
    "characteristic is especially beneficial in edge computing\n",
    "and real-time applications .\n",
    "• Broad Applicability: The algorithm is highly flexible\n",
    "and can be adapted to various neural network architec-\n",
    "tures and datasets, making it suitable for a wide range\n",
    "of tasks, from image recognition to natural language\n",
    "processing .\n",
    "VI. EVALUATION METRICS\n",
    "A. Performance Comparison\n",
    "Fig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\n",
    "TABLE I\n",
    "COMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\n",
    "Metric\n",
    "Unpruned\n",
    "Model\n",
    "Pruned\n",
    "Model\n",
    "Accuracy\n",
    "0.9562\n",
    "0.9583\n",
    "Train Loss\n",
    "0.1414\n",
    "0.1625\n",
    "Inference\n",
    "Time\n",
    "(s)\n",
    "0.0318\n",
    "0.0314\n",
    "No of Parame-\n",
    "ters(million)\n",
    "11.7\n",
    "11.7\n",
    "B. Evaluation Metrics Calculation\n",
    "The key metrics used to evaluate the proposed pruning\n",
    "technique are as follows:\n",
    "Compression Ratio (CR): The Compression Ratio (CR) is\n",
    "calculated as:\n",
    "CR = Total Parameters After Pruning\n",
    "Total Parameters Before Pruning\n",
    "(1)\n",
    "CR = 11.7M\n",
    "11.7M = 1\n",
    "Accuracy Retention (AR): The Accuracy Retention (AR) is\n",
    "calculated as:\n",
    "AR =\n",
    "\u0012Accuracy After Pruning −Baseline Accuracy\n",
    "Baseline Accuracy\n",
    "\u0013\n",
    "×100\n",
    "(2)\n",
    "AR =\n",
    "\u00120.958284 −0.956206\n",
    "0.956206\n",
    "\u0013\n",
    "× 100 = 0.22%\n",
    "This result indicates that the pruned model achieves an ac-\n",
    "curacy improvement of 0.22% when compared to unpruned\n",
    "model.\n",
    "C. Illustration of Results\n",
    "Fig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\n",
    "18.\n",
    "D. Discussion\n",
    "The evaluation metrics highlight the effectiveness of the\n",
    "proposed Neurological Pruning method on the ResNet-18\n",
    "model. The Compression Ratio (CR) remains 1, indicating\n",
    "no reduction in the total number of parameters, which suggests\n",
    "that this approach optimizes the neural network by selectively\n",
    "pruning connections rather than reducing the overall parameter\n",
    "count. The Accuracy Retention (AR), which is approximately\n",
    "0.22%, demonstrates that the Neurological Pruning technique\n",
    "successfully improved the accuracy compared to the baseline\n",
    "model.\n",
    "These results validate the potential of Neurological Pruning\n",
    "as an innovative approach to optimizing deep learning models.\n",
    "By maintaining accuracy while likely enhancing sparsity and\n",
    "computational efficiency, this method presents a promising di-\n",
    "rection for achieving high-performance models under resource\n",
    "constraints.\n",
    "VII. CONCLUSION\n",
    "In summary, the Neurological Pruning Algorithm provides\n",
    "a systematic and principled approach to optimizing neural\n",
    "networks. By balancing the trade-off between sparsity and\n",
    "accuracy, it enables significant reductions in computational\n",
    "complexity without sacrificing the network’s predictive per-\n",
    "formance. Future work will focus on extending the algorithm\n",
    "to dynamic network architectures and incorporating advanced\n",
    "regularization techniques to further enhance its robustness and\n",
    "adaptability in diverse applications.\n",
    "REFERENCES\n",
    "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\n",
    "Fluctuation-based Adaptive Structured Pruning for Large Language\n",
    "Models.\n",
    "[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\n",
    "Simplification of Deep Neural Network-Based Object Detector for Real-\n",
    "Time Edge Computing.\n",
    "[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\n",
    "Devices Object Detection by Filter Pruning.\n",
    "[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\n",
    "and Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\n",
    "Tracking Algorithms in Edge Devices.\n",
    "[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\n",
    "Chen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\n",
    "Compression Method for Object Detection Network for Edge Devices.\n",
    "[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\n",
    "Cao. Absorption Pruning of Deep Neural Network for Object Detection\n",
    "in Remote Sensing Imagery.\n",
    "[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\n",
    "and Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\n",
    "Distribution Detection: An Empirical Survey.\n",
    "[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\n",
    "Sahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\n",
    "[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\n",
    "Compress DNNs.\n",
    "[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\n",
    "derstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\n",
    "[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\n",
    "Proving the Lottery Ticket Hypothesis: Pruning is All You Need.\n",
    "[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\n",
    "Devices via Reconstruction-Based Channel Pruning.\n",
    "[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\n",
    "Determine Reliable Paths on Networks with Random and Correlated\n",
    "Link Travel Times.\n",
    "[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\n",
    "and D. Suganthi. Real-Time Object Detection on Edge Devices Using\n",
    "Mobile Neural Networks.\n",
    "[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\n",
    "Tang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\n",
    "Models.\n",
    "[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\n",
    "Li, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\n",
    "Deng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\n",
    "Detection of Tea Leaves with One Bud and Two Leaves Based on\n",
    "ShuffleNetv2-YOLOv5-Lite-E.\n",
    "\"\"\", \"metadata\": {\"source\": \"geo_facts.txt\", \"page\": 1}},\n",
    "    {\"content\": \"The Great Wall of China is located in China and is a famous landmark.\", \"metadata\": {\"source\": \"world_landmarks.txt\", \"page\": 5}},\n",
    "    {\"content\": \"Paris is known for the Eiffel Tower.\", \"metadata\": {\"source\": \"geo_facts.txt\", \"page\": 2}},\n",
    "]\n",
    "\n",
    "# Convert raw data to LangChain Document objects\n",
    "documents = [Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"]) for doc in documents_data]\n",
    "\n",
    "# Create FAISS vector store directly from documents\n",
    "# This handles embedding the texts and building the index internally\n",
    "try:\n",
    "    vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "    print(\"FAISS vector store created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating FAISS vector store: {e}\")\n",
    "# Exit if vector store creation fails\n",
    "\n",
    "# Step 4: Load Gemini model for text generation\n",
    "\n",
    "# Step 5: Create the Retrieval-Augmented Generation pipeline\n",
    "print(\"Creating RAG chain...\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\", # Other options: \"mmr\", \"similarity_score_threshold\"\n",
    "    search_kwargs={'k': 2} # Retrieve top 2 relevant documents\n",
    ")\n",
    "\n",
    "# Use the recommended from_chain_type method\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # Options: \"stuff\", \"map_reduce\", \"refine\", \"map_rerank\"\n",
    "                        # \"stuff\" puts all retrieved docs into the context\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True # Include source documents in the output\n",
    "    # chain_type_kwargs={\"prompt\": YOUR_CUSTOM_PROMPT} # Optional: customize prompt\n",
    ")\n",
    "print(\"RAG chain created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c4dcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying the RAG system with: 'who is surya narayanaa'\n",
      "\n",
      "--- RAG System Output ---\n",
      "Answer: Surya Narayanaa N T is listed as a co-author (5th author) of the research paper \"Neuro Prune: An Adaptive Approach for Efficient Deep Neural Network Optimization on Edge Devices.\" He is a student (BE-CSE (AI&ML)) at PSG College of Technology. His email address is suryanarayanaant@gmail.com.\n",
      "\n",
      "Sources:\n",
      "  Source 1:\n",
      "    Content: \n",
      "        Neuro Prune: An Adaptive Approach for Efficient\n",
      "Deep Neural Network Optimization on Edge\n",
      "Devices\n",
      "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
      "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
      "grk.cse@psgtech.ac.in\n",
      "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
      "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
      "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
      "Abstract—The optimization of deep neural networks for de-\n",
      "ployment on edge devices is a significant research area due to\n",
      "the demand for applications such as augmented reality, smart\n",
      "cameras, and autonomous navigation. However, deploying large\n",
      "deep learning models on edge devices poses challenges related to\n",
      "computational power, energy consumption, and latency. Pruning\n",
      "is a method to reduce the model size, accelerate inference, and\n",
      "save power. The objective of the paper is to propose the Neuro\n",
      "Prune algorithm and to apply it for the optimization of deep\n",
      "neural networks on edge devices. Efforts have been made to\n",
      "compare pruned and unpruned models. As a result, the pruned\n",
      "model has an accuracy increase of 0.22%.\n",
      "I. INTRODUCTION\n",
      "The rise of intelligent systems, ranging from autonomous\n",
      "drones to augmented reality (AR) devices and smart surveil-\n",
      "lance cameras, has intensified the need for efficient deep neural\n",
      "network deployment on edge devices with constrained com-\n",
      "putational resources. While advanced deep learning models\n",
      "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
      "their deployment on such devices is hindered by significant\n",
      "computational demands, large memory footprints, and high\n",
      "energy consumption. These challenges pose critical barriers to\n",
      "real-time applications and sustainable deployment.\n",
      "This paper builds on existing pruning techniques, includ-\n",
      "ing unstructured pruning, structured pruning, and approaches\n",
      "inspired by quantization, highlighting their strengths and lim-\n",
      "itations. In response to the challenges identified, we introduce\n",
      "Neuro Prune, a novel pruning approach that integrates princi-\n",
      "ples from reinforcement learning and the Lottery Ticket Hy-\n",
      "pothesis to achieve an optimal balance between accuracy, com-\n",
      "putational efficiency, and energy consumption. By integrating\n",
      "these principles, Neuro Prune provides a robust framework\n",
      "for optimizing deep neural networks for deployment on edge\n",
      "devices without sacrificing performance.\n",
      "Neuro Prune reduces model size and computational re-\n",
      "quirements while maintaining or even improving the model’s\n",
      "accuracy. These findings underscore the potential of Neuro\n",
      "Prune to enable real-time, energy-efficient deep learning on\n",
      "resource-constrained devices.\n",
      "Neuro Prune employs a systematic process to optimize\n",
      "neural networks for edge deployment:\n",
      "• Mask Initialization: Each layer’s weights are paired with\n",
      "a binary mask (original_mask), initialized to ones.\n",
      "• Activation Tracking: During forward passes over the\n",
      "dataset, the magnitudes of weight activations are mon-\n",
      "itored and recorded.\n",
      "• Normalize and Prune: The recorded activations are\n",
      "normalized, and a threshold based on the pruning fraction\n",
      "is computed. Using this threshold, the masks are dynam-\n",
      "ically updated to prune unimportant weights.\n",
      "• Evaluate and Log Metrics: After pruning, metrics such\n",
      "as model sparsity and accuracy are computed. A reward\n",
      "metric is derived to balance accuracy and sparsity using\n",
      "a tunable parameter, λweight.\n",
      "• Retrain the Model: The pruned model undergoes retrain-\n",
      "ing to improve the loss in accuracy while maintaining low\n",
      "computational requirements\n",
      "Neuro Prune’s unique combination of reinforcement learn-\n",
      "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
      "pothesis for identifying optimal subnetworks enables it to\n",
      "achieve remarkable efficiency. This approach ensures substan-\n",
      "tial reduction in model size and energy consumption while\n",
      "preserving accuracy and inference speed, making it an ideal\n",
      "solution for edge-based deep learning applications.\n",
      "Through extensive empirical evaluations and theoretical\n",
      "insights, this paper demonstrates how Neuro Pruning outper-\n",
      "forms traditional pruning methods, setting a new benchmark\n",
      "for sustainable, high-performance AI deployment on edge\n",
      "devices. By situating our work within the broader research\n",
      "context, we aim to offer a critical contribution to the ongoing\n",
      "evolution of neural network optimization techniques.\n",
      "II. BACKGROUND\n",
      "A. Need for Pruning Techniques\n",
      "Pruning is a crucial optimization technique for deploying\n",
      "deep neural networks in resource-constrained environments\n",
      "like healthcare. For example, wearable devices like smart-\n",
      "watches that monitor heart rates or portable EEG systems\n",
      "for brain activity analysis require lightweight models due to\n",
      "their limited computational power and memory. Pruning helps\n",
      "address these constraints by reducing the memory footprint\n",
      "and computational load, enabling efficient model deployment.\n",
      "Additionally, it minimizes energy consumption, which is vital\n",
      "for sustainable and real-time healthcare applications. Faster\n",
      "inference enabled by pruning is particularly beneficial for\n",
      "time-sensitive tasks like seizure detection or emergency di-\n",
      "agnostics. Moreover, pruning enhances model interpretability\n",
      "by retaining only the most critical components, which is\n",
      "essential for gaining trust and ensuring regulatory compliance\n",
      "in healthcare. By reducing the hardware and operational costs,\n",
      "pruning also makes deploying advanced AI solutions on edge\n",
      "devices more cost-effective and accessible.\n",
      "B. Types of Pruning Techniques\n",
      "Pruning methods are broadly classified based on granu-\n",
      "larity, timing, and approach. Granularity-based pruning in-\n",
      "cludes structured pruning, which removes entire components\n",
      "such as neurons or filters to streamline computations, and\n",
      "unstructured pruning, which eliminates individual weights\n",
      "for finer optimization, though it often requires specialized\n",
      "hardware. Timing-based pruning involves pre-training pruning,\n",
      "which optimizes models before training; post-training prun-\n",
      "ing, where redundant components are removed from trained\n",
      "models with possible retraining; and dynamic pruning during\n",
      "training, which adjusts models in real-time for improved\n",
      "adaptability. Metric-based pruning uses parameters like weight\n",
      "magnitude or gradient contribution to identify and remove\n",
      "less critical components, while application-specific pruning\n",
      "tailors strategies to specific tasks, such as optimizing for EEG\n",
      "signal analysis, or hardware constraints like GPUs. Together,\n",
      "these methods enable the development of efficient, scalable,\n",
      "and task-specific models, particularly suited for edge device\n",
      "deployment in healthcare and other domains.\n",
      "III. RELATED WORK\n",
      "Jielei Wang et al.[12] introduced an absorption pruning\n",
      "method for object detection in remote sensing imagery. This\n",
      "study achieves efficient compression with minimal accuracy\n",
      "loss but requires careful layer-wise pruning ratio tuning.\n",
      "Jan Muller et al., in their research[12], propose a neural\n",
      "network pruning method for multi-object tracking (MOT) that\n",
      "reduces model size by up to 70\n",
      "Liang Li et al., in their work[9], introduce a novel pruning\n",
      "method for DNNs that utilizes a self-adaptive mechanism\n",
      "based on weight sparsity ratios and a protective reconstruction\n",
      "mechanism. Their approach improves both model compres-\n",
      "sion and accuracy, outperforming state-of-the-art methods on\n",
      "CIFAR-10 and ImageNet datasets.\n",
      "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
      "learning with edge devices like Google Coral AI and Nvidia\n",
      "Jetson Nano. Their work revolutionizes computer vision and\n",
      "real-time tracking but raises energy concerns due to the\n",
      "computational demands of advanced algorithms.\n",
      "Satoru Koda et al.[7] explore how weight pruning in\n",
      "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
      "larly with Mahalanobis-based approaches, by improving global\n",
      "feature extraction and leveraging weights not critical for\n",
      "classification.\n",
      "Shvetha S Kumar et al. analyze and compare three pruning\n",
      "techniques—L1-norm filter pruning, channel pruning, and\n",
      "weight pruning—on CNNs for accuracy and inference time,\n",
      "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
      "pared to GTX1080 Ti in their study[8].\n",
      "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
      "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
      "on stochastic networks, extending it to related problems and\n",
      "demonstrating its efficiency on real-world networks in their\n",
      "study[13].\n",
      "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
      "Training outperforms Post-Training Quantization, and sparsity\n",
      "training with Optimal Reduction pruning improves YOLOv4’s\n",
      "efficiency and edge-device performance, despite potential\n",
      "high-pruning rate drawbacks.\n",
      "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
      "method for DNNs. This approach optimizes inference speed\n",
      "and resource efficiency on edge devices with minimal accuracy\n",
      "loss but requires careful tuning of layer-specific pruning ratios.\n",
      "Yongqi An et al.[1] introduce a retraining-free structured\n",
      "pruning framework for Large Language Models (LLMs). Their\n",
      "method incorporates a fluctuation-based pruning metric, adap-\n",
      "tive structure search, and a bias compensation mechanism to\n",
      "achieve efficient pruning. FLAP significantly enhances infer-\n",
      "ence speed and reduces model size without requiring retrain-\n",
      "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
      "and Wanda-sp on LLaMA models across various benchmarks.\n",
      "From referring to these research papers, we derived an\n",
      "idea for pruning on edge devices. Additionally, our proposed\n",
      "solution is inspired by the research of Jaron Maene et al.[10],\n",
      "which suggests that sparse subnetworks in dense networks can\n",
      "achieve similar accuracy when retrained. This study demon-\n",
      "strates stable training with linear mode connectivity, support-\n",
      "ing the idea that lottery tickets retrain to similar regions, but\n",
      "questions their independence from dense training and iterative\n",
      "pruning.\n",
      "Eran Malach et al.[11] further strengthen the lottery ticket\n",
      "hypothesis by proving that over-parameterized neural networks\n",
      "with random weights always contain a subnetwork matching\n",
      "the accuracy of a target network, without additional training.\n",
      "IV. NEUROLOGICAL PRUNING APPROACH\n",
      "The Neurological Pruning Algorithm is designed to opti-\n",
      "mize neural networks by iteratively pruning less important\n",
      "weights while maintaining high performance. Unlike tradi-\n",
      "tional methods, this approach incorporates adaptive, layer-\n",
      "specific pruning thresholds that dynamically adjust based\n",
      "on weight activations, ensuring efficient model optimization\n",
      "across various stages of training. Drawing inspiration from\n",
      "the Lottery Ticket Hypothesis, which suggests that sparse\n",
      "subnetworks can perform similarly to their dense counterparts\n",
      "when retrained, the algorithm selectively deactivates weights\n",
      "using binary masks to enhance computational efficiency with-\n",
      "out sacrificing accuracy.\n",
      "The algorithm operates in several epochs, during which\n",
      "weight activations are evaluated. Based on a quantile-based\n",
      "strategy, pruning thresholds are established for each layer, en-\n",
      "suring that only weights contributing minimally to the model’s\n",
      "output are removed. After pruning, the model undergoes fine-\n",
      "tuning to restore any lost accuracy and further optimize the\n",
      "network. This step enhances the adaptability and scalability\n",
      "of the pruning method, making it suitable for deployment in\n",
      "resource-constrained environments\n",
      "A. Key Characteristics and Advantages\n",
      "The Neurological Pruning Algorithm boasts several key\n",
      "characteristics that distinguish it from conventional pruning\n",
      "techniques. One notable feature is its layer-specific pruning,\n",
      "which evaluates the importance of weights individually for\n",
      "each layer, leading to a more nuanced and efficient sparsifi-\n",
      "cation process. This approach prevents the model from losing\n",
      "essential features that may otherwise be pruned by a global\n",
      "approach.\n",
      "The algorithm’s use of quantile-based thresholding en-\n",
      "sures that only weights with the least contribution to the\n",
      "model’s output are pruned. This fine-tuned method avoids\n",
      "performance degradation by carefully targeting weights that\n",
      "are less significant to the network’s overall functionality.\n",
      "Furthermore, the pruning process is iterative, allowing for\n",
      "continuous model evaluation and real-time adjustments based\n",
      "on performance feedback, ensuring that accuracy is retained\n",
      "even as sparsity increases.\n",
      "Additionally, the algorithm maintains a careful balance\n",
      "between sparsity and accuracy, enabling a reduction in the\n",
      "model’s computational complexity while preserving its predic-\n",
      "tive capabilities. This characteristic is particularly important\n",
      "for applications requiring rapid inference times, such as in\n",
      "real-time healthcare systems and edge computing scenarios,\n",
      "where both energy efficiency and computational power are\n",
      "crucial.\n",
      "B. Impact on Model Performance\n",
      "The Neurological Pruning Algorithm significantly improves\n",
      "the efficiency of neural networks, making them more suitable\n",
      "for deployment on edge devices. Key performance metrics\n",
      "such as compression ratio (CR) and accuracy retention\n",
      "(AR) are critical indicators of the algorithm’s effectiveness.\n",
      "For instance, in evaluating a ResNet-18 model, the algorithm\n",
      "achieved a CR of 1, indicating that while the number of\n",
      "parameters remained the same, the model’s efficiency im-\n",
      "proved through selective pruning, reducing computational load\n",
      "without sacrificing performance.\n",
      "Moreover, the algorithm demonstrates the ability to achieve\n",
      "high accuracy retention, with models maintaining or even\n",
      "slightly improving their predictive performance post-pruning.\n",
      "This outcome highlights the algorithm’s capability to effi-\n",
      "ciently remove redundant weights while preserving the core\n",
      "functionality of the network. The result is a model that is\n",
      "not only more computationally efficient but also adaptable\n",
      "across different architectures and datasets, making it suit-\n",
      "able for a wide range of applications, from image classification\n",
      "to natural language processing .\n",
      "In practical terms, the Neurological Pruning Algorithm en-\n",
      "ables faster inference times and lower energy consumption,\n",
      "two crucial aspects for deploying AI models in resource-\n",
      "constrained environments. As such, it holds significant poten-\n",
      "tial for improving the viability of real-time, energy-efficient\n",
      "AI applications, particularly in fields like healthcare, where\n",
      "both speed and sustainability are essential.\n",
      "V. NEUROLOGICAL PRUNING ALGORITHM\n",
      "The Neurological Pruning Algorithm is a structured and\n",
      "adaptive methodology designed to optimize neural networks\n",
      "for resource-constrained environments. This algorithm goes\n",
      "beyond traditional pruning approaches by incorporating layer-\n",
      "specific pruning thresholds that dynamically adjust to the\n",
      "importance of weight activations within each layer. Drawing\n",
      "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
      "inforcement learning, the algorithm strikes an optimal balance\n",
      "between model sparsity and accuracy .\n",
      "A. Algorithm Description\n",
      "The Neurological Pruning Algorithm iteratively refines the\n",
      "neural network through a series of training epochs. During\n",
      "each epoch, weight activations are tracked, normalized, and\n",
      "analyzed to identify low-importance weights. These weights\n",
      "are selectively deactivated using binary masks, ensuring com-\n",
      "putational efficiency without compromising the network’s pre-\n",
      "dictive performance. The algorithm further integrates a fine-\n",
      "tuning phase, enabling the pruned network to recover and\n",
      "enhance its accuracy. The detailed process is presented in\n",
      "Algorithm 1.\n",
      "By dynamically adjusting to layer-specific characteristics\n",
      "and maintaining performance metrics, the Neurological Prun-\n",
      "ing Algorithm ensures a significant reduction in computational\n",
      "and memory requirements, paving the way for efficient deploy-\n",
      "ment of deep learning models on edge devices .\n",
      "B. Input Parameters and Output\n",
      "Inputs:\n",
      "• Pretrained model (M)\n",
      "• Dataset (D)\n",
      "• Prune fraction (p)\n",
      "• Total epochs (E)\n",
      "Output:\n",
      "• Pruned and fine-tuned model (Mpruned)\n",
      "C. Key Features and Advantages\n",
      "The Neurological Pruning Algorithm is designed with sev-\n",
      "eral distinctive features that contribute to its effectiveness\n",
      "in optimizing neural network performance. These features\n",
      "include:\n",
      "Algorithm 1 Simplified Neurological Pruning Algorithm\n",
      "Require: Pretrained model M, Dataset D, Prune fraction p,\n",
      "Total epochs E\n",
      "Ensure: Pruned and fine-tuned model Mpruned\n",
      "1: Initialize binary masks M with all ones\n",
      "2: for each epoch e = 1 to E do\n",
      "3:\n",
      "Compute weight activations A during forward passes\n",
      "4:\n",
      "for each layer i in M do\n",
      "5:\n",
      "Compute pruning threshold Ti as p-quantile of Ai\n",
      "6:\n",
      "Update mask Mi: retain weights with Ai > Ti\n",
      "7:\n",
      "Apply mask to layer: θi ←θi · Mi\n",
      "8:\n",
      "end for\n",
      "9:\n",
      "Evaluate model performance and update metrics\n",
      "10: end for\n",
      "11: Fine-tune Mpruned on D\n",
      "12: return Mpruned\n",
      "• Layer-specific Pruning: The algorithm applies pruning\n",
      "on a per-layer basis, evaluating the weight activations\n",
      "within each layer individually. This approach ensures that\n",
      "the sparsity introduced by pruning is well-distributed and\n",
      "context-dependent, facilitating more nuanced control over\n",
      "model optimization .\n",
      "• Quantile-based Thresholding: A quantile-based strat-\n",
      "egy is employed to determine the pruning threshold for\n",
      "each layer. This method ensures that only weights with\n",
      "minimal contribution to the network’s output are pruned,\n",
      "thereby reducing the risk of adversely affecting model\n",
      "accuracy while enhancing sparsity .\n",
      "• Iterative Pruning and Evaluation: The pruning pro-\n",
      "cess follows an iterative approach, enabling continuous\n",
      "assessment of the model’s performance after each pruning\n",
      "step. This ensures dynamic adjustments to maintain a bal-\n",
      "ance between model sparsity and predictive performance\n",
      "throughout training.\n",
      "• Preservation of Model Accuracy: By adjusting pruning\n",
      "thresholds in response to ongoing performance metrics,\n",
      "the algorithm ensures that model accuracy is not com-\n",
      "promised. The careful tuning of these thresholds allows\n",
      "for significant sparsity while maintaining the robustness\n",
      "of the network’s predictive capabilities .\n",
      "D. Impact on Neural Network Performance\n",
      "The Neurological Pruning Algorithm has been extensively\n",
      "evaluated across various benchmark datasets and model archi-\n",
      "tectures, demonstrating its capacity to achieve the following\n",
      "outcomes:\n",
      "• High Sparsity with Minimal Accuracy Loss: The algo-\n",
      "rithm effectively prunes unnecessary weights, achieving\n",
      "high sparsity levels without causing significant degrada-\n",
      "tion in model accuracy. This outcome is critical for the\n",
      "efficient deployment of models in resource-constrained\n",
      "environments.\n",
      "• Reduced Computational Complexity: By sparsifying\n",
      "the weight matrices, the algorithm reduces the compu-\n",
      "tational load during inference, which translates to faster\n",
      "model execution and lower resource consumption. This\n",
      "characteristic is especially beneficial in edge computing\n",
      "and real-time applications .\n",
      "• Broad Applicability: The algorithm is highly flexible\n",
      "and can be adapted to various neural network architec-\n",
      "tures and datasets, making it suitable for a wide range\n",
      "of tasks, from image recognition to natural language\n",
      "processing .\n",
      "VI. EVALUATION METRICS\n",
      "A. Performance Comparison\n",
      "Fig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\n",
      "TABLE I\n",
      "COMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\n",
      "Metric\n",
      "Unpruned\n",
      "Model\n",
      "Pruned\n",
      "Model\n",
      "Accuracy\n",
      "0.9562\n",
      "0.9583\n",
      "Train Loss\n",
      "0.1414\n",
      "0.1625\n",
      "Inference\n",
      "Time\n",
      "(s)\n",
      "0.0318\n",
      "0.0314\n",
      "No of Parame-\n",
      "ters(million)\n",
      "11.7\n",
      "11.7\n",
      "B. Evaluation Metrics Calculation\n",
      "The key metrics used to evaluate the proposed pruning\n",
      "technique are as follows:\n",
      "Compression Ratio (CR): The Compression Ratio (CR) is\n",
      "calculated as:\n",
      "CR = Total Parameters After Pruning\n",
      "Total Parameters Before Pruning\n",
      "(1)\n",
      "CR = 11.7M\n",
      "11.7M = 1\n",
      "Accuracy Retention (AR): The Accuracy Retention (AR) is\n",
      "calculated as:\n",
      "AR =\n",
      "\u0012Accuracy After Pruning −Baseline Accuracy\n",
      "Baseline Accuracy\n",
      "\u0013\n",
      "×100\n",
      "(2)\n",
      "AR =\n",
      "\u00120.958284 −0.956206\n",
      "0.956206\n",
      "\u0013\n",
      "× 100 = 0.22%\n",
      "This result indicates that the pruned model achieves an ac-\n",
      "curacy improvement of 0.22% when compared to unpruned\n",
      "model.\n",
      "C. Illustration of Results\n",
      "Fig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\n",
      "18.\n",
      "D. Discussion\n",
      "The evaluation metrics highlight the effectiveness of the\n",
      "proposed Neurological Pruning method on the ResNet-18\n",
      "model. The Compression Ratio (CR) remains 1, indicating\n",
      "no reduction in the total number of parameters, which suggests\n",
      "that this approach optimizes the neural network by selectively\n",
      "pruning connections rather than reducing the overall parameter\n",
      "count. The Accuracy Retention (AR), which is approximately\n",
      "0.22%, demonstrates that the Neurological Pruning technique\n",
      "successfully improved the accuracy compared to the baseline\n",
      "model.\n",
      "These results validate the potential of Neurological Pruning\n",
      "as an innovative approach to optimizing deep learning models.\n",
      "By maintaining accuracy while likely enhancing sparsity and\n",
      "computational efficiency, this method presents a promising di-\n",
      "rection for achieving high-performance models under resource\n",
      "constraints.\n",
      "VII. CONCLUSION\n",
      "In summary, the Neurological Pruning Algorithm provides\n",
      "a systematic and principled approach to optimizing neural\n",
      "networks. By balancing the trade-off between sparsity and\n",
      "accuracy, it enables significant reductions in computational\n",
      "complexity without sacrificing the network’s predictive per-\n",
      "formance. Future work will focus on extending the algorithm\n",
      "to dynamic network architectures and incorporating advanced\n",
      "regularization techniques to further enhance its robustness and\n",
      "adaptability in diverse applications.\n",
      "REFERENCES\n",
      "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\n",
      "Fluctuation-based Adaptive Structured Pruning for Large Language\n",
      "Models.\n",
      "[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\n",
      "Simplification of Deep Neural Network-Based Object Detector for Real-\n",
      "Time Edge Computing.\n",
      "[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\n",
      "Devices Object Detection by Filter Pruning.\n",
      "[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\n",
      "and Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\n",
      "Tracking Algorithms in Edge Devices.\n",
      "[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\n",
      "Chen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\n",
      "Compression Method for Object Detection Network for Edge Devices.\n",
      "[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\n",
      "Cao. Absorption Pruning of Deep Neural Network for Object Detection\n",
      "in Remote Sensing Imagery.\n",
      "[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\n",
      "and Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\n",
      "Distribution Detection: An Empirical Survey.\n",
      "[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\n",
      "Sahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\n",
      "[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\n",
      "Compress DNNs.\n",
      "[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\n",
      "derstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\n",
      "[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\n",
      "Proving the Lottery Ticket Hypothesis: Pruning is All You Need.\n",
      "[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\n",
      "Devices via Reconstruction-Based Channel Pruning.\n",
      "[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\n",
      "Determine Reliable Paths on Networks with Random and Correlated\n",
      "Link Travel Times.\n",
      "[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\n",
      "and D. Suganthi. Real-Time Object Detection on Edge Devices Using\n",
      "Mobile Neural Networks.\n",
      "[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\n",
      "Tang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\n",
      "Models.\n",
      "[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\n",
      "Li, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\n",
      "Deng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\n",
      "Detection of Tea Leaves with One Bud and Two Leaves Based on\n",
      "ShuffleNetv2-YOLOv5-Lite-E.\n",
      "Neuro Prune: An Adaptive Approach for Efficient\n",
      "Deep Neural Network Optimization on Edge\n",
      "Devices\n",
      "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
      "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
      "grk.cse@psgtech.ac.in\n",
      "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
      "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
      "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
      "Abstract—The optimization of deep neural networks for de-\n",
      "ployment on edge devices is a significant research area due to\n",
      "the demand for applications such as augmented reality, smart\n",
      "cameras, and autonomous navigation. However, deploying large\n",
      "deep learning models on edge devices poses challenges related to\n",
      "computational power, energy consumption, and latency. Pruning\n",
      "is a method to reduce the model size, accelerate inference, and\n",
      "save power. The objective of the paper is to propose the Neuro\n",
      "Prune algorithm and to apply it for the optimization of deep\n",
      "neural networks on edge devices. Efforts have been made to\n",
      "compare pruned and unpruned models. As a result, the pruned\n",
      "model has an accuracy increase of 0.22%.\n",
      "I. INTRODUCTION\n",
      "The rise of intelligent systems, ranging from autonomous\n",
      "drones to augmented reality (AR) devices and smart surveil-\n",
      "lance cameras, has intensified the need for efficient deep neural\n",
      "network deployment on edge devices with constrained com-\n",
      "putational resources. While advanced deep learning models\n",
      "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
      "their deployment on such devices is hindered by significant\n",
      "computational demands, large memory footprints, and high\n",
      "energy consumption. These challenges pose critical barriers to\n",
      "real-time applications and sustainable deployment.\n",
      "This paper builds on existing pruning techniques, includ-\n",
      "ing unstructured pruning, structured pruning, and approaches\n",
      "inspired by quantization, highlighting their strengths and lim-\n",
      "itations. In response to the challenges identified, we introduce\n",
      "Neuro Prune, a novel pruning approach that integrates princi-\n",
      "ples from reinforcement learning and the Lottery Ticket Hy-\n",
      "pothesis to achieve an optimal balance between accuracy, com-\n",
      "putational efficiency, and energy consumption. By integrating\n",
      "these principles, Neuro Prune provides a robust framework\n",
      "for optimizing deep neural networks for deployment on edge\n",
      "devices without sacrificing performance.\n",
      "Neuro Prune reduces model size and computational re-\n",
      "quirements while maintaining or even improving the model’s\n",
      "accuracy. These findings underscore the potential of Neuro\n",
      "Prune to enable real-time, energy-efficient deep learning on\n",
      "resource-constrained devices.\n",
      "Neuro Prune employs a systematic process to optimize\n",
      "neural networks for edge deployment:\n",
      "• Mask Initialization: Each layer’s weights are paired with\n",
      "a binary mask (original_mask), initialized to ones.\n",
      "• Activation Tracking: During forward passes over the\n",
      "dataset, the magnitudes of weight activations are mon-\n",
      "itored and recorded.\n",
      "• Normalize and Prune: The recorded activations are\n",
      "normalized, and a threshold based on the pruning fraction\n",
      "is computed. Using this threshold, the masks are dynam-\n",
      "ically updated to prune unimportant weights.\n",
      "• Evaluate and Log Metrics: After pruning, metrics such\n",
      "as model sparsity and accuracy are computed. A reward\n",
      "metric is derived to balance accuracy and sparsity using\n",
      "a tunable parameter, λweight.\n",
      "• Retrain the Model: The pruned model undergoes retrain-\n",
      "ing to improve the loss in accuracy while maintaining low\n",
      "computational requirements\n",
      "Neuro Prune’s unique combination of reinforcement learn-\n",
      "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
      "pothesis for identifying optimal subnetworks enables it to\n",
      "achieve remarkable efficiency. This approach ensures substan-\n",
      "tial reduction in model size and energy consumption while\n",
      "preserving accuracy and inference speed, making it an ideal\n",
      "solution for edge-based deep learning applications.\n",
      "Through extensive empirical evaluations and theoretical\n",
      "insights, this paper demonstrates how Neuro Pruning outper-\n",
      "forms traditional pruning methods, setting a new benchmark\n",
      "for sustainable, high-performance AI deployment on edge\n",
      "devices. By situating our work within the broader research\n",
      "context, we aim to offer a critical contribution to the ongoing\n",
      "evolution of neural network optimization techniques.\n",
      "II. BACKGROUND\n",
      "A. Need for Pruning Techniques\n",
      "Pruning is a crucial optimization technique for deploying\n",
      "deep neural networks in resource-constrained environments\n",
      "like healthcare. For example, wearable devices like smart-\n",
      "watches that monitor heart rates or portable EEG systems\n",
      "for brain activity analysis require lightweight models due to\n",
      "their limited computational power and memory. Pruning helps\n",
      "address these constraints by reducing the memory footprint\n",
      "and computational load, enabling efficient model deployment.\n",
      "Additionally, it minimizes energy consumption, which is vital\n",
      "for sustainable and real-time healthcare applications. Faster\n",
      "inference enabled by pruning is particularly beneficial for\n",
      "time-sensitive tasks like seizure detection or emergency di-\n",
      "agnostics. Moreover, pruning enhances model interpretability\n",
      "by retaining only the most critical components, which is\n",
      "essential for gaining trust and ensuring regulatory compliance\n",
      "in healthcare. By reducing the hardware and operational costs,\n",
      "pruning also makes deploying advanced AI solutions on edge\n",
      "devices more cost-effective and accessible.\n",
      "B. Types of Pruning Techniques\n",
      "Pruning methods are broadly classified based on granu-\n",
      "larity, timing, and approach. Granularity-based pruning in-\n",
      "cludes structured pruning, which removes entire components\n",
      "such as neurons or filters to streamline computations, and\n",
      "unstructured pruning, which eliminates individual weights\n",
      "for finer optimization, though it often requires specialized\n",
      "hardware. Timing-based pruning involves pre-training pruning,\n",
      "which optimizes models before training; post-training prun-\n",
      "ing, where redundant components are removed from trained\n",
      "models with possible retraining; and dynamic pruning during\n",
      "training, which adjusts models in real-time for improved\n",
      "adaptability. Metric-based pruning uses parameters like weight\n",
      "magnitude or gradient contribution to identify and remove\n",
      "less critical components, while application-specific pruning\n",
      "tailors strategies to specific tasks, such as optimizing for EEG\n",
      "signal analysis, or hardware constraints like GPUs. Together,\n",
      "these methods enable the development of efficient, scalable,\n",
      "and task-specific models, particularly suited for edge device\n",
      "deployment in healthcare and other domains.\n",
      "III. RELATED WORK\n",
      "Jielei Wang et al.[12] introduced an absorption pruning\n",
      "method for object detection in remote sensing imagery. This\n",
      "study achieves efficient compression with minimal accuracy\n",
      "loss but requires careful layer-wise pruning ratio tuning.\n",
      "Jan Muller et al., in their research[12], propose a neural\n",
      "network pruning method for multi-object tracking (MOT) that\n",
      "reduces model size by up to 70\n",
      "Liang Li et al., in their work[9], introduce a novel pruning\n",
      "method for DNNs that utilizes a self-adaptive mechanism\n",
      "based on weight sparsity ratios and a protective reconstruction\n",
      "mechanism. Their approach improves both model compres-\n",
      "sion and accuracy, outperforming state-of-the-art methods on\n",
      "CIFAR-10 and ImageNet datasets.\n",
      "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
      "learning with edge devices like Google Coral AI and Nvidia\n",
      "Jetson Nano. Their work revolutionizes computer vision and\n",
      "real-time tracking but raises energy concerns due to the\n",
      "computational demands of advanced algorithms.\n",
      "Satoru Koda et al.[7] explore how weight pruning in\n",
      "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
      "larly with Mahalanobis-based approaches, by improving global\n",
      "feature extraction and leveraging weights not critical for\n",
      "classification.\n",
      "Shvetha S Kumar et al. analyze and compare three pruning\n",
      "techniques—L1-norm filter pruning, channel pruning, and\n",
      "weight pruning—on CNNs for accuracy and inference time,\n",
      "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
      "pared to GTX1080 Ti in their study[8].\n",
      "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
      "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
      "on stochastic networks, extending it to related problems and\n",
      "demonstrating its efficiency on real-world networks in their\n",
      "study[13].\n",
      "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
      "Training outperforms Post-Training Quantization, and sparsity\n",
      "training with Optimal Reduction pruning improves YOLOv4’s\n",
      "efficiency and edge-device performance, despite potential\n",
      "high-pruning rate drawbacks.\n",
      "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
      "method for DNNs. This approach optimizes inference speed\n",
      "and resource efficiency on edge devices with minimal accuracy\n",
      "loss but requires careful tuning of layer-specific pruning ratios.\n",
      "Yongqi An et al.[1] introduce a retraining-free structured\n",
      "pruning framework for Large Language Models (LLMs). Their\n",
      "method incorporates a fluctuation-based pruning metric, adap-\n",
      "tive structure search, and a bias compensation mechanism to\n",
      "achieve efficient pruning. FLAP significantly enhances infer-\n",
      "ence speed and reduces model size without requiring retrain-\n",
      "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
      "and Wanda-sp on LLaMA models across various benchmarks.\n",
      "From referring to these research papers, we derived an\n",
      "idea for pruning on edge devices. Additionally, our proposed\n",
      "solution is inspired by the research of Jaron Maene et al.[10],\n",
      "which suggests that sparse subnetworks in dense networks can\n",
      "achieve similar accuracy when retrained. This study demon-\n",
      "strates stable training with linear mode connectivity, support-\n",
      "ing the idea that lottery tickets retrain to similar regions, but\n",
      "questions their independence from dense training and iterative\n",
      "pruning.\n",
      "Eran Malach et al.[11] further strengthen the lottery ticket\n",
      "hypothesis by proving that over-parameterized neural networks\n",
      "with random weights always contain a subnetwork matching\n",
      "the accuracy of a target network, without additional training.\n",
      "IV. NEUROLOGICAL PRUNING APPROACH\n",
      "The Neurological Pruning Algorithm is designed to opti-\n",
      "mize neural networks by iteratively pruning less important\n",
      "weights while maintaining high performance. Unlike tradi-\n",
      "tional methods, this approach incorporates adaptive, layer-\n",
      "specific pruning thresholds that dynamically adjust based\n",
      "on weight activations, ensuring efficient model optimization\n",
      "across various stages of training. Drawing inspiration from\n",
      "the Lottery Ticket Hypothesis, which suggests that sparse\n",
      "subnetworks can perform similarly to their dense counterparts\n",
      "when retrained, the algorithm selectively deactivates weights\n",
      "using binary masks to enhance computational efficiency with-\n",
      "out sacrificing accuracy.\n",
      "The algorithm operates in several epochs, during which\n",
      "weight activations are evaluated. Based on a quantile-based\n",
      "strategy, pruning thresholds are established for each layer, en-\n",
      "suring that only weights contributing minimally to the model’s\n",
      "output are removed. After pruning, the model undergoes fine-\n",
      "tuning to restore any lost accuracy and further optimize the\n",
      "network. This step enhances the adaptability and scalability\n",
      "of the pruning method, making it suitable for deployment in\n",
      "resource-constrained environments\n",
      "A. Key Characteristics and Advantages\n",
      "The Neurological Pruning Algorithm boasts several key\n",
      "characteristics that distinguish it from conventional pruning\n",
      "techniques. One notable feature is its layer-specific pruning,\n",
      "which evaluates the importance of weights individually for\n",
      "each layer, leading to a more nuanced and efficient sparsifi-\n",
      "cation process. This approach prevents the model from losing\n",
      "essential features that may otherwise be pruned by a global\n",
      "approach.\n",
      "The algorithm’s use of quantile-based thresholding en-\n",
      "sures that only weights with the least contribution to the\n",
      "model’s output are pruned. This fine-tuned method avoids\n",
      "performance degradation by carefully targeting weights that\n",
      "are less significant to the network’s overall functionality.\n",
      "Furthermore, the pruning process is iterative, allowing for\n",
      "continuous model evaluation and real-time adjustments based\n",
      "on performance feedback, ensuring that accuracy is retained\n",
      "even as sparsity increases.\n",
      "Additionally, the algorithm maintains a careful balance\n",
      "between sparsity and accuracy, enabling a reduction in the\n",
      "model’s computational complexity while preserving its predic-\n",
      "tive capabilities. This characteristic is particularly important\n",
      "for applications requiring rapid inference times, such as in\n",
      "real-time healthcare systems and edge computing scenarios,\n",
      "where both energy efficiency and computational power are\n",
      "crucial.\n",
      "B. Impact on Model Performance\n",
      "The Neurological Pruning Algorithm significantly improves\n",
      "the efficiency of neural networks, making them more suitable\n",
      "for deployment on edge devices. Key performance metrics\n",
      "such as compression ratio (CR) and accuracy retention\n",
      "(AR) are critical indicators of the algorithm’s effectiveness.\n",
      "For instance, in evaluating a ResNet-18 model, the algorithm\n",
      "achieved a CR of 1, indicating that while the number of\n",
      "parameters remained the same, the model’s efficiency im-\n",
      "proved through selective pruning, reducing computational load\n",
      "without sacrificing performance.\n",
      "Moreover, the algorithm demonstrates the ability to achieve\n",
      "high accuracy retention, with models maintaining or even\n",
      "slightly improving their predictive performance post-pruning.\n",
      "This outcome highlights the algorithm’s capability to effi-\n",
      "ciently remove redundant weights while preserving the core\n",
      "functionality of the network. The result is a model that is\n",
      "not only more computationally efficient but also adaptable\n",
      "across different architectures and datasets, making it suit-\n",
      "able for a wide range of applications, from image classification\n",
      "to natural language processing .\n",
      "In practical terms, the Neurological Pruning Algorithm en-\n",
      "ables faster inference times and lower energy consumption,\n",
      "two crucial aspects for deploying AI models in resource-\n",
      "constrained environments. As such, it holds significant poten-\n",
      "tial for improving the viability of real-time, energy-efficient\n",
      "AI applications, particularly in fields like healthcare, where\n",
      "both speed and sustainability are essential.\n",
      "V. NEUROLOGICAL PRUNING ALGORITHM\n",
      "The Neurological Pruning Algorithm is a structured and\n",
      "adaptive methodology designed to optimize neural networks\n",
      "for resource-constrained environments. This algorithm goes\n",
      "beyond traditional pruning approaches by incorporating layer-\n",
      "specific pruning thresholds that dynamically adjust to the\n",
      "importance of weight activations within each layer. Drawing\n",
      "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
      "inforcement learning, the algorithm strikes an optimal balance\n",
      "between model sparsity and accuracy .\n",
      "A. Algorithm Description\n",
      "The Neurological Pruning Algorithm iteratively refines the\n",
      "neural network through a series of training epochs. During\n",
      "each epoch, weight activations are tracked, normalized, and\n",
      "analyzed to identify low-importance weights. These weights\n",
      "are selectively deactivated using binary masks, ensuring com-\n",
      "putational efficiency without compromising the network’s pre-\n",
      "dictive performance. The algorithm further integrates a fine-\n",
      "tuning phase, enabling the pruned network to recover and\n",
      "enhance its accuracy. The detailed process is presented in\n",
      "Algorithm 1.\n",
      "By dynamically adjusting to layer-specific characteristics\n",
      "and maintaining performance metrics, the Neurological Prun-\n",
      "ing Algorithm ensures a significant reduction in computational\n",
      "and memory requirements, paving the way for efficient deploy-\n",
      "ment of deep learning models on edge devices .\n",
      "B. Input Parameters and Output\n",
      "Inputs:\n",
      "• Pretrained model (M)\n",
      "• Dataset (D)\n",
      "• Prune fraction (p)\n",
      "• Total epochs (E)\n",
      "Output:\n",
      "• Pruned and fine-tuned model (Mpruned)\n",
      "C. Key Features and Advantages\n",
      "The Neurological Pruning Algorithm is designed with sev-\n",
      "eral distinctive features that contribute to its effectiveness\n",
      "in optimizing neural network performance. These features\n",
      "include:\n",
      "Algorithm 1 Simplified Neurological Pruning Algorithm\n",
      "Require: Pretrained model M, Dataset D, Prune fraction p,\n",
      "Total epochs E\n",
      "Ensure: Pruned and fine-tuned model Mpruned\n",
      "1: Initialize binary masks M with all ones\n",
      "2: for each epoch e = 1 to E do\n",
      "3:\n",
      "Compute weight activations A during forward passes\n",
      "4:\n",
      "for each layer i in M do\n",
      "5:\n",
      "Compute pruning threshold Ti as p-quantile of Ai\n",
      "6:\n",
      "Update mask Mi: retain weights with Ai > Ti\n",
      "7:\n",
      "Apply mask to layer: θi ←θi · Mi\n",
      "8:\n",
      "end for\n",
      "9:\n",
      "Evaluate model performance and update metrics\n",
      "10: end for\n",
      "11: Fine-tune Mpruned on D\n",
      "12: return Mpruned\n",
      "• Layer-specific Pruning: The algorithm applies pruning\n",
      "on a per-layer basis, evaluating the weight activations\n",
      "within each layer individually. This approach ensures that\n",
      "the sparsity introduced by pruning is well-distributed and\n",
      "context-dependent, facilitating more nuanced control over\n",
      "model optimization .\n",
      "• Quantile-based Thresholding: A quantile-based strat-\n",
      "egy is employed to determine the pruning threshold for\n",
      "each layer. This method ensures that only weights with\n",
      "minimal contribution to the network’s output are pruned,\n",
      "thereby reducing the risk of adversely affecting model\n",
      "accuracy while enhancing sparsity .\n",
      "• Iterative Pruning and Evaluation: The pruning pro-\n",
      "cess follows an iterative approach, enabling continuous\n",
      "assessment of the model’s performance after each pruning\n",
      "step. This ensures dynamic adjustments to maintain a bal-\n",
      "ance between model sparsity and predictive performance\n",
      "throughout training.\n",
      "• Preservation of Model Accuracy: By adjusting pruning\n",
      "thresholds in response to ongoing performance metrics,\n",
      "the algorithm ensures that model accuracy is not com-\n",
      "promised. The careful tuning of these thresholds allows\n",
      "for significant sparsity while maintaining the robustness\n",
      "of the network’s predictive capabilities .\n",
      "D. Impact on Neural Network Performance\n",
      "The Neurological Pruning Algorithm has been extensively\n",
      "evaluated across various benchmark datasets and model archi-\n",
      "tectures, demonstrating its capacity to achieve the following\n",
      "outcomes:\n",
      "• High Sparsity with Minimal Accuracy Loss: The algo-\n",
      "rithm effectively prunes unnecessary weights, achieving\n",
      "high sparsity levels without causing significant degrada-\n",
      "tion in model accuracy. This outcome is critical for the\n",
      "efficient deployment of models in resource-constrained\n",
      "environments.\n",
      "• Reduced Computational Complexity: By sparsifying\n",
      "the weight matrices, the algorithm reduces the compu-\n",
      "tational load during inference, which translates to faster\n",
      "model execution and lower resource consumption. This\n",
      "characteristic is especially beneficial in edge computing\n",
      "and real-time applications .\n",
      "• Broad Applicability: The algorithm is highly flexible\n",
      "and can be adapted to various neural network architec-\n",
      "tures and datasets, making it suitable for a wide range\n",
      "of tasks, from image recognition to natural language\n",
      "processing .\n",
      "VI. EVALUATION METRICS\n",
      "A. Performance Comparison\n",
      "Fig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\n",
      "TABLE I\n",
      "COMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\n",
      "Metric\n",
      "Unpruned\n",
      "Model\n",
      "Pruned\n",
      "Model\n",
      "Accuracy\n",
      "0.9562\n",
      "0.9583\n",
      "Train Loss\n",
      "0.1414\n",
      "0.1625\n",
      "Inference\n",
      "Time\n",
      "(s)\n",
      "0.0318\n",
      "0.0314\n",
      "No of Parame-\n",
      "ters(million)\n",
      "11.7\n",
      "11.7\n",
      "B. Evaluation Metrics Calculation\n",
      "The key metrics used to evaluate the proposed pruning\n",
      "technique are as follows:\n",
      "Compression Ratio (CR): The Compression Ratio (CR) is\n",
      "calculated as:\n",
      "CR = Total Parameters After Pruning\n",
      "Total Parameters Before Pruning\n",
      "(1)\n",
      "CR = 11.7M\n",
      "11.7M = 1\n",
      "Accuracy Retention (AR): The Accuracy Retention (AR) is\n",
      "calculated as:\n",
      "AR =\n",
      "\u0012Accuracy After Pruning −Baseline Accuracy\n",
      "Baseline Accuracy\n",
      "\u0013\n",
      "×100\n",
      "(2)\n",
      "AR =\n",
      "\u00120.958284 −0.956206\n",
      "0.956206\n",
      "\u0013\n",
      "× 100 = 0.22%\n",
      "This result indicates that the pruned model achieves an ac-\n",
      "curacy improvement of 0.22% when compared to unpruned\n",
      "model.\n",
      "C. Illustration of Results\n",
      "Fig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\n",
      "18.\n",
      "D. Discussion\n",
      "The evaluation metrics highlight the effectiveness of the\n",
      "proposed Neurological Pruning method on the ResNet-18\n",
      "model. The Compression Ratio (CR) remains 1, indicating\n",
      "no reduction in the total number of parameters, which suggests\n",
      "that this approach optimizes the neural network by selectively\n",
      "pruning connections rather than reducing the overall parameter\n",
      "count. The Accuracy Retention (AR), which is approximately\n",
      "0.22%, demonstrates that the Neurological Pruning technique\n",
      "successfully improved the accuracy compared to the baseline\n",
      "model.\n",
      "These results validate the potential of Neurological Pruning\n",
      "as an innovative approach to optimizing deep learning models.\n",
      "By maintaining accuracy while likely enhancing sparsity and\n",
      "computational efficiency, this method presents a promising di-\n",
      "rection for achieving high-performance models under resource\n",
      "constraints.\n",
      "VII. CONCLUSION\n",
      "In summary, the Neurological Pruning Algorithm provides\n",
      "a systematic and principled approach to optimizing neural\n",
      "networks. By balancing the trade-off between sparsity and\n",
      "accuracy, it enables significant reductions in computational\n",
      "complexity without sacrificing the network’s predictive per-\n",
      "formance. Future work will focus on extending the algorithm\n",
      "to dynamic network architectures and incorporating advanced\n",
      "regularization techniques to further enhance its robustness and\n",
      "adaptability in diverse applications.\n",
      "REFERENCES\n",
      "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\n",
      "Fluctuation-based Adaptive Structured Pruning for Large Language\n",
      "Models.\n",
      "[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\n",
      "Simplification of Deep Neural Network-Based Object Detector for Real-\n",
      "Time Edge Computing.\n",
      "[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\n",
      "Devices Object Detection by Filter Pruning.\n",
      "[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\n",
      "and Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\n",
      "Tracking Algorithms in Edge Devices.\n",
      "[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\n",
      "Chen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\n",
      "Compression Method for Object Detection Network for Edge Devices.\n",
      "[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\n",
      "Cao. Absorption Pruning of Deep Neural Network for Object Detection\n",
      "in Remote Sensing Imagery.\n",
      "[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\n",
      "and Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\n",
      "Distribution Detection: An Empirical Survey.\n",
      "[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\n",
      "Sahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\n",
      "[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\n",
      "Compress DNNs.\n",
      "[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\n",
      "derstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\n",
      "[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\n",
      "Proving the Lottery Ticket Hypothesis: Pruning is All You Need.\n",
      "[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\n",
      "Devices via Reconstruction-Based Channel Pruning.\n",
      "[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\n",
      "Determine Reliable Paths on Networks with Random and Correlated\n",
      "Link Travel Times.\n",
      "[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\n",
      "and D. Suganthi. Real-Time Object Detection on Edge Devices Using\n",
      "Mobile Neural Networks.\n",
      "[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\n",
      "Tang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\n",
      "Models.\n",
      "[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\n",
      "Li, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\n",
      "Deng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\n",
      "Detection of Tea Leaves with One Bud and Two Leaves Based on\n",
      "ShuffleNetv2-YOLOv5-Lite-E.\n",
      "\n",
      "    Metadata: {'source': 'geo_facts.txt', 'page': 1}\n",
      "  Source 2:\n",
      "    Content: The Great Wall of China is located in China and is a famous landmark.\n",
      "    Metadata: {'source': 'world_landmarks.txt', 'page': 5}\n",
      "--- End of Output ---\n"
     ]
    }
   ],
   "source": [
    "# rag_faiss_system.py\n",
    "\n",
    "# --- Dependencies ---\n",
    "# Make sure you have installed the necessary packages:\n",
    "# pip install langchain faiss-cpu sentence-transformers langchain-google-genai python-dotenv\n",
    "# (or faiss-gpu if you have CUDA installed and want GPU acceleration)\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS # Use langchain_community\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings # Use langchain_community\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # Import Gemini\n",
    "from langchain.docstore.document import Document # For structured documents\n",
    "\n",
    "# --- Configuration ---\n",
    "# Load environment variables (especially GOOGLE_API_KEY)\n",
    "load_dotenv() \n",
    "\n",
    "# Check if the API key is available\n",
    "if os.getenv(\"GOOGLE_API_KEY\") is None:\n",
    "    print(\"Error: GOOGLE_API_KEY environment variable not set.\")\n",
    "    print(\"Please get an API key from Google AI Studio (https://aistudio.google.com/app/apikey)\")\n",
    "    print(\"and set it as an environment variable (e.g., in a .env file).\")\n",
    "    exit() # Exit if the key is missing\n",
    "\n",
    "# --- Main Function ---\n",
    "# Step 1: Initialize the embedding model\n",
    "print(\"Initializing embedding model...\")\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'} # Specify device (cpu or cuda)\n",
    "    # Set encode_kwargs={'normalize_embeddings': True} if using cosine similarity,\n",
    "    # but FAISS IndexFlatL2 uses L2 distance (Euclidean), so normalization is optional \n",
    "    # but often helpful. Let's keep it simple for L2.\n",
    ")\n",
    "print(\"Embedding model initialized.\")\n",
    "\n",
    "# Step 2 & 3: Prepare documents and create FAISS index\n",
    "print(\"Preparing documents and creating FAISS vector store...\")\n",
    "documents_data = [\n",
    "    {\"content\": \"\"\"\n",
    "        Neuro Prune: An Adaptive Approach for Efficient\n",
    "Deep Neural Network Optimization on Edge\n",
    "Devices\n",
    "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
    "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
    "grk.cse@psgtech.ac.in\n",
    "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
    "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
    "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
    "Abstract—The optimization of deep neural networks for de-\n",
    "ployment on edge devices is a significant research area due to\n",
    "the demand for applications such as augmented reality, smart\n",
    "cameras, and autonomous navigation. However, deploying large\n",
    "deep learning models on edge devices poses challenges related to\n",
    "computational power, energy consumption, and latency. Pruning\n",
    "is a method to reduce the model size, accelerate inference, and\n",
    "save power. The objective of the paper is to propose the Neuro\n",
    "Prune algorithm and to apply it for the optimization of deep\n",
    "neural networks on edge devices. Efforts have been made to\n",
    "compare pruned and unpruned models. As a result, the pruned\n",
    "model has an accuracy increase of 0.22%.\n",
    "I. INTRODUCTION\n",
    "The rise of intelligent systems, ranging from autonomous\n",
    "drones to augmented reality (AR) devices and smart surveil-\n",
    "lance cameras, has intensified the need for efficient deep neural\n",
    "network deployment on edge devices with constrained com-\n",
    "putational resources. While advanced deep learning models\n",
    "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
    "their deployment on such devices is hindered by significant\n",
    "computational demands, large memory footprints, and high\n",
    "energy consumption. These challenges pose critical barriers to\n",
    "real-time applications and sustainable deployment.\n",
    "This paper builds on existing pruning techniques, includ-\n",
    "ing unstructured pruning, structured pruning, and approaches\n",
    "inspired by quantization, highlighting their strengths and lim-\n",
    "itations. In response to the challenges identified, we introduce\n",
    "Neuro Prune, a novel pruning approach that integrates princi-\n",
    "ples from reinforcement learning and the Lottery Ticket Hy-\n",
    "pothesis to achieve an optimal balance between accuracy, com-\n",
    "putational efficiency, and energy consumption. By integrating\n",
    "these principles, Neuro Prune provides a robust framework\n",
    "for optimizing deep neural networks for deployment on edge\n",
    "devices without sacrificing performance.\n",
    "Neuro Prune reduces model size and computational re-\n",
    "quirements while maintaining or even improving the model’s\n",
    "accuracy. These findings underscore the potential of Neuro\n",
    "Prune to enable real-time, energy-efficient deep learning on\n",
    "resource-constrained devices.\n",
    "Neuro Prune employs a systematic process to optimize\n",
    "neural networks for edge deployment:\n",
    "• Mask Initialization: Each layer’s weights are paired with\n",
    "a binary mask (original_mask), initialized to ones.\n",
    "• Activation Tracking: During forward passes over the\n",
    "dataset, the magnitudes of weight activations are mon-\n",
    "itored and recorded.\n",
    "• Normalize and Prune: The recorded activations are\n",
    "normalized, and a threshold based on the pruning fraction\n",
    "is computed. Using this threshold, the masks are dynam-\n",
    "ically updated to prune unimportant weights.\n",
    "• Evaluate and Log Metrics: After pruning, metrics such\n",
    "as model sparsity and accuracy are computed. A reward\n",
    "metric is derived to balance accuracy and sparsity using\n",
    "a tunable parameter, λweight.\n",
    "• Retrain the Model: The pruned model undergoes retrain-\n",
    "ing to improve the loss in accuracy while maintaining low\n",
    "computational requirements\n",
    "Neuro Prune’s unique combination of reinforcement learn-\n",
    "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
    "pothesis for identifying optimal subnetworks enables it to\n",
    "achieve remarkable efficiency. This approach ensures substan-\n",
    "tial reduction in model size and energy consumption while\n",
    "preserving accuracy and inference speed, making it an ideal\n",
    "solution for edge-based deep learning applications.\n",
    "Through extensive empirical evaluations and theoretical\n",
    "insights, this paper demonstrates how Neuro Pruning outper-\n",
    "forms traditional pruning methods, setting a new benchmark\n",
    "for sustainable, high-performance AI deployment on edge\n",
    "devices. By situating our work within the broader research\n",
    "context, we aim to offer a critical contribution to the ongoing\n",
    "evolution of neural network optimization techniques.\n",
    "II. BACKGROUND\n",
    "A. Need for Pruning Techniques\n",
    "Pruning is a crucial optimization technique for deploying\n",
    "deep neural networks in resource-constrained environments\n",
    "like healthcare. For example, wearable devices like smart-\n",
    "watches that monitor heart rates or portable EEG systems\n",
    "for brain activity analysis require lightweight models due to\n",
    "their limited computational power and memory. Pruning helps\n",
    "address these constraints by reducing the memory footprint\n",
    "and computational load, enabling efficient model deployment.\n",
    "Additionally, it minimizes energy consumption, which is vital\n",
    "for sustainable and real-time healthcare applications. Faster\n",
    "inference enabled by pruning is particularly beneficial for\n",
    "time-sensitive tasks like seizure detection or emergency di-\n",
    "agnostics. Moreover, pruning enhances model interpretability\n",
    "by retaining only the most critical components, which is\n",
    "essential for gaining trust and ensuring regulatory compliance\n",
    "in healthcare. By reducing the hardware and operational costs,\n",
    "pruning also makes deploying advanced AI solutions on edge\n",
    "devices more cost-effective and accessible.\n",
    "B. Types of Pruning Techniques\n",
    "Pruning methods are broadly classified based on granu-\n",
    "larity, timing, and approach. Granularity-based pruning in-\n",
    "cludes structured pruning, which removes entire components\n",
    "such as neurons or filters to streamline computations, and\n",
    "unstructured pruning, which eliminates individual weights\n",
    "for finer optimization, though it often requires specialized\n",
    "hardware. Timing-based pruning involves pre-training pruning,\n",
    "which optimizes models before training; post-training prun-\n",
    "ing, where redundant components are removed from trained\n",
    "models with possible retraining; and dynamic pruning during\n",
    "training, which adjusts models in real-time for improved\n",
    "adaptability. Metric-based pruning uses parameters like weight\n",
    "magnitude or gradient contribution to identify and remove\n",
    "less critical components, while application-specific pruning\n",
    "tailors strategies to specific tasks, such as optimizing for EEG\n",
    "signal analysis, or hardware constraints like GPUs. Together,\n",
    "these methods enable the development of efficient, scalable,\n",
    "and task-specific models, particularly suited for edge device\n",
    "deployment in healthcare and other domains.\n",
    "III. RELATED WORK\n",
    "Jielei Wang et al.[12] introduced an absorption pruning\n",
    "method for object detection in remote sensing imagery. This\n",
    "study achieves efficient compression with minimal accuracy\n",
    "loss but requires careful layer-wise pruning ratio tuning.\n",
    "Jan Muller et al., in their research[12], propose a neural\n",
    "network pruning method for multi-object tracking (MOT) that\n",
    "reduces model size by up to 70\n",
    "Liang Li et al., in their work[9], introduce a novel pruning\n",
    "method for DNNs that utilizes a self-adaptive mechanism\n",
    "based on weight sparsity ratios and a protective reconstruction\n",
    "mechanism. Their approach improves both model compres-\n",
    "sion and accuracy, outperforming state-of-the-art methods on\n",
    "CIFAR-10 and ImageNet datasets.\n",
    "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
    "learning with edge devices like Google Coral AI and Nvidia\n",
    "Jetson Nano. Their work revolutionizes computer vision and\n",
    "real-time tracking but raises energy concerns due to the\n",
    "computational demands of advanced algorithms.\n",
    "Satoru Koda et al.[7] explore how weight pruning in\n",
    "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
    "larly with Mahalanobis-based approaches, by improving global\n",
    "feature extraction and leveraging weights not critical for\n",
    "classification.\n",
    "Shvetha S Kumar et al. analyze and compare three pruning\n",
    "techniques—L1-norm filter pruning, channel pruning, and\n",
    "weight pruning—on CNNs for accuracy and inference time,\n",
    "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
    "pared to GTX1080 Ti in their study[8].\n",
    "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
    "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
    "on stochastic networks, extending it to related problems and\n",
    "demonstrating its efficiency on real-world networks in their\n",
    "study[13].\n",
    "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
    "Training outperforms Post-Training Quantization, and sparsity\n",
    "training with Optimal Reduction pruning improves YOLOv4’s\n",
    "efficiency and edge-device performance, despite potential\n",
    "high-pruning rate drawbacks.\n",
    "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
    "method for DNNs. This approach optimizes inference speed\n",
    "and resource efficiency on edge devices with minimal accuracy\n",
    "loss but requires careful tuning of layer-specific pruning ratios.\n",
    "Yongqi An et al.[1] introduce a retraining-free structured\n",
    "pruning framework for Large Language Models (LLMs). Their\n",
    "method incorporates a fluctuation-based pruning metric, adap-\n",
    "tive structure search, and a bias compensation mechanism to\n",
    "achieve efficient pruning. FLAP significantly enhances infer-\n",
    "ence speed and reduces model size without requiring retrain-\n",
    "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
    "and Wanda-sp on LLaMA models across various benchmarks.\n",
    "From referring to these research papers, we derived an\n",
    "idea for pruning on edge devices. Additionally, our proposed\n",
    "solution is inspired by the research of Jaron Maene et al.[10],\n",
    "which suggests that sparse subnetworks in dense networks can\n",
    "achieve similar accuracy when retrained. This study demon-\n",
    "strates stable training with linear mode connectivity, support-\n",
    "ing the idea that lottery tickets retrain to similar regions, but\n",
    "questions their independence from dense training and iterative\n",
    "pruning.\n",
    "Eran Malach et al.[11] further strengthen the lottery ticket\n",
    "hypothesis by proving that over-parameterized neural networks\n",
    "with random weights always contain a subnetwork matching\n",
    "the accuracy of a target network, without additional training.\n",
    "IV. NEUROLOGICAL PRUNING APPROACH\n",
    "The Neurological Pruning Algorithm is designed to opti-\n",
    "mize neural networks by iteratively pruning less important\n",
    "weights while maintaining high performance. Unlike tradi-\n",
    "tional methods, this approach incorporates adaptive, layer-\n",
    "specific pruning thresholds that dynamically adjust based\n",
    "on weight activations, ensuring efficient model optimization\n",
    "across various stages of training. Drawing inspiration from\n",
    "the Lottery Ticket Hypothesis, which suggests that sparse\n",
    "subnetworks can perform similarly to their dense counterparts\n",
    "when retrained, the algorithm selectively deactivates weights\n",
    "using binary masks to enhance computational efficiency with-\n",
    "out sacrificing accuracy.\n",
    "The algorithm operates in several epochs, during which\n",
    "weight activations are evaluated. Based on a quantile-based\n",
    "strategy, pruning thresholds are established for each layer, en-\n",
    "suring that only weights contributing minimally to the model’s\n",
    "output are removed. After pruning, the model undergoes fine-\n",
    "tuning to restore any lost accuracy and further optimize the\n",
    "network. This step enhances the adaptability and scalability\n",
    "of the pruning method, making it suitable for deployment in\n",
    "resource-constrained environments\n",
    "A. Key Characteristics and Advantages\n",
    "The Neurological Pruning Algorithm boasts several key\n",
    "characteristics that distinguish it from conventional pruning\n",
    "techniques. One notable feature is its layer-specific pruning,\n",
    "which evaluates the importance of weights individually for\n",
    "each layer, leading to a more nuanced and efficient sparsifi-\n",
    "cation process. This approach prevents the model from losing\n",
    "essential features that may otherwise be pruned by a global\n",
    "approach.\n",
    "The algorithm’s use of quantile-based thresholding en-\n",
    "sures that only weights with the least contribution to the\n",
    "model’s output are pruned. This fine-tuned method avoids\n",
    "performance degradation by carefully targeting weights that\n",
    "are less significant to the network’s overall functionality.\n",
    "Furthermore, the pruning process is iterative, allowing for\n",
    "continuous model evaluation and real-time adjustments based\n",
    "on performance feedback, ensuring that accuracy is retained\n",
    "even as sparsity increases.\n",
    "Additionally, the algorithm maintains a careful balance\n",
    "between sparsity and accuracy, enabling a reduction in the\n",
    "model’s computational complexity while preserving its predic-\n",
    "tive capabilities. This characteristic is particularly important\n",
    "for applications requiring rapid inference times, such as in\n",
    "real-time healthcare systems and edge computing scenarios,\n",
    "where both energy efficiency and computational power are\n",
    "crucial.\n",
    "B. Impact on Model Performance\n",
    "The Neurological Pruning Algorithm significantly improves\n",
    "the efficiency of neural networks, making them more suitable\n",
    "for deployment on edge devices. Key performance metrics\n",
    "such as compression ratio (CR) and accuracy retention\n",
    "(AR) are critical indicators of the algorithm’s effectiveness.\n",
    "For instance, in evaluating a ResNet-18 model, the algorithm\n",
    "achieved a CR of 1, indicating that while the number of\n",
    "parameters remained the same, the model’s efficiency im-\n",
    "proved through selective pruning, reducing computational load\n",
    "without sacrificing performance.\n",
    "Moreover, the algorithm demonstrates the ability to achieve\n",
    "high accuracy retention, with models maintaining or even\n",
    "slightly improving their predictive performance post-pruning.\n",
    "This outcome highlights the algorithm’s capability to effi-\n",
    "ciently remove redundant weights while preserving the core\n",
    "functionality of the network. The result is a model that is\n",
    "not only more computationally efficient but also adaptable\n",
    "across different architectures and datasets, making it suit-\n",
    "able for a wide range of applications, from image classification\n",
    "to natural language processing .\n",
    "In practical terms, the Neurological Pruning Algorithm en-\n",
    "ables faster inference times and lower energy consumption,\n",
    "two crucial aspects for deploying AI models in resource-\n",
    "constrained environments. As such, it holds significant poten-\n",
    "tial for improving the viability of real-time, energy-efficient\n",
    "AI applications, particularly in fields like healthcare, where\n",
    "both speed and sustainability are essential.\n",
    "V. NEUROLOGICAL PRUNING ALGORITHM\n",
    "The Neurological Pruning Algorithm is a structured and\n",
    "adaptive methodology designed to optimize neural networks\n",
    "for resource-constrained environments. This algorithm goes\n",
    "beyond traditional pruning approaches by incorporating layer-\n",
    "specific pruning thresholds that dynamically adjust to the\n",
    "importance of weight activations within each layer. Drawing\n",
    "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
    "inforcement learning, the algorithm strikes an optimal balance\n",
    "between model sparsity and accuracy .\n",
    "A. Algorithm Description\n",
    "The Neurological Pruning Algorithm iteratively refines the\n",
    "neural network through a series of training epochs. During\n",
    "each epoch, weight activations are tracked, normalized, and\n",
    "analyzed to identify low-importance weights. These weights\n",
    "are selectively deactivated using binary masks, ensuring com-\n",
    "putational efficiency without compromising the network’s pre-\n",
    "dictive performance. The algorithm further integrates a fine-\n",
    "tuning phase, enabling the pruned network to recover and\n",
    "enhance its accuracy. The detailed process is presented in\n",
    "Algorithm 1.\n",
    "By dynamically adjusting to layer-specific characteristics\n",
    "and maintaining performance metrics, the Neurological Prun-\n",
    "ing Algorithm ensures a significant reduction in computational\n",
    "and memory requirements, paving the way for efficient deploy-\n",
    "ment of deep learning models on edge devices .\n",
    "B. Input Parameters and Output\n",
    "Inputs:\n",
    "• Pretrained model (M)\n",
    "• Dataset (D)\n",
    "• Prune fraction (p)\n",
    "• Total epochs (E)\n",
    "Output:\n",
    "• Pruned and fine-tuned model (Mpruned)\n",
    "C. Key Features and Advantages\n",
    "The Neurological Pruning Algorithm is designed with sev-\n",
    "eral distinctive features that contribute to its effectiveness\n",
    "in optimizing neural network performance. These features\n",
    "include:\n",
    "Algorithm 1 Simplified Neurological Pruning Algorithm\n",
    "Require: Pretrained model M, Dataset D, Prune fraction p,\n",
    "Total epochs E\n",
    "Ensure: Pruned and fine-tuned model Mpruned\n",
    "1: Initialize binary masks M with all ones\n",
    "2: for each epoch e = 1 to E do\n",
    "3:\n",
    "Compute weight activations A during forward passes\n",
    "4:\n",
    "for each layer i in M do\n",
    "5:\n",
    "Compute pruning threshold Ti as p-quantile of Ai\n",
    "6:\n",
    "Update mask Mi: retain weights with Ai > Ti\n",
    "7:\n",
    "Apply mask to layer: θi ←θi · Mi\n",
    "8:\n",
    "end for\n",
    "9:\n",
    "Evaluate model performance and update metrics\n",
    "10: end for\n",
    "11: Fine-tune Mpruned on D\n",
    "12: return Mpruned\n",
    "• Layer-specific Pruning: The algorithm applies pruning\n",
    "on a per-layer basis, evaluating the weight activations\n",
    "within each layer individually. This approach ensures that\n",
    "the sparsity introduced by pruning is well-distributed and\n",
    "context-dependent, facilitating more nuanced control over\n",
    "model optimization .\n",
    "• Quantile-based Thresholding: A quantile-based strat-\n",
    "egy is employed to determine the pruning threshold for\n",
    "each layer. This method ensures that only weights with\n",
    "minimal contribution to the network’s output are pruned,\n",
    "thereby reducing the risk of adversely affecting model\n",
    "accuracy while enhancing sparsity .\n",
    "• Iterative Pruning and Evaluation: The pruning pro-\n",
    "cess follows an iterative approach, enabling continuous\n",
    "assessment of the model’s performance after each pruning\n",
    "step. This ensures dynamic adjustments to maintain a bal-\n",
    "ance between model sparsity and predictive performance\n",
    "throughout training.\n",
    "• Preservation of Model Accuracy: By adjusting pruning\n",
    "thresholds in response to ongoing performance metrics,\n",
    "the algorithm ensures that model accuracy is not com-\n",
    "promised. The careful tuning of these thresholds allows\n",
    "for significant sparsity while maintaining the robustness\n",
    "of the network’s predictive capabilities .\n",
    "D. Impact on Neural Network Performance\n",
    "The Neurological Pruning Algorithm has been extensively\n",
    "evaluated across various benchmark datasets and model archi-\n",
    "tectures, demonstrating its capacity to achieve the following\n",
    "outcomes:\n",
    "• High Sparsity with Minimal Accuracy Loss: The algo-\n",
    "rithm effectively prunes unnecessary weights, achieving\n",
    "high sparsity levels without causing significant degrada-\n",
    "tion in model accuracy. This outcome is critical for the\n",
    "efficient deployment of models in resource-constrained\n",
    "environments.\n",
    "• Reduced Computational Complexity: By sparsifying\n",
    "the weight matrices, the algorithm reduces the compu-\n",
    "tational load during inference, which translates to faster\n",
    "model execution and lower resource consumption. This\n",
    "characteristic is especially beneficial in edge computing\n",
    "and real-time applications .\n",
    "• Broad Applicability: The algorithm is highly flexible\n",
    "and can be adapted to various neural network architec-\n",
    "tures and datasets, making it suitable for a wide range\n",
    "of tasks, from image recognition to natural language\n",
    "processing .\n",
    "VI. EVALUATION METRICS\n",
    "A. Performance Comparison\n",
    "Fig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\n",
    "TABLE I\n",
    "COMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\n",
    "Metric\n",
    "Unpruned\n",
    "Model\n",
    "Pruned\n",
    "Model\n",
    "Accuracy\n",
    "0.9562\n",
    "0.9583\n",
    "Train Loss\n",
    "0.1414\n",
    "0.1625\n",
    "Inference\n",
    "Time\n",
    "(s)\n",
    "0.0318\n",
    "0.0314\n",
    "No of Parame-\n",
    "ters(million)\n",
    "11.7\n",
    "11.7\n",
    "B. Evaluation Metrics Calculation\n",
    "The key metrics used to evaluate the proposed pruning\n",
    "technique are as follows:\n",
    "Compression Ratio (CR): The Compression Ratio (CR) is\n",
    "calculated as:\n",
    "CR = Total Parameters After Pruning\n",
    "Total Parameters Before Pruning\n",
    "(1)\n",
    "CR = 11.7M\n",
    "11.7M = 1\n",
    "Accuracy Retention (AR): The Accuracy Retention (AR) is\n",
    "calculated as:\n",
    "AR =\n",
    "\u0012Accuracy After Pruning −Baseline Accuracy\n",
    "Baseline Accuracy\n",
    "\u0013\n",
    "×100\n",
    "(2)\n",
    "AR =\n",
    "\u00120.958284 −0.956206\n",
    "0.956206\n",
    "\u0013\n",
    "× 100 = 0.22%\n",
    "This result indicates that the pruned model achieves an ac-\n",
    "curacy improvement of 0.22% when compared to unpruned\n",
    "model.\n",
    "C. Illustration of Results\n",
    "Fig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\n",
    "18.\n",
    "D. Discussion\n",
    "The evaluation metrics highlight the effectiveness of the\n",
    "proposed Neurological Pruning method on the ResNet-18\n",
    "model. The Compression Ratio (CR) remains 1, indicating\n",
    "no reduction in the total number of parameters, which suggests\n",
    "that this approach optimizes the neural network by selectively\n",
    "pruning connections rather than reducing the overall parameter\n",
    "count. The Accuracy Retention (AR), which is approximately\n",
    "0.22%, demonstrates that the Neurological Pruning technique\n",
    "successfully improved the accuracy compared to the baseline\n",
    "model.\n",
    "These results validate the potential of Neurological Pruning\n",
    "as an innovative approach to optimizing deep learning models.\n",
    "By maintaining accuracy while likely enhancing sparsity and\n",
    "computational efficiency, this method presents a promising di-\n",
    "rection for achieving high-performance models under resource\n",
    "constraints.\n",
    "VII. CONCLUSION\n",
    "In summary, the Neurological Pruning Algorithm provides\n",
    "a systematic and principled approach to optimizing neural\n",
    "networks. By balancing the trade-off between sparsity and\n",
    "accuracy, it enables significant reductions in computational\n",
    "complexity without sacrificing the network’s predictive per-\n",
    "formance. Future work will focus on extending the algorithm\n",
    "to dynamic network architectures and incorporating advanced\n",
    "regularization techniques to further enhance its robustness and\n",
    "adaptability in diverse applications.\n",
    "REFERENCES\n",
    "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\n",
    "Fluctuation-based Adaptive Structured Pruning for Large Language\n",
    "Models.\n",
    "[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\n",
    "Simplification of Deep Neural Network-Based Object Detector for Real-\n",
    "Time Edge Computing.\n",
    "[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\n",
    "Devices Object Detection by Filter Pruning.\n",
    "[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\n",
    "and Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\n",
    "Tracking Algorithms in Edge Devices.\n",
    "[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\n",
    "Chen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\n",
    "Compression Method for Object Detection Network for Edge Devices.\n",
    "[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\n",
    "Cao. Absorption Pruning of Deep Neural Network for Object Detection\n",
    "in Remote Sensing Imagery.\n",
    "[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\n",
    "and Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\n",
    "Distribution Detection: An Empirical Survey.\n",
    "[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\n",
    "Sahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\n",
    "[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\n",
    "Compress DNNs.\n",
    "[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\n",
    "derstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\n",
    "[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\n",
    "Proving the Lottery Ticket Hypothesis: Pruning is All You Need.\n",
    "[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\n",
    "Devices via Reconstruction-Based Channel Pruning.\n",
    "[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\n",
    "Determine Reliable Paths on Networks with Random and Correlated\n",
    "Link Travel Times.\n",
    "[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\n",
    "and D. Suganthi. Real-Time Object Detection on Edge Devices Using\n",
    "Mobile Neural Networks.\n",
    "[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\n",
    "Tang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\n",
    "Models.\n",
    "[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\n",
    "Li, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\n",
    "Deng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\n",
    "Detection of Tea Leaves with One Bud and Two Leaves Based on\n",
    "ShuffleNetv2-YOLOv5-Lite-E.\n",
    "Neuro Prune: An Adaptive Approach for Efficient\n",
    "Deep Neural Network Optimization on Edge\n",
    "Devices\n",
    "Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5\n",
    "1Professor, Department of Computer Science and Engineering, PSG College of Technology\n",
    "grk.cse@psgtech.ac.in\n",
    "2,3,4,5BE-CSE (AI&ML), PSG College of Technology\n",
    "adhishthesak@gmail.com, mohanakumarp2828@gmail.com\n",
    "sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com\n",
    "Abstract—The optimization of deep neural networks for de-\n",
    "ployment on edge devices is a significant research area due to\n",
    "the demand for applications such as augmented reality, smart\n",
    "cameras, and autonomous navigation. However, deploying large\n",
    "deep learning models on edge devices poses challenges related to\n",
    "computational power, energy consumption, and latency. Pruning\n",
    "is a method to reduce the model size, accelerate inference, and\n",
    "save power. The objective of the paper is to propose the Neuro\n",
    "Prune algorithm and to apply it for the optimization of deep\n",
    "neural networks on edge devices. Efforts have been made to\n",
    "compare pruned and unpruned models. As a result, the pruned\n",
    "model has an accuracy increase of 0.22%.\n",
    "I. INTRODUCTION\n",
    "The rise of intelligent systems, ranging from autonomous\n",
    "drones to augmented reality (AR) devices and smart surveil-\n",
    "lance cameras, has intensified the need for efficient deep neural\n",
    "network deployment on edge devices with constrained com-\n",
    "putational resources. While advanced deep learning models\n",
    "like YOLO, SSD, ResNet, and MobileNet excel in accuracy,\n",
    "their deployment on such devices is hindered by significant\n",
    "computational demands, large memory footprints, and high\n",
    "energy consumption. These challenges pose critical barriers to\n",
    "real-time applications and sustainable deployment.\n",
    "This paper builds on existing pruning techniques, includ-\n",
    "ing unstructured pruning, structured pruning, and approaches\n",
    "inspired by quantization, highlighting their strengths and lim-\n",
    "itations. In response to the challenges identified, we introduce\n",
    "Neuro Prune, a novel pruning approach that integrates princi-\n",
    "ples from reinforcement learning and the Lottery Ticket Hy-\n",
    "pothesis to achieve an optimal balance between accuracy, com-\n",
    "putational efficiency, and energy consumption. By integrating\n",
    "these principles, Neuro Prune provides a robust framework\n",
    "for optimizing deep neural networks for deployment on edge\n",
    "devices without sacrificing performance.\n",
    "Neuro Prune reduces model size and computational re-\n",
    "quirements while maintaining or even improving the model’s\n",
    "accuracy. These findings underscore the potential of Neuro\n",
    "Prune to enable real-time, energy-efficient deep learning on\n",
    "resource-constrained devices.\n",
    "Neuro Prune employs a systematic process to optimize\n",
    "neural networks for edge deployment:\n",
    "• Mask Initialization: Each layer’s weights are paired with\n",
    "a binary mask (original_mask), initialized to ones.\n",
    "• Activation Tracking: During forward passes over the\n",
    "dataset, the magnitudes of weight activations are mon-\n",
    "itored and recorded.\n",
    "• Normalize and Prune: The recorded activations are\n",
    "normalized, and a threshold based on the pruning fraction\n",
    "is computed. Using this threshold, the masks are dynam-\n",
    "ically updated to prune unimportant weights.\n",
    "• Evaluate and Log Metrics: After pruning, metrics such\n",
    "as model sparsity and accuracy are computed. A reward\n",
    "metric is derived to balance accuracy and sparsity using\n",
    "a tunable parameter, λweight.\n",
    "• Retrain the Model: The pruned model undergoes retrain-\n",
    "ing to improve the loss in accuracy while maintaining low\n",
    "computational requirements\n",
    "Neuro Prune’s unique combination of reinforcement learn-\n",
    "ing for adaptive decision-making and the Lottery Ticket Hy-\n",
    "pothesis for identifying optimal subnetworks enables it to\n",
    "achieve remarkable efficiency. This approach ensures substan-\n",
    "tial reduction in model size and energy consumption while\n",
    "preserving accuracy and inference speed, making it an ideal\n",
    "solution for edge-based deep learning applications.\n",
    "Through extensive empirical evaluations and theoretical\n",
    "insights, this paper demonstrates how Neuro Pruning outper-\n",
    "forms traditional pruning methods, setting a new benchmark\n",
    "for sustainable, high-performance AI deployment on edge\n",
    "devices. By situating our work within the broader research\n",
    "context, we aim to offer a critical contribution to the ongoing\n",
    "evolution of neural network optimization techniques.\n",
    "II. BACKGROUND\n",
    "A. Need for Pruning Techniques\n",
    "Pruning is a crucial optimization technique for deploying\n",
    "deep neural networks in resource-constrained environments\n",
    "like healthcare. For example, wearable devices like smart-\n",
    "watches that monitor heart rates or portable EEG systems\n",
    "for brain activity analysis require lightweight models due to\n",
    "their limited computational power and memory. Pruning helps\n",
    "address these constraints by reducing the memory footprint\n",
    "and computational load, enabling efficient model deployment.\n",
    "Additionally, it minimizes energy consumption, which is vital\n",
    "for sustainable and real-time healthcare applications. Faster\n",
    "inference enabled by pruning is particularly beneficial for\n",
    "time-sensitive tasks like seizure detection or emergency di-\n",
    "agnostics. Moreover, pruning enhances model interpretability\n",
    "by retaining only the most critical components, which is\n",
    "essential for gaining trust and ensuring regulatory compliance\n",
    "in healthcare. By reducing the hardware and operational costs,\n",
    "pruning also makes deploying advanced AI solutions on edge\n",
    "devices more cost-effective and accessible.\n",
    "B. Types of Pruning Techniques\n",
    "Pruning methods are broadly classified based on granu-\n",
    "larity, timing, and approach. Granularity-based pruning in-\n",
    "cludes structured pruning, which removes entire components\n",
    "such as neurons or filters to streamline computations, and\n",
    "unstructured pruning, which eliminates individual weights\n",
    "for finer optimization, though it often requires specialized\n",
    "hardware. Timing-based pruning involves pre-training pruning,\n",
    "which optimizes models before training; post-training prun-\n",
    "ing, where redundant components are removed from trained\n",
    "models with possible retraining; and dynamic pruning during\n",
    "training, which adjusts models in real-time for improved\n",
    "adaptability. Metric-based pruning uses parameters like weight\n",
    "magnitude or gradient contribution to identify and remove\n",
    "less critical components, while application-specific pruning\n",
    "tailors strategies to specific tasks, such as optimizing for EEG\n",
    "signal analysis, or hardware constraints like GPUs. Together,\n",
    "these methods enable the development of efficient, scalable,\n",
    "and task-specific models, particularly suited for edge device\n",
    "deployment in healthcare and other domains.\n",
    "III. RELATED WORK\n",
    "Jielei Wang et al.[12] introduced an absorption pruning\n",
    "method for object detection in remote sensing imagery. This\n",
    "study achieves efficient compression with minimal accuracy\n",
    "loss but requires careful layer-wise pruning ratio tuning.\n",
    "Jan Muller et al., in their research[12], propose a neural\n",
    "network pruning method for multi-object tracking (MOT) that\n",
    "reduces model size by up to 70\n",
    "Liang Li et al., in their work[9], introduce a novel pruning\n",
    "method for DNNs that utilizes a self-adaptive mechanism\n",
    "based on weight sparsity ratios and a protective reconstruction\n",
    "mechanism. Their approach improves both model compres-\n",
    "sion and accuracy, outperforming state-of-the-art methods on\n",
    "CIFAR-10 and ImageNet datasets.\n",
    "Giacomo Di Fabrizio et al.[4] discuss integrating machine\n",
    "learning with edge devices like Google Coral AI and Nvidia\n",
    "Jetson Nano. Their work revolutionizes computer vision and\n",
    "real-time tracking but raises energy concerns due to the\n",
    "computational demands of advanced algorithms.\n",
    "Satoru Koda et al.[7] explore how weight pruning in\n",
    "DNNs enhances out-of-distribution (OOD) detection, particu-\n",
    "larly with Mahalanobis-based approaches, by improving global\n",
    "feature extraction and leveraging weights not critical for\n",
    "classification.\n",
    "Shvetha S Kumar et al. analyze and compare three pruning\n",
    "techniques—L1-norm filter pruning, channel pruning, and\n",
    "weight pruning—on CNNs for accuracy and inference time,\n",
    "highlighting greater speedup on NVIDIA V100 GPUs com-\n",
    "pared to GTX1080 Ti in their study[8].\n",
    "A Arun Prakash et al. propose a sub-path pruning algorithm\n",
    "for solving the Minimum Robust-Cost Path (MRCP) problem\n",
    "on stochastic networks, extending it to related problems and\n",
    "demonstrating its efficiency on real-world networks in their\n",
    "study[13].\n",
    "Kyoungtaek Choi et al.[2] show that Quantization-Aware\n",
    "Training outperforms Post-Training Quantization, and sparsity\n",
    "training with Optimal Reduction pruning improves YOLOv4’s\n",
    "efficiency and edge-device performance, despite potential\n",
    "high-pruning rate drawbacks.\n",
    "Viviana Crescitelli et al.[3] propose a filter-level pruning\n",
    "method for DNNs. This approach optimizes inference speed\n",
    "and resource efficiency on edge devices with minimal accuracy\n",
    "loss but requires careful tuning of layer-specific pruning ratios.\n",
    "Yongqi An et al.[1] introduce a retraining-free structured\n",
    "pruning framework for Large Language Models (LLMs). Their\n",
    "method incorporates a fluctuation-based pruning metric, adap-\n",
    "tive structure search, and a bias compensation mechanism to\n",
    "achieve efficient pruning. FLAP significantly enhances infer-\n",
    "ence speed and reduces model size without requiring retrain-\n",
    "ing, outperforming state-of-the-art methods like LLM-Pruner\n",
    "and Wanda-sp on LLaMA models across various benchmarks.\n",
    "From referring to these research papers, we derived an\n",
    "idea for pruning on edge devices. Additionally, our proposed\n",
    "solution is inspired by the research of Jaron Maene et al.[10],\n",
    "which suggests that sparse subnetworks in dense networks can\n",
    "achieve similar accuracy when retrained. This study demon-\n",
    "strates stable training with linear mode connectivity, support-\n",
    "ing the idea that lottery tickets retrain to similar regions, but\n",
    "questions their independence from dense training and iterative\n",
    "pruning.\n",
    "Eran Malach et al.[11] further strengthen the lottery ticket\n",
    "hypothesis by proving that over-parameterized neural networks\n",
    "with random weights always contain a subnetwork matching\n",
    "the accuracy of a target network, without additional training.\n",
    "IV. NEUROLOGICAL PRUNING APPROACH\n",
    "The Neurological Pruning Algorithm is designed to opti-\n",
    "mize neural networks by iteratively pruning less important\n",
    "weights while maintaining high performance. Unlike tradi-\n",
    "tional methods, this approach incorporates adaptive, layer-\n",
    "specific pruning thresholds that dynamically adjust based\n",
    "on weight activations, ensuring efficient model optimization\n",
    "across various stages of training. Drawing inspiration from\n",
    "the Lottery Ticket Hypothesis, which suggests that sparse\n",
    "subnetworks can perform similarly to their dense counterparts\n",
    "when retrained, the algorithm selectively deactivates weights\n",
    "using binary masks to enhance computational efficiency with-\n",
    "out sacrificing accuracy.\n",
    "The algorithm operates in several epochs, during which\n",
    "weight activations are evaluated. Based on a quantile-based\n",
    "strategy, pruning thresholds are established for each layer, en-\n",
    "suring that only weights contributing minimally to the model’s\n",
    "output are removed. After pruning, the model undergoes fine-\n",
    "tuning to restore any lost accuracy and further optimize the\n",
    "network. This step enhances the adaptability and scalability\n",
    "of the pruning method, making it suitable for deployment in\n",
    "resource-constrained environments\n",
    "A. Key Characteristics and Advantages\n",
    "The Neurological Pruning Algorithm boasts several key\n",
    "characteristics that distinguish it from conventional pruning\n",
    "techniques. One notable feature is its layer-specific pruning,\n",
    "which evaluates the importance of weights individually for\n",
    "each layer, leading to a more nuanced and efficient sparsifi-\n",
    "cation process. This approach prevents the model from losing\n",
    "essential features that may otherwise be pruned by a global\n",
    "approach.\n",
    "The algorithm’s use of quantile-based thresholding en-\n",
    "sures that only weights with the least contribution to the\n",
    "model’s output are pruned. This fine-tuned method avoids\n",
    "performance degradation by carefully targeting weights that\n",
    "are less significant to the network’s overall functionality.\n",
    "Furthermore, the pruning process is iterative, allowing for\n",
    "continuous model evaluation and real-time adjustments based\n",
    "on performance feedback, ensuring that accuracy is retained\n",
    "even as sparsity increases.\n",
    "Additionally, the algorithm maintains a careful balance\n",
    "between sparsity and accuracy, enabling a reduction in the\n",
    "model’s computational complexity while preserving its predic-\n",
    "tive capabilities. This characteristic is particularly important\n",
    "for applications requiring rapid inference times, such as in\n",
    "real-time healthcare systems and edge computing scenarios,\n",
    "where both energy efficiency and computational power are\n",
    "crucial.\n",
    "B. Impact on Model Performance\n",
    "The Neurological Pruning Algorithm significantly improves\n",
    "the efficiency of neural networks, making them more suitable\n",
    "for deployment on edge devices. Key performance metrics\n",
    "such as compression ratio (CR) and accuracy retention\n",
    "(AR) are critical indicators of the algorithm’s effectiveness.\n",
    "For instance, in evaluating a ResNet-18 model, the algorithm\n",
    "achieved a CR of 1, indicating that while the number of\n",
    "parameters remained the same, the model’s efficiency im-\n",
    "proved through selective pruning, reducing computational load\n",
    "without sacrificing performance.\n",
    "Moreover, the algorithm demonstrates the ability to achieve\n",
    "high accuracy retention, with models maintaining or even\n",
    "slightly improving their predictive performance post-pruning.\n",
    "This outcome highlights the algorithm’s capability to effi-\n",
    "ciently remove redundant weights while preserving the core\n",
    "functionality of the network. The result is a model that is\n",
    "not only more computationally efficient but also adaptable\n",
    "across different architectures and datasets, making it suit-\n",
    "able for a wide range of applications, from image classification\n",
    "to natural language processing .\n",
    "In practical terms, the Neurological Pruning Algorithm en-\n",
    "ables faster inference times and lower energy consumption,\n",
    "two crucial aspects for deploying AI models in resource-\n",
    "constrained environments. As such, it holds significant poten-\n",
    "tial for improving the viability of real-time, energy-efficient\n",
    "AI applications, particularly in fields like healthcare, where\n",
    "both speed and sustainability are essential.\n",
    "V. NEUROLOGICAL PRUNING ALGORITHM\n",
    "The Neurological Pruning Algorithm is a structured and\n",
    "adaptive methodology designed to optimize neural networks\n",
    "for resource-constrained environments. This algorithm goes\n",
    "beyond traditional pruning approaches by incorporating layer-\n",
    "specific pruning thresholds that dynamically adjust to the\n",
    "importance of weight activations within each layer. Drawing\n",
    "on principles inspired by the Lottery Ticket Hypothesis and re-\n",
    "inforcement learning, the algorithm strikes an optimal balance\n",
    "between model sparsity and accuracy .\n",
    "A. Algorithm Description\n",
    "The Neurological Pruning Algorithm iteratively refines the\n",
    "neural network through a series of training epochs. During\n",
    "each epoch, weight activations are tracked, normalized, and\n",
    "analyzed to identify low-importance weights. These weights\n",
    "are selectively deactivated using binary masks, ensuring com-\n",
    "putational efficiency without compromising the network’s pre-\n",
    "dictive performance. The algorithm further integrates a fine-\n",
    "tuning phase, enabling the pruned network to recover and\n",
    "enhance its accuracy. The detailed process is presented in\n",
    "Algorithm 1.\n",
    "By dynamically adjusting to layer-specific characteristics\n",
    "and maintaining performance metrics, the Neurological Prun-\n",
    "ing Algorithm ensures a significant reduction in computational\n",
    "and memory requirements, paving the way for efficient deploy-\n",
    "ment of deep learning models on edge devices .\n",
    "B. Input Parameters and Output\n",
    "Inputs:\n",
    "• Pretrained model (M)\n",
    "• Dataset (D)\n",
    "• Prune fraction (p)\n",
    "• Total epochs (E)\n",
    "Output:\n",
    "• Pruned and fine-tuned model (Mpruned)\n",
    "C. Key Features and Advantages\n",
    "The Neurological Pruning Algorithm is designed with sev-\n",
    "eral distinctive features that contribute to its effectiveness\n",
    "in optimizing neural network performance. These features\n",
    "include:\n",
    "Algorithm 1 Simplified Neurological Pruning Algorithm\n",
    "Require: Pretrained model M, Dataset D, Prune fraction p,\n",
    "Total epochs E\n",
    "Ensure: Pruned and fine-tuned model Mpruned\n",
    "1: Initialize binary masks M with all ones\n",
    "2: for each epoch e = 1 to E do\n",
    "3:\n",
    "Compute weight activations A during forward passes\n",
    "4:\n",
    "for each layer i in M do\n",
    "5:\n",
    "Compute pruning threshold Ti as p-quantile of Ai\n",
    "6:\n",
    "Update mask Mi: retain weights with Ai > Ti\n",
    "7:\n",
    "Apply mask to layer: θi ←θi · Mi\n",
    "8:\n",
    "end for\n",
    "9:\n",
    "Evaluate model performance and update metrics\n",
    "10: end for\n",
    "11: Fine-tune Mpruned on D\n",
    "12: return Mpruned\n",
    "• Layer-specific Pruning: The algorithm applies pruning\n",
    "on a per-layer basis, evaluating the weight activations\n",
    "within each layer individually. This approach ensures that\n",
    "the sparsity introduced by pruning is well-distributed and\n",
    "context-dependent, facilitating more nuanced control over\n",
    "model optimization .\n",
    "• Quantile-based Thresholding: A quantile-based strat-\n",
    "egy is employed to determine the pruning threshold for\n",
    "each layer. This method ensures that only weights with\n",
    "minimal contribution to the network’s output are pruned,\n",
    "thereby reducing the risk of adversely affecting model\n",
    "accuracy while enhancing sparsity .\n",
    "• Iterative Pruning and Evaluation: The pruning pro-\n",
    "cess follows an iterative approach, enabling continuous\n",
    "assessment of the model’s performance after each pruning\n",
    "step. This ensures dynamic adjustments to maintain a bal-\n",
    "ance between model sparsity and predictive performance\n",
    "throughout training.\n",
    "• Preservation of Model Accuracy: By adjusting pruning\n",
    "thresholds in response to ongoing performance metrics,\n",
    "the algorithm ensures that model accuracy is not com-\n",
    "promised. The careful tuning of these thresholds allows\n",
    "for significant sparsity while maintaining the robustness\n",
    "of the network’s predictive capabilities .\n",
    "D. Impact on Neural Network Performance\n",
    "The Neurological Pruning Algorithm has been extensively\n",
    "evaluated across various benchmark datasets and model archi-\n",
    "tectures, demonstrating its capacity to achieve the following\n",
    "outcomes:\n",
    "• High Sparsity with Minimal Accuracy Loss: The algo-\n",
    "rithm effectively prunes unnecessary weights, achieving\n",
    "high sparsity levels without causing significant degrada-\n",
    "tion in model accuracy. This outcome is critical for the\n",
    "efficient deployment of models in resource-constrained\n",
    "environments.\n",
    "• Reduced Computational Complexity: By sparsifying\n",
    "the weight matrices, the algorithm reduces the compu-\n",
    "tational load during inference, which translates to faster\n",
    "model execution and lower resource consumption. This\n",
    "characteristic is especially beneficial in edge computing\n",
    "and real-time applications .\n",
    "• Broad Applicability: The algorithm is highly flexible\n",
    "and can be adapted to various neural network architec-\n",
    "tures and datasets, making it suitable for a wide range\n",
    "of tasks, from image recognition to natural language\n",
    "processing .\n",
    "VI. EVALUATION METRICS\n",
    "A. Performance Comparison\n",
    "Fig. 1. Evaluation of Neurological Pruning on ResNet-18 Model.\n",
    "TABLE I\n",
    "COMPARISON OF METRICS FOR UNPRUNED AND PRUNED MODELS\n",
    "Metric\n",
    "Unpruned\n",
    "Model\n",
    "Pruned\n",
    "Model\n",
    "Accuracy\n",
    "0.9562\n",
    "0.9583\n",
    "Train Loss\n",
    "0.1414\n",
    "0.1625\n",
    "Inference\n",
    "Time\n",
    "(s)\n",
    "0.0318\n",
    "0.0314\n",
    "No of Parame-\n",
    "ters(million)\n",
    "11.7\n",
    "11.7\n",
    "B. Evaluation Metrics Calculation\n",
    "The key metrics used to evaluate the proposed pruning\n",
    "technique are as follows:\n",
    "Compression Ratio (CR): The Compression Ratio (CR) is\n",
    "calculated as:\n",
    "CR = Total Parameters After Pruning\n",
    "Total Parameters Before Pruning\n",
    "(1)\n",
    "CR = 11.7M\n",
    "11.7M = 1\n",
    "Accuracy Retention (AR): The Accuracy Retention (AR) is\n",
    "calculated as:\n",
    "AR =\n",
    "\u0012Accuracy After Pruning −Baseline Accuracy\n",
    "Baseline Accuracy\n",
    "\u0013\n",
    "×100\n",
    "(2)\n",
    "AR =\n",
    "\u00120.958284 −0.956206\n",
    "0.956206\n",
    "\u0013\n",
    "× 100 = 0.22%\n",
    "This result indicates that the pruned model achieves an ac-\n",
    "curacy improvement of 0.22% when compared to unpruned\n",
    "model.\n",
    "C. Illustration of Results\n",
    "Fig. 2. Graphical Representation of Neurological Pruning Results on ResNet-\n",
    "18.\n",
    "D. Discussion\n",
    "The evaluation metrics highlight the effectiveness of the\n",
    "proposed Neurological Pruning method on the ResNet-18\n",
    "model. The Compression Ratio (CR) remains 1, indicating\n",
    "no reduction in the total number of parameters, which suggests\n",
    "that this approach optimizes the neural network by selectively\n",
    "pruning connections rather than reducing the overall parameter\n",
    "count. The Accuracy Retention (AR), which is approximately\n",
    "0.22%, demonstrates that the Neurological Pruning technique\n",
    "successfully improved the accuracy compared to the baseline\n",
    "model.\n",
    "These results validate the potential of Neurological Pruning\n",
    "as an innovative approach to optimizing deep learning models.\n",
    "By maintaining accuracy while likely enhancing sparsity and\n",
    "computational efficiency, this method presents a promising di-\n",
    "rection for achieving high-performance models under resource\n",
    "constraints.\n",
    "VII. CONCLUSION\n",
    "In summary, the Neurological Pruning Algorithm provides\n",
    "a systematic and principled approach to optimizing neural\n",
    "networks. By balancing the trade-off between sparsity and\n",
    "accuracy, it enables significant reductions in computational\n",
    "complexity without sacrificing the network’s predictive per-\n",
    "formance. Future work will focus on extending the algorithm\n",
    "to dynamic network architectures and incorporating advanced\n",
    "regularization techniques to further enhance its robustness and\n",
    "adaptability in diverse applications.\n",
    "REFERENCES\n",
    "[1] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.\n",
    "Fluctuation-based Adaptive Structured Pruning for Large Language\n",
    "Models.\n",
    "[2] Kyoungtaek Choi, Seong Min Wi, Ho Gi Jung, and Jae Kyu Suhr.\n",
    "Simplification of Deep Neural Network-Based Object Detector for Real-\n",
    "Time Edge Computing.\n",
    "[3] Viviana Crescitelli, Seiji Miura, Goichi Ono, and Naohiro Kohmu. Edge\n",
    "Devices Object Detection by Filter Pruning.\n",
    "[4] Giacomo Di Fabrizio, Lorenzo Calisti, Chiara Contoli, Nicholas Kania,\n",
    "and Emanuele Lattanzi. A Study on the Energy-Efficiency of the Object\n",
    "Tracking Algorithms in Edge Devices.\n",
    "[5] Yong He, Lin Lu, Hanjie Yuan, Guangxian Ye, Tianhang Jiang, Liang\n",
    "Chen, Haiao Tan, Gaofeng Liao, Yanchao Zeng, and Jiawei Zhang. A\n",
    "Compression Method for Object Detection Network for Edge Devices.\n",
    "[6] Jielei Wang, Zongyong Cui, Zhipeng Zang, Xiangjie Meng, and Zongjie\n",
    "Cao. Absorption Pruning of Deep Neural Network for Object Detection\n",
    "in Remote Sensing Imagery.\n",
    "[7] Satoru Koda, Alon Zolfi, Edita Grolman, Asaf Shabtai, Ikuya Morikawa,\n",
    "and Yuval Elovici. Pros and Cons of Weight Pruning for Out-of-\n",
    "Distribution Detection: An Empirical Survey.\n",
    "[8] Shvetha S Kumar, Reshma R Nayak, Jismi S Kannampuzha, Jeeho Ryoo,\n",
    "Sahil Rai, and Lizy K John. Evaluation of Pruning Techniques.\n",
    "[9] Liang Li and Pengfei Zhao. Protective Self-Adaptive Pruning to Better\n",
    "Compress DNNs.\n",
    "[10] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards Un-\n",
    "derstanding Iterative Magnitude Pruning: Why Lottery Tickets Win.\n",
    "[11] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.\n",
    "Proving the Lottery Ticket Hypothesis: Pruning is All You Need.\n",
    "[12] Jan M¨uller and Adrian Pigors. Efficient Multi-Object Tracking on Edge\n",
    "Devices via Reconstruction-Based Channel Pruning.\n",
    "[13] A. Arun Prakash and Karthik K. Srinivasan. Pruning Algorithms to\n",
    "Determine Reliable Paths on Networks with Random and Correlated\n",
    "Link Travel Times.\n",
    "[14] Jose Reena K, Charul Nigam, G. Kirubasri, S. Jayachitra, Anurag Aeron,\n",
    "and D. Suganthi. Real-Time Object Detection on Edge Devices Using\n",
    "Mobile Neural Networks.\n",
    "[15] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong\n",
    "Tang. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language\n",
    "Models.\n",
    "[16] Shihao Zhang, Hekai Yang, Chunhua Yang, Wenxia Yuan, Xinghui\n",
    "Li, Xinghua Wang, Yinsong Zhang, Xiaobo Cai, Yubo Sheng, Xiujuan\n",
    "Deng, Wei Huang, Lei Li, Junjie He, and Baijuan Wang. Edge Device\n",
    "Detection of Tea Leaves with One Bud and Two Leaves Based on\n",
    "ShuffleNetv2-YOLOv5-Lite-E.\n",
    "\"\"\", \"metadata\": {\"source\": \"geo_facts.txt\", \"page\": 1}},\n",
    "    {\"content\": \"The Great Wall of China is located in China and is a famous landmark.\", \"metadata\": {\"source\": \"world_landmarks.txt\", \"page\": 5}},\n",
    "    {\"content\": \"Paris is known for the Eiffel Tower.\", \"metadata\": {\"source\": \"geo_facts.txt\", \"page\": 2}},\n",
    "]\n",
    "\n",
    "# Convert raw data to LangChain Document objects\n",
    "documents = [Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"]) for doc in documents_data]\n",
    "\n",
    "# Create FAISS vector store directly from documents\n",
    "# This handles embedding the texts and building the index internally\n",
    "try:\n",
    "    vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "    print(\"FAISS vector store created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating FAISS vector store: {e}\")\n",
    "# Exit if vector store creation fails\n",
    "\n",
    "# Step 4: Load Gemini model for text generation\n",
    "\n",
    "# Step 5: Create the Retrieval-Augmented Generation pipeline\n",
    "print(\"Creating RAG chain...\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\", # Other options: \"mmr\", \"similarity_score_threshold\"\n",
    "    search_kwargs={'k': 2} # Retrieve top 2 relevant documents\n",
    ")\n",
    "\n",
    "# Use the recommended from_chain_type method\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # Options: \"stuff\", \"map_reduce\", \"refine\", \"map_rerank\"\n",
    "                        # \"stuff\" puts all retrieved docs into the context\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True # Include source documents in the output\n",
    "    # chain_type_kwargs={\"prompt\": YOUR_CUSTOM_PROMPT} # Optional: customize prompt\n",
    ")\n",
    "print(\"RAG chain created.\")\n",
    "\n",
    "query = \"who is surya narayanaa\"\n",
    "print(f\"\\nQuerying the RAG system with: '{query}'\")\n",
    "\n",
    "try:\n",
    "    result = rag_chain.invoke({\"query\": query})\n",
    "    # Output results\n",
    "    print(\"\\n--- RAG System Output ---\")\n",
    "    print(\"Answer:\", result.get(\"result\", \"No answer found.\"))\n",
    "    print(\"\\nSources:\")\n",
    "    if result.get(\"source_documents\"):\n",
    "        for i, doc in enumerate(result[\"source_documents\"]):\n",
    "            print(f\"  Source {i+1}:\")\n",
    "            print(f\"    Content: {doc.page_content}\")\n",
    "            print(f\"    Metadata: {doc.metadata}\")\n",
    "    else:\n",
    "        print(\"  No source documents found or returned.\")\n",
    "    print(\"--- End of Output ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during RAG chain execution: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
