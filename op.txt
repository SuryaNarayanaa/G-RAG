   Neuro Prune: An Adaptive Approach for Efficient
   Deep Neural Network Optimization on Edge
   Devices
   Dr. Karpagam G R1, Adhish Krishna S2, Mohana Kumar P3, Sanjay J4, Surya Narayanaa N T5
   1Professor, Department of Computer Science and Engineering, PSG College of Technology
   grk.cse@psgtech.ac.in
   2,3,4,5BE-CSE (AI&ML), PSG College of Technology
   adhishthesak@gmail.com, mohanakumarp2828@gmail.com
   sanjayjayakumar91@gmail.com , suryanarayanaant@gmail.com
   Abstract—The optimization of deep neural networks for de-
   ployment on edge devices is a significant research area due to
   the demand for applications such as augmented reality, smart
   cameras, and autonomous navigation. However, deploying large
   deep learning models on edge devices poses challenges related to
   computational power, energy consumption, and latency. Pruning
   is a method to reduce the model size, accelerate inference, and
   save power. The objective of the paper is to propose the Neuro
   Prune algorithm and to apply it for the optimization of deep
   neural networks on edge devices. Efforts have been made to
   compare pruned and unpruned models. As a result, the pruned
   model has an accuracy increase of 0.22%.
   I. INTRODUCTION
   The rise of intelligent systems, ranging from autonomous
   drones to augmented reality (AR) devices and smart surveil-
   lance cameras, has intensified the need for efficient deep neural
   network deployment on edge devices with constrained com-
   putational resources. While advanced deep learning models
   like YOLO, SSD, ResNet, and MobileNet excel in accuracy,
   their deployment on such devices is hindered by significant
   computational demands, large memory footprints, and high
   energy consumption. These challenges pose critical barriers to
   real-time applications and sustainable deployment.
   This paper builds on existing pruning techniques, includ-
   ing unstructured pruning, structured pruning, and approaches
   inspired by quantization, highlighting their strengths and lim-
   itations. In response to the challenges identified, we introduce
   Neuro Prune, a novel pruning approach that integrates princi-
   ples from reinforcement learning and the Lottery Ticket Hy-
   pothesis to achieve an optimal balance between accuracy, com-
   putational efficiency, and energy consumption. By integrating
   these principles, Neuro Prune provides a robust framework
   for optimizing deep neural networks for deployment on edge
   devices without sacrificing performance.
   Neuro Prune reduces model size and computational re-
   quirements while maintaining or even improving the model’s
   accuracy. These findings underscore the potential of Neuro
   Prune to enable real-time, energy-efficient deep learning on
   resource-constrained devices.
   Neuro Prune employs a systematic process to optimize
   neural networks for edge deployment:
   • Mask Initialization: Each layer’s weights are paired with
   a binary mask (original_mask), initialized to ones.
   • Activation Tracking: During forward passes over the
   dataset, the magnitudes of weight activations are mon-
   itored and recorded.
   • Normalize and Prune: The recorded activations are
   normalized, and a threshold based on the pruning fraction
   is computed. Using this threshold, the masks are dynam-
   ically updated to prune unimportant weights.
   • Evaluate and Log Metrics: After pruning, metrics such
   as model sparsity and accuracy are computed. A reward
   metric is derived to balance accuracy and sparsity using
   a tunable parameter, λweight.
   • Retrain the Model: The pruned model undergoes retrain-
   ing to improve the loss in accuracy while maintaining low
   computational requirements
   Neuro Prune’s unique combination of reinforcement learn-
   ing for adaptive decision-making and the Lottery Ticket Hy-
   pothesis for identifying optimal subnetworks enables it to
   achieve remarkable efficiency. This approach ensures substan-
   tial reduction in model size and energy consumption while
   preserving accuracy and inference speed, making it an ideal
   solution for edge-based deep learning applications.
   Through extensive empirical evaluations and theoretical
   insights, this paper demonstrates how Neuro Pruning outper-
   forms traditional pruning methods, setting a new benchmark
   for sustainable, high-performance AI deployment on edge
   devices. By situating our work within the broader research
   context, we aim to offer a critical contribution to the ongoing
   evolution of neural network optimization techniques.
   II. BACKGROUND
   A. Need for Pruning Techniques
   Pruning is a crucial optimization technique for deploying
   deep neural networks in resource-constrained environments
   like healthcare. For example, wearable devices like smart-
   watches that monitor heart rates or portable EEG systems
   for brain activity analysis require lightweight models due to
   their limited computational power and memory. Pruning helps
   address these constraints by reducing the memory footprint
   and computational load, enabling efficient model deployment.
   Additionally, it minimizes energy consumption, which is vital
   for sustainable and real-time healthcare applications. Faster
   inference enabled by pruning is particularly beneficial for
   time-sensitive tasks like seizure detection or emergency di-
   agnostics. Moreover, pruning enhances model interpretability
   by retaining only the most critical components, which is
   essential for gaining trust and ensuring regulatory compliance
   in healthcare. By reducing the hardware and operational costs,
   pruning also makes deploying advanced AI solutions on edge
   devices more cost-effective and accessible.
   B. Types of Pruning Techniques
   Pruning methods are broadly classified based on granu-
   larity, timing, and approach. Granularity-based pruning in-
   cludes structured pruning, which removes entire components
   such as neurons or filters to streamline computations, and
   unstructured pruning, which eliminates individual weights
   for finer optimization, though it often requires specialized
   hardware. Timing-based pruning involves pre-training pruning,
   which optimizes models before training; post-training prun-
   ing, where redundant components are removed from trained
   models with possible retraining; and dynamic pruning during
   training, which adjusts models in real-time for improved
   adaptability. Metric-based pruning uses parameters like weight
   magnitude or gradient contribution to identify and remove
   less critical components, while application-specific pruning
   tailors strategies to specific tasks, such as optimizing for EEG
   signal analysis, or hardware constraints like GPUs. Together,
   these methods enable the development of efficient, scalable,
   and task-specific models, particularly suited for edge device
   deployment in healthcare and other domains.
   