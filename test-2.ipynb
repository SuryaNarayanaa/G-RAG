{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fe6756f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This sample demonstrates a basic call to the chat completion API.\n",
    "It is leveraging your endpoint and key. The call is synchronous.\"\"\"\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "token = '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'\n",
    "endpoint = \"https://models.github.ai/inference\"\n",
    "\n",
    "# Pick one of the Azure OpenAI models from the GitHub Models service\n",
    "model_name = \"openai/gpt-4o-mini\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=token,\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\",\n",
    "        },\n",
    "    ],\n",
    "    model=model_name,\n",
    "    # Optional parameters\n",
    "    temperature=1.,\n",
    "    max_tokens=1000,\n",
    "    top_p=1.    \n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "159d6282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j Driver\n",
    "import neo4j\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"iambatman\"\n",
    "neo4j_driver = neo4j.GraphDatabase.driver(NEO4J_URI,\n",
    "                auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "# LLM and Embedding Model\n",
    "from neo4j_graphrag.llm import OllamaLLM\n",
    "from neo4j_graphrag.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "llm=OllamaLLM(\n",
    "   model_name=\"llama2\",\n",
    "   model_params={\n",
    "       \"response_format\": {\"type\": \"json_object\"}, # use json_object formatting for best results\n",
    "       \"temperature\": 0 # turning temperature down for more deterministic results\n",
    "   }\n",
    ")\n",
    "\n",
    "# Graph Schema Setup\n",
    "basic_node_labels = [\"Object\", \"Entity\", \"Group\", \"Person\", \"Organization\", \"Place\"]\n",
    "\n",
    "academic_node_labels = [\"ArticleOrPaper\", \"PublicationOrJournal\"]\n",
    "\n",
    "medical_node_labels = [\"Anatomy\", \"BiologicalProcess\", \"Cell\", \"CellularComponent\",\n",
    "                      \"CellType\", \"Condition\", \"Disease\", \"Drug\",\n",
    "                      \"EffectOrPhenotype\", \"Exposure\", \"GeneOrProtein\", \"Molecule\",\n",
    "                      \"MolecularFunction\", \"Pathway\"]\n",
    "\n",
    "node_labels = basic_node_labels + academic_node_labels + medical_node_labels\n",
    "\n",
    "# define relationship types\n",
    "rel_types = [\"ACTIVATES\", \"AFFECTS\", \"ASSESSES\", \"ASSOCIATED_WITH\", \"AUTHORED\",\n",
    "   \"BIOMARKER_FOR\"]\n",
    "\n",
    "#create text embedder\n",
    "embedder = OllamaEmbeddings(model='llama2')\n",
    "\n",
    "# define prompt template\n",
    "prompt_template = '''\n",
    "You are a medical researcher tasks with extracting information from papers\n",
    "and structuring it in a property graph to inform further medical and research Q&A.\n",
    "\n",
    "Extract the entities (nodes) and specify their type from the following Input text.\n",
    "Also extract the relationships between these nodes. the relationship direction goes from the start node to the end node.\n",
    "\n",
    "\n",
    "Return result as JSON using the following format:\n",
    "{{\"nodes\": [ {{\"id\": \"0\", \"label\": \"the type of entity\", \"properties\": {{\"name\": \"name of entity\" }} }}],\n",
    "  \"relationships\": [{{\"type\": \"TYPE_OF_RELATIONSHIP\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {{\"details\": \"Description of the relationship\"}} }}] }}\n",
    "\n",
    "...\n",
    "\n",
    "Use only fhe following nodes and relationships:\n",
    "{schema}\n",
    "\n",
    "Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
    "Do respect the source and target node types for relationship and the relationship direction.\n",
    "\n",
    "Do not return any additional information other than the JSON in it.\n",
    "\n",
    "Examples:\n",
    "{examples}\n",
    "\n",
    "Input text:\n",
    "\n",
    "{text}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b03522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : testing/1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM response is not valid JSON for chunk_index=1\n"
     ]
    }
   ],
   "source": [
    "# Knowledge Graph Builder\n",
    "from neo4j_graphrag.experimental.components.text_splitters.fixed_size_splitter import FixedSizeSplitter\n",
    "from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline\n",
    "driver = neo4j_driver\n",
    "kg_builder_pdf = SimpleKGPipeline(\n",
    "   llm=llm ,\n",
    "   driver=driver,\n",
    "   text_splitter=FixedSizeSplitter(chunk_size=500, chunk_overlap=100),\n",
    "   embedder=embedder,\n",
    "   entities=node_labels,\n",
    "   relations=rel_types,\n",
    "   prompt_template=prompt_template,\n",
    "   from_pdf=True\n",
    ")\n",
    "\n",
    "pdf_file_paths = ['testing/1.pdf',\n",
    "            'testing/tables.pdf',\n",
    "            'testing/tables2.pdf']\n",
    "\n",
    "for path in pdf_file_paths:\n",
    "    print(f\"Processing : {path}\")\n",
    "    pdf_result = await kg_builder_pdf.run_async(file_path=path)\n",
    "    print(f\"Result: {pdf_result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "G-RAG (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
